\chapter{Background}

% \section{Rendering an Image of a 3D Scene}

\section{Displaying an image}
An image is displayed on a computer screen or, in general, a display system, after the processing of the stored data in a computer. Here, both the computer and display system operate with discrete data, known as bits and pixels. Considering real-world objects as continuous structures, the object shapes need to be broken down into discrete surfaces, pixels, for the display of their image. This operation is known as discretization. 

Let's consider the display of a sphere on a computer screen. We apply a grid on the sphere to represent pixels. Some pixels have a constant color of the object, while some are blank. On the other hand, some pixels include different shades of the object color or some are half-overlapped with the sphere. The question here is how to fill the pixels with mixed colors. 

\begin{figure}
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.4\linewidth]{Images/grid_circle.png}
    \includegraphics[width=0.4\linewidth]{Images/merged_grid_circle2-crop.pdf}

   \caption{Need a figure to represent pixel representation of continuous image}
   \label{fig:display-grid}
\end{figure}



In a simple case where we only have a single color with a blank background, we can subdivide this pixel into sub-pixels and count the number of pixels that belong to the object and the background. Later, the color of the corresponding pixel can be approximated by taking the weighted average between the background and object color. For instance, Figure \ref{fig:display-grid} shows the display of a sphere with grids symbolizing pixels. The sub-pixels on the right can be counted to compute the color of the corresponding large pixel. The generalized implementation of this approach can be seen in Figure \ref{fig:color-approximate}, where I computed the mean value with a sliding window of different sizes (50, 100 and 200 in order for image size of 3024 x 4032).




\begin{figure}
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}

    \includegraphics[width=0.9\linewidth]{Images/pixelate_image_snowman.png}

   \caption{}
   \label{fig:color-approximate}
\end{figure}


\begin{wrapfigure}{l}{8.5cm}
\includegraphics[width=0.9\linewidth]{Images/Colorspace.png}
\caption{Comparison of different color gamuts \cite{schewe2007color}}\label{fig:color-gamut}
    
\end{wrapfigure} 

Another approach would be to increase the resolution of an image, that is, increase the number of pixels per image. However, we will still be limited by the resolution of the screen. As we simulate the real world, the discretization process will remain a part of this virtual representation. It is also not only the surface of the objects we need discretize. The color of a pixel should also be broken down into pieces, bits, for the storage in computers. In other words, the number of colors that we can display is limited by the number of bits used to encode the color. 

When computers were first invented, the brightness of each pixel on the screen was encoded using just one bit. The pixel would be white when the bit value was 1, and black when it had a value of 0. These days, one of the most commonly used color spaces, standard RGB space (sRGB), encodes each color with 8 bits, requiring $8 * 3 = 24$ bits per pixel. Here, the question would be how to display a color that is not available in a color space. This problem is known as color quantization. The solution would be again approximating the desired color with the closest matching color available in the space/palette.

% \begin{figure}
%   \centering
% \caption{Comparison of different color gamuts \cite{schewe2007color}}\label{fig:color-gamut}
%     \includegraphics[width=0.44\linewidth]{Images/Colorspace.png}
% \end{figure} 
% %-----------------



The issue with color quantization is that few color samples might not accurately represent the continuous color spectrum, causing discrete steps, bands, between the color samples that can be visible in the images (Figure \ref{fig:color-band}). Luckily, the current image formats allow 24 (sRGB) / 32 (RGBA) bits to encode colors, displaying more than 16 million distinct colors, reducing the color banding effect significantly. Nevertheless, the representation of continuous signals in the virtual environment remains limited due to the discretization of the data with bits for the storage in a computer.

Another problem we need to consider is aliasing, which occurs when sampling frequency for discretization is lower than twice the continuous signal bandwidth (Nyquist Theorem). The range of frequencies you can capture in a scene depends on the image resolution or pixel size. For instance, if an object is too far, then its projection can be smaller than the pixel size, in which case we end up seeing the object as a dot.



\begin{figure}
  \centering
    \includegraphics[width=0.9\linewidth]{Images/color_quantization.png}

    \caption{When a color is represented with too few bits, the transition between colors appears as discrete steps, known as color banding (right).}\label{fig:color-band}
\end{figure} 



Lastly, I would like to mention that the pixel-based displays discussed above is called raster graphics or raster displays. It lay outs the image onto a two-dimensional array of pixels with a grid of x and y coordinates on a display space. Here, zoom-in does not bring us any more details as we are limited by the image or screen resolution. To address this issue, vector graphics stores the shape of objects with mathematical expressions instead of pixel values. This offers flexibility with the resolution as the shape of the objects is computed based on the desired resolution.

\section{3D scene components}

A 3D scene is composed of objects of different sizes, shapes, and appearances . To render an image of a scene, we first need a viewpoint, which will be represented by a camera. This mimics the case in the human visual system (HVS) where we look at a scene from a certain viewpoint, and the image of the scene appears in our retina. Without any light, all scene components appear dark. Therefore, light is a crucial aspect while describing a scene. Another important aspect is geometry which describes the shape of the objects. Geometry, camera and light are three essential components of a 3D scene. While rendering a scene in a 3D software platform, such as Blender or Maya, a scene file contains all this information along with material/texture details. The light-material interaction defines the appearance, which this thesis is focused on. Before delving into the details of appearance, I will briefly review geometry as it builds up a baseline for the appearance representation in computer graphics.

\begin{figure}
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.7\linewidth]{Images/scene_with_camera.jpg}
   \caption{Rendering an image of a 3D scene requires lighting, viewpoint (camera) and geometry. Image courtesy of \citeauthor{boss2021nerd} \cite{boss2021nerd}.}
   \label{fig:teaser}
\end{figure}


\paragraph{Geometry}
In real world, what differentiates the state of the objects is the density of the matter the objects are made of. For instance, clouds are made of loosely connected molecules with large holes in-between. Wood and metal, on the other hand, are densely composed with little empty space between the molecules. On the other hand, computer graphics assumes that objects are either solid or not. This helps keep things simple since the external shape of an object is what matters at the end. 


In general, the shape of an object is represented with 3D points in computers' memory, with $x$, $y$, $z$ axes defined in the Cartesian coordinate system. The surface (a polygon) can then be constructed by connecting multiple points that lie on the same plane (co-planar). The simplest polygon we can create is a triangle. Triangles are widely used because of their simplicity and efficiency which led to the development of multiple efficient algorithms to compute the intersection of a triangle with a line. In case surfaces are defined by multiple points, it is common to  divide them into multiple triangles, known as triangulation. Although the real-world objects are not naturally polygonal, this simplification helps efficient rendering and digital representation.

\begin{figure}
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.7\linewidth]{Images/Triangulation-of-surfaces-Any-curved-surface-in-this-case-a-sphere-can-be-approximated.png}
   \caption{Triangulation of surfaces is a commonly used method in computer graphics to define surfaces. It allows the creation of complex shapes. Increasing the number of triangles also increases the resolution of the shape that leads to smoother surface representations. Image courtesy of \citeauthor{triangulation} \cite{triangulation}.}
   \label{fig:triangulation}
\end{figure}

One might wonder how triangulation handles the reconstruction of smooth surfaces. It is indeed not not optimal solution. However, considering a smooth curve, we can approximate it by taking a few points (samples) on the curve and connecting them with straight lines (segments). To enhance the resolution, we can take more samples, that is, reduce the size of the segments. Applying this to smooth surfaces, we can increase the number of triangles (Figure \ref{fig:triangulation}, which will eventually increase the rendering time. The process of converting a smooth surface to triangular mesh is known as tesselation.


Returning back to the early discussion on discretization, computer graphics only approximates the shape of the real-world continuous objects with discrete data. Since the ultimate goal is to display these shapes on a screen that also operates with discrete data, one well-known approach to conceal the triangular look of a surface is to keep the size of the triangles smaller than the pixel size. This approach has been used widely in both professional rendering programs, such Pixar's RenderMan, and recently taken its place in real-time applications.

Ray tracing algorithms, which HyperBRDF is built on, do not necessarily need a polygonal representation of objects. Ray tracing computes the intersection point of a ray (a straight line) with the object surface, which can be found either with a geometric or an algebraic solution. The algebraic solution becomes useful when the surfaces are defined by mathematical equations, known as implicit/algebraic surfaces. A ray equation, essentially a line equation, along with the mathematical equation for the surface of an object give us a system of linear equations, for which a solution exists if the ray and the object intersect.

 Although geometry can be defined by various methods, such as meshes, NURBS, subdivision surfaces, implicit surfaces, etc, this section only focuses on polygonal mesh representation. Triangle is the most common rendering primitive used in ray tracing and on modern GPUs. Since the support of only one primitive is preferred due to simplicity and efficiency, most 3D platforms first convert geometry to triangular meshes before rendering.
 
\section{Photo-realistic rendering}
There are different approaches to render an image of a scene. Whichever rendering method we choose, we should expect the same image as the output. In photo-realistic rendering, which is often the most desired, the expectation from a rendered image is to look the same as how our eyes perceive the scene. In other words, the image should look like a "photograph". Achieving such photo-realism requires us to understand two main aspects of our visual system: 1) geometric construction of the shapes with respect to other objects, 2) the physics laws behind appearance. More specifically, photo-realistic rendering focuses on simulating the behaviour of light throughout its propagation and its interaction with matter.

\paragraph{Perspective projection and visibility.}

The human visual system is inherently an optical system that converges light rays that are reflected from objects to a focal point. Following the geometry, our eyes perceive objects that are further away smaller than the closer objects of the same size. That is, objects get smaller in the rendered image in our retina as they move away from our eyes. This is known as foreshortening effect. The lens systems in cameras already replicate this effect, and photo-realistic rendering also aims to achieve this foreshortening effect along with simulating the physics laws for appearance.  

To achieve foreshortening effect, let us imagine we have a canvas or an image plane between the eye and the object. By tracing the lines from the eye to the corners of the objects, we can find the location of the projected corners (points) on the image plane. This is known as perspective projection. We can then draw the edges of the object on the image plane to complete the look. However, this look is unlikely to be photo-realistic as we haven't figured what edges are visible from the viewpoint. For instance, if it was an opaque cube, then the back sides would remain hidden behind the front sides. Therefore, rendering involves computations for the visibility problem along with the perspective projection. 

\begin{figure}[h]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.6\linewidth]{Images/perspective_projection.png}
   \caption{}
   \label{fig:perspective_projection}
\end{figure}



There are different ways to project a 3D scene onto a flat surface. For instance, in artistic drawing, perspective projection can include multiple focal points (Figure \ref{fig:artistic_drawing}). However, computer graphics generally assumes one point of perspective as it mimics our visual system as well as cameras.  In this approach, the line of sight, the line from the eye perpendicular to the canvas, passes through the center of the canvas. Viewing frustum is then defined as the region of the scene that is projected onto the canvas. That is, the lines passing through the corners of the canvas comprise the corners of the frustum base. Here, we can change the size of the canvas, which results in the change of the frustum size, and so, the visible region of the scene. Akin to photography, changing the focal length of the camera lenses adjusts the field of view.

\begin{figure}[h]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.4\linewidth, height=4.5cm]{Images/single_point_perspective.png}
    \includegraphics[width=0.4\linewidth, height=4.5cm]{Images/two_points_perspective.png}

   \caption{Single-point vs two-point perspective. Computer graphics mimics our visual system, focusing on single-point projection. However, multi-point perspective is widely explored in artistic drawing.}
   \label{fig:artistic_drawing}
\end{figure}


% \paragraph{Visibility problem}
The visibility problem occurs when some parts of the scene are not visible from a certain viewpoint. For instance, if we simply project the corners of an object and draw the edges, then some corners that shouldn't be visible from  a certain viewpoint will be included in the rendered image. Furthermore, some objects will be occluded by others, hiding them from the viewpoint. Computer graphics tackles the visibility problem in two different ways: rasterization, and ray tracing. I will not go into the details of these algorithms as this is beyond the focus of this thesis.

% In this section, I will mostly focus on ray tracing as HyperBRDF is designed for such renderers.Photo-realistic rendering aims to project or flatten a 3D scene onto an image plane that lies between the scene and the camera position or the viewpoint. Mimicking our visual system, we first apply perspective projection via similarity formula in geometry and then decide which points are visible from the viewpoint. Some will be indeed occluded by other objects or the front sides. 

\paragraph{Appearance.}
Perspective projection and visibility only projects the size and shape of the scene realistically without any consideration for appearance, such as the color, texture, brightness, etc. We can only perceive an object if the light bounces off their surface. Therefore, appearance is defined by the light-matter interactions, where the light travels in a ray form in space and gets absorbed or reflected when it interacts with a surface of an object. It can be emitted by different light sources, such as the sun, or light bulbs. After the light bounces off a surface, it continues its journey until it either reaches our eyes where it is converted to electrical signals or another surface where the interaction steps repeat.

The color of an object is defined by a mixture of reflected light colors. The light color is a continuous spectrum with the visible light lying between 380 to 700 nanometers in wavelength. For instance, a lemon reflects green and red light, while absorbing the blue. Another example would be a black object that absorbs all visible lights, or white object that reflects them all.

At the object level, reflection can be considered as a mirror-like look, where the light turns around the normal of the surface at the point of contact. The outgoing direction becomes the reflected version of the incoming one around this normal. On the other hand, microscopic level interactions are more complex with light bouncing off in random directions. This is called scattering. Computing the photon-atom interactions is indeed impractical. Therefore, computer graphics researchers have developed mathematical models to simulate the light-matter interactions at the microscopic level. I will discuss them later in this chapter. Section \ref{} will also compare HyperBRDF with such mathematical formulas. Here I will overview some of the well-known shading and lighting effects:

\textbf{\textit{Reflection}} occurs when light interacts with mirror-like objects that change the direction of light in equal but reverse angle around the point of contact. The incoming light, also known as incident light, turns around the normal and leaves the surface in the reflected direction, outgoing direction (Figure \ref{fig:microfacet} - right). Metals, such as silver or aluminium are considered as materials with high reflectivity. Surface of the water or glass also creates reflection, but their reflectivity is considerably lower than metals.

\begin{wrapfigure}{r}{5.5cm}
\includegraphics[width=0.9\linewidth]{Images/rough_river_surface_reflection.png}
\caption{The ripples on the water's surface cause a blurred image of the bridge, acting as a rough surface.}\label{fig:water_reflection}
    
\end{wrapfigure} 

\textbf{\textit{Specular reflection}} appears when the surface is not perfectly smooth, and its roughness causes lights to bounce off in different directions. Imagine a rough surface at the microscopic level having many microfacets looking in slightly different directions. The complex nature of the material surface then reflects the lights at a microfacet level, each microfacet acting like a mirror (Figure \ref{fig:microfacet}). The overall outgoing light becomes the accumulation of each of these reflections, known as specular or glossy reflection. Most often, these microfacets are not visible to our eyes akin to smooth surface appearance. The deviation of lights from the mirror direction depends on how rough the surface is. Here, roughness and glossiness are used antonymous to describe the specularity of a material. This type of reflection often leads to blurred or deformed image, as shown in Figure \ref{fig:water_reflection}. 


\begin{figure}
\includegraphics[width=0.9\linewidth]{Images/diagram_microfacet.png}
\caption{Reflection on a surface with microfacets vs smooth surface. Image from \citeauthor{googlePhysicallyBased} \cite{googlePhysicallyBased}}\label{fig:microfacet}
    
\end{figure} 


% \begin{figure}
%   \centering
%   % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    \includegraphics[width=0.5\linewidth]{Images/rough_river_surface_reflection.png}
%    \caption{The ripples on the surface of the water causes a blurred image of the bridge, acting like a rough surface.}
%    \label{fig:water_reflection}
% \end{figure}


\textbf{\textit{Diffuse reflection}} is the reflection that we observe when the incident lights are so scattered that they reflect in random directions, equally spread. Diffuse surfaces are either extremely rough or composed of tiny structures where the light gets trapped, reflected and refracted multiple times before leaving the surface. The outgoing direction becomes independent from the incident direction. This randomness results in the brightness of the object appear equal in all viewing directions. 



\begin{figure}[h]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.5\linewidth]{Images/Differences-between-the-specular-and-diffuse-reflections-specuclar-reflections-occur-on.png}
   \caption{Specular vs diffuse reflection. Diffuse surfaces cause rays to scatter randomly, brightening the object equally in all viewing directions. Specular reflection is, on the other hand, view-dependent, reflected rays concentrated around the mirror reflection. Image courtesy of \citeauthor{specfig}.}
   \label{fig:specularvsdiffuse}
\end{figure}

Some other lighting and shading effects include transparency, subsurface scattering, indirect lighting and shadows. When light contacts a transparent object, such as glass or water, it bends and passes through the surface in a different direction, which can be computed using Snell's Law. Subsurface scattering, also known as translucency, occurs when light travels within the object, leaving the object at a different point and direction. Wax, marble or thin layers of skin are considered as translucent objects. Subsurface scattering is challenging to simulate. Indirect lighting is the result of the light being reflected from other surfaces, illuminating the object without any direct light from the light source. Lastly, shadows are dark regions due to blocked light by an object. 

The appearance of a scene depends on how light interacts with an object and the path light travels through. All the aforementioned effects contribute to appearance. Some effects, such as reflection, specularity, diffuse, transparency and subsurface scattering, relate to material properties. They affect shading, the appearance of an object. Other effects, shadows and indirect lighting, depend on the light's journey (light transport), that is, how much light the object gets after the light interacts with multiple surfaces. 

Shading is concerned with the interaction of light with the material. Light transport tracks the light's path throughout its journey while it bounces off surfaces. It takes into account the obstructions, changes of reflections, etc. In the real world, the two are not differentiated as they determine the appearance together. However, computer graphics keep these two distinct for efficient and practical computations. It is worth mentioning that simulating light-matter interactions is more challenging than tracking the light path. The latter is more straightforward, while the former requires simplifications for the material representations.



\paragraph{Ray tracing - Light transport simulation.}

Light starts its journey from a light source and travels in a straight line, bouncing off surfaces, Following this path is known as forward tracing. However, not all rays emitted from a source will reach the eye/camera. Considering the size of an eye, most rays are unlikely to contribute to our view of the scene, travelling in different directions. As it is highly impractical to trace all rays including those not contributing to our view, ray tracing algorithms trace the rays from the eye to the light source in the reverse direction for efficient computations (backward tracing). 




\begin{figure}[h]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.5\linewidth]{Images/ray-tracing-image-1.jpg}
   \caption{Ray tracing -Image courtesy of \href{https://commons.wikimedia.org/wiki/File:Ray_trace_diagram.svg}{Henrik, CC BY-SA 4.0}}
   \label{fig:raytracing}
\end{figure}




Simulating the light-matter interactions is very complex not because we cannot accurately simulate the physics laws but because of the huge amount of computations required to trace the rays. This brings us to the trade-off between the render time and accuracy of the rendered image. This trade-off often leads to different applications, such as virtual effects in film industry requiring high accuracy with longer render time and real-time AR/VR applications with fast rendering but relatively low quality.




\TODO{You should start with rendering. What it actually means. How is that related to image formation or rendering an image of a virtual environment.}
Rendering aims to reproduce the shape, visibility, and appearance of objects as seen from a given viewpoint. 

the appearance of objects, which we aim to simulate in photorealistic rendering, is essentially the by-product of two main factors: lighting and the properties of the object

Shading
Even though we haven't detailed how the visibility problem can be solved, let's assume for now that we know how to flatten a 3D scene onto a flat surface using perspective projection and determine which part of the geometry is visible from a certain viewpoint. This is a significant step towards generating a photorealistic image, but what else do we need? Objects are not only defined by their shape but also by their appearance. This includes not just how big they appear on the scene, but in terms of their look, color, texture, and how bright they are. Furthermore, objects are only visible to the human eye because light bounces off their surface. How can we define the appearance of an object? The appearance can be defined by how the material of the object interacts with light. Light is emitted by sources (such as the sun, light bulbs, or the flame of a candle) and travels in a straight line until it contacts an object. Here, it can either be absorbed by the object or reflected into the environment. When light is reflected off an object's surface, it continues traveling (potentially in a different direction) until it either contacts another object (repeating the process) or reaches our eyes (where it is converted into electrical signals sent to the brain).

Finding the right balance between photorealism and render time is key to effective rendering


\section{Rendering and Image Formation}
\section{Deep learning}
\subsection{Multilayer Perceptron}
\subsection{Hypernetworks}
\subsection{Diffusion models}