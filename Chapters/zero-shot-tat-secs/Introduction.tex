\section{Introduction}\label{sec:zero-shot-intro}


Attributes are high-level descriptions of visual concepts. In contrast to categories, which broadly classify objects or scenes based on high-level features, attributes often remain continuous or variable, changing across instances of the same category. For instance, in the "outdoor scene" category, attributes can be associated with seasonal transitions (e.g., winter to spring), weather changes (e.g., rainy, sunny), or times of day (e.g., daylight, sunset, night). In this work, I focus on such transient attributes that dynamically shape the visual appearance of an outdoor scene.


\begin{figure}[ht]
  \includegraphics[width=\textwidth]{Chapters/zero-shot-tat-figs/tat-teaser.pdf}
  \caption{Transition of the illumination throughout the day or the weather changes across seasons alter the scene appearance significantly in terms of colour, tone, texture, and style. However, main content, such as the structure, permanent objects, etc. remains unchanged. Transient attribute transfer aims to capture alterations related to such temporal effects. Images from the Transient Attribute Dataset \cite{laffont2014transient}.}
  \label{fig:zero-shot-teaser}
\end{figure}

Transient attributes influence our perception of a scene by altering appearance-related properties, such as colour, texture, and overall atmosphere, i.e., style and tone. From the slow transition of sunrise to daylight, the accumulation of snow on the ground, to the clearing sky after an overcast day, these attributes define the dynamic and captivating nature of the visual world. Recent works in the fields of computer vision and graphics have engaged with the transfer of such attributes, either under the umbrella term of high-level image editing or as standalone tasks. Capturing such temporal changes plays a crucial role in a range of applications, such as photography, filmmaking, gaming, and virtual and augmented reality, for producing photorealistic simulations. However, the accurate transfer of transient attributes is highly challenging due to the requirement for a deep understanding of the scene components as well as the need to maintain essential content.

This chapter aims to address these challenges by exploring recent advancements in generative models, more specifically diffusion-based approaches. The observation that fine-tuning these foundation models with a small dataset can lead to overfitting motivates me to leverage the pre-trained models to guide the transfer process in a way that improves generalisability. Therefore, I focus on a zero-shot approach that eliminates the need for extensive additional datasets, instead utilising the robust priors embedded in foundation models. I evaluate this approach through qualitative comparisons with fine-tuned models and demonstrate its ability to effectively capture transfers, such as seasonal changes and varying lighting and weather conditions, while preserving the scene's core features.

%By advancing the understanding and manipulation of transient scene attributes, this research contributes to the broader goal of enhancing the realism and flexibility of visual content generation, paving the way for more immersive and contextually adaptive visual experiences.

In summary, this chapter contributes to the broader goal of manipulating appearance in a data-efficient manner by:

%Multiple Transformation Blending
\begin{itemize}

   \item Exploiting a pre-trained diffusion model in a zero-shot setting, requiring only an input image and a single-word text prompt describing the target attribute.
    
   \item Capturing the ever-changing transitions in an outdoor scene without any explicit decomposition of scene components.

\end{itemize}
