\section{Related work}\label{zero-shot-RW}
This work fall into the broader category of image-to-image translation. Here, I will review the recent works on diffusion-based techniques.

\subsection{Text-to-image synthesis}

Recent years have seen significant advancements in text-to-image synthesis from the initial Generative Adversarial Network (GAN)-based approaches \cite{li2020manigan,xu2018attngan, zhang2017stackgan, zhang2018stackgan++} to the latest diffusion \cite{gu2022vector, ho2020denoising,nichol2021improved,ramesh2022hierarchical, rombach2022high, saharia2022photorealistic,song2020denoising,zhang2023adding} and transformer models \cite{ding2022cogview2, esser2021taming, ramesh2021zero, yu2022scaling}. Notably, DALLÂ·E 2 \cite{ramesh2022hierarchical} and Imagen \cite{ho2022imagen} condition the diffusion models with text prompts.  Pre-trained CLIP models \cite{radford2021learning} are also explored for guidance of image generation with text descriptions \cite{crowson2022vqgan, ramesh2022hierarchical}. More recently, Stable Diffusion \cite{rombach2022high}, trained on vast image-text pairs \cite{schuhmann2021laion}, was released to the public and has since become a key tool in the field for both image generation and manipulation. Later, ControlNet \cite{zhang2023adding} includes spatial conditioning controls, such as edges, depth, segmentation, human pose, etc, to Stable Diffusion for image generation. This work utilises the latest advancements on conditional image generation and explore their pre-trained capacity to transfer transient attributes.

\subsection{Text-guided image manipulation}
Pre-trained generator models and CLIP \cite{radford2021learning} have been deployed for text-guided image manipulation with extended applications including video editing and style transfer \cite{bar2022text2live, gal2022stylegan,kwon2022clipstyler, liu2021fusedream, patashnik2021styleclip}. For instance, StyleCLIP \cite{patashnik2021styleclip}, based on a GAN-based generative model StyleGAN \cite{karras2020analyzing}, modifies input latent vectors with a CLIP-based loss, while VQGAN-CLIP \cite{crowson2022vqgan} guides VQ-GAN \cite{esser2021taming} using CLIP for high-quality image generation and editing. Furthermore, diffusion models have been extensively studied for text-driven image manipulation \cite{avrahami2022blended, gal2022image, kawar2023imagic, kim2022diffusionclip, liu2023more, meng2021sdedit,nichol2021glide,ruiz2023dreambooth}. Imagic \cite{kawar2023imagic} obtains text embeddings that align with the input image and the target text prompt and fine-tune a pre-trained diffusion model to capture the desired edits. InstructPix2Pix \cite{brooks2023instructpix2pix} fine-tunes Stable Diffusion \cite{rombach2022high} with human instructions generated with the help of GPT-3 \cite{brown2020language}. 

Similar to this work, DiffEdit  \cite{couairon2022diffedit}, MasaCtrl \cite{cao2023masactrl} and ZeCon \cite{yang2023zero} perform tuning-free text-guided image manipulation without requiring additional extensive datasets. DiffEdit  \cite{couairon2022diffedit} utilises DDIM inversion \cite{dhariwal2021diffusion, song2020denoising} along with automatically generated masks to capture local image edits. MasaCtrl \cite{cao2023masactrl} introduces a mutual self-attention method guided by a mask that is extracted from the cross-attention maps in the diffusion models. ZeCon \cite{yang2023zero} introduces a zero-shot contrastive loss that works in a patch-based feature space for image style transfer. DiffEdit  \cite{couairon2022diffedit} and MasaCtrl \cite{cao2023masactrl} aim for local edits with mask guidance, potentially missing global transfers that are more dominant in the transitions between the transient attributes. ZeCon \cite{yang2023zero} edits high-level content without concerns about photo-realism, consequently not preserving the main structures. In contrast, I explore a tuning-free method that can 1) maintain photo-realism and the essential content, 2) capture global transfers, such as illumination changes, without any mask guidance.
