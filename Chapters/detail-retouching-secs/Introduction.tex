\section{Introduction}\label{sec:introduction}

Photo retouching is often desirable as it improves the aesthetic quality of photographs by eliminating imperfections and \nobreak highlighting subjects of interest. Despite significant advancements in digital photography, owing to improvements in camera sensors and image processing algorithms, professional retouching through manual adjustments is still required to achieve a desired look. These artistic edits demand considerable manual effort, as they involve global adjustments, such as brightening and contrast enhancement, as well as fine edits applied to local regions. Professionals invest a great deal of time in producing such edits, which motivates us to automatically replicate a specific style or type of retouch.

The development of automatic photo retouching tools can benefit both novice users and experts, as it provides a foundation for achieving a professional retouching style. However, automating detailed edits of professionals is challenging since their editing workflows are spatially varying, context-aware, and highly nonlinear, involving per-pixel adjustments. Recent learning-based methods attempt to address this complexity in image-to-image translation, including pixel-adaptive neural network architectures \cite{shaham2021spatially, li2020lapar}, learning parameters of local filters \cite{moran2020deeplpf}, or multi-stream models to extract global and local features separately \cite{Gharbi17Deep}. However, these data-driven methods rely on large datasets of matching example image pairs. Furthermore, the mappings remain sensitive to segmentation errors, unseen semantic regions, and image content~\cite{yan2016automatic}.


 %Many successful methods require example and input images to be content-wise pixel-level aligned to achieve plausible results \cite{Shih14Style}.

% in intricate details 

The gap between manual and automatic enhancement motivates the development of a novel photo retouching technique that can learn global and local adjustments from just \emph{a single example image pair}. This method therefore eliminates the need for large datasets, which are particularly difficult to obtain for detailed retouching tasks. Users can choose one example \emph{before-after} pair from which the model learns the underlying retouching style. Subsequently, the retouching edit can be applied to a different input image.

Here, we assume that example and input images share similar local content. The user can thus decide on the semantics of the example and input photos and the structural changes to be transferred. This process is intuitive for humans and suitable for various scenarios, e.g. face edits transferred to faces. The framework then handles the difficult part for humans: capturing the fine detail changes in an edit and applying them automatically to a new image. Additionally, the method can be combined with brushes if fully automatic transfers are not desired.

This is achieved by defining the retouching problem as a map given by a \emph{spatio-spectral patch-space neural field based transformation blending}. This representation is primarily inspired by professional detail retouching pipelines as I elaborate on in Section \ref{chap:motivations}. This map representation is composed of learned patch maps at multiple scales, i.e. frequency bands. Each of these maps is represented by a number of \emph{transformation matrices} blended with \emph{patch-adaptive weights} that are learned by a multilayer perceptron. I jointly optimise the transformation matrices and corresponding weights for each band. This representation captures edits to details better than any previous techniques while staying generalisable to new images. It is also simple enough to be extended in many different ways in future works.

This is achieved by defining the retouching problem as a map given by a \emph{spatio-spectral patch-space neural field-based transformation blending}. This representation is primarily inspired by professional detail retouching pipelines, as elaborated in Section \ref{chap:motivations}. This map representation is composed of learned patch maps at multiple scales, i.e. frequency bands. Each of these maps is represented by several \emph{transformation matrices} blended with \emph{patch-adaptive weights} that are learned by a multilayer perceptron. The transformation matrices and corresponding weights are jointly optimised for each band. This representation captures edits to details better than any previous techniques while remaining generalisable to new images. It is also simple enough to be extended in many different ways in future works.

In summary, there are two main contributions of this work: 

%Multiple Transformation Blending
\begin{itemize}

    \item \textbf{A novel patch-space image map representation} as a weighted blending of transformation matrices with neural fields.
    
	\item \textbf{A one-shot detail retouching algorithm} that allows the transfer of edits in details to new images based on a single before-after image pair.

\end{itemize}
