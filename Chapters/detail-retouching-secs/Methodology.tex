
\section{One-shot retouching}
\label{sec:Methodology}

\subsection{Frequency decomposition}\label{sec:thePatchMap}
% \myworries{Modify equations in the box - transpose is unnecessary}

I first decompose example and input images into different frequency bands by constructing a Laplacian pyramid to capture details at multiple scales. In principle, it is possible to utilise any multiscale image decomposition method. However, I observed that a basic Laplacian pyramid helped in capturing more accurate and generalisable results compared to a guided or bilateral pyramid. Therefore, I decompose images by

\begin{equation} 
	X_l = L_l(X) = 
 \left \{ \begin{aligned}
        X - G(2) \ast X \hspace{5mm} &l=0\\
        G(2^l) \ast X - G(2^{l+1}) \ast X \hspace{5mm} &l>0,
       \end{aligned}
 \right.
\end{equation}
where $G(\sigma)$ is the normalised Gaussian kernel, and $\ast$ denotes convolution. I also store the low-pass filtered image $S(X)$ such that $X = S(X) + \sum_{l=0}^{n_L} L_l(X)$. I then downsample each $L_l(X)$ and $S(X)$ according to the maximum frequency present at that band. This allows me to use small $3 \times 3$ patches at each band. In the experiments, I used $n_L = 5$ bands for the Laplacian pyramid.


Since each band is processed independently, I explain the steps of our technique below for two generic images $X$ and $Y$.


\subsection{Transformation blending}\label{sec:Blending}

The mapping is defined between patches $\mathbf{x} \in \mathbb{R}^{d_X}$ to $\mathbf{y} \in \mathbb{R}^{d_Y}$ extracted from $X$ and $Y$, respectively, where the patches are denoted by vectors stacking the pixel values, and the patch spaces are defined by $\mathbb{R}^{d_X}$ and  $\mathbb{R}^{d_Y}$. For all results in this work, I work with $3 \times 3$ patches and thus $d_X = d_Y = 9$.

The mapping takes the form of a weighted average of learned transformation matrices $\mathbf{A}$, where each transformation matrix $\mathbf{A}_k$ is first multiplied with its corresponding blending weight: 

\begin{equation} 
	\mathbf{y} (\mathbf{x}) = \sum_{k=1}^K
	\mathit{f}_k (\mathbf{x}) \mathbf{A}_k \mathbf{x},
	\label{eq:weightedSum}
\end{equation} 
Here, $K$ is the number of transformation matrices, and $f_k$ are the blending weights, learned by an MLP block of output size $K$. The $\mathbf{A}_k$'s and $f_k$'s are jointly learned by minimizing the following loss on patches extracted from the before and after images.

\begin{equation}
    L_{loss}  = \mathbb{E}_{X, Y} || \mathbf{y}_i -   \mathbf{y} (\mathbf{x}_i) ||
\end{equation}

Each $\mathbf{A}_k$ corresponds to a different type of transformation and the $f_k(\mathbf{x})$'s, represented with the MLP, allow for a smooth transition between different transformations. The form of $f_k$'s is relatively simple with three fully-connected layers and nonlinear activation functions applied after each layer. This blending forms a simple but expressive transformation as illustrated in Section \ref{sec:results}.


\subsection{Retouching an input image}

I process the input image $I$ the same way as the before-after pair. First, I decompose the input into its Laplacian layers and then extract patches for each layer. After applying the learned mappings $M_l$ to the patches of the corresponding layers $L_{l}(I)$ independently, I then reconstruct the Laplacian bands of the output image $O_l$ by placing the patches in their spatial locations and averaging over the overlapping regions. Later, I obtain the final output image $O$ by summing the outputs $O_l$ and the residual of the input image:
\begin{equation}
    O = S(I) + \sum_{l=0}^{n_L} M_l(L_l(I))\,.
\end{equation}

