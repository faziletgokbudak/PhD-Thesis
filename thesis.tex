%%%%%
%%
%% Sample document ``thesis.tex''
%%
%% Version: v0.2
%% Authors: Jean Martina, Rok Strnisa, Matej Urbas
%% Date: 30/07/2008
%%
%% Copyright (c) 2008-2011, Rok Strni≈°a, Jean Martina, Matej Urbas
%% License: Simplified BSD License
%% License file: ./License
%% Original License URL: http://www.freebsd.org/copyright/freebsd-license.html
%%%%%

% Available documentclass options:
%
%   <all `report` document class options, e.g.: `a5paper`>
%   withindex   - enables the index. New index entries can be added through `\index{my entry}`
%   glossary    - enables the glossary.
%   techreport  - typesets the thesis in the technical report format.
%   firstyr     - formats the document as a first-year report.
%   times       - uses the `Times` font.
%   backrefs    - add back references in the Bibliography section
%
% For more info see `README.md`
\documentclass[withindex, glossary]{cam-thesis}
\usepackage[table]{xcolor}% http://ctan.org/pkg/xcolor

% Citations using numbers
\usepackage[numbers]{natbib}
% \usepackage[dvipsnames]{xcolor}
\usepackage{graphicx,wrapfig,lipsum}

\usepackage[export]{adjustbox}
% \usepackage[square,sort,comma,numbers]{natbib}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{booktabs}
%\usepackage{algorithmic}
%\usepackage{algorithm}
\usepackage{textcomp}
\usepackage{mathtools,xparse}

\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{algpseudocode}

\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{pdfpages}
\usepackage{glossaries}

\usepackage{listings}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

\usepackage{subcaption}
\usepackage{float}
\usepackage{stfloats}

\usepackage{lscape}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

\renewcommand{\algorithmiccomment}[1]{$\triangleright$\textit{#1}}


\usepackage{lipsum} % Only used to generate random text.
% \usepackage{natbib}
% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

% The "axessiblity" package can be found at: https://ctan.org/pkg/axessibility?lang=en
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

\usepackage{bm}


\newcommand{\TODO}[1]{\textbf{\color{cyan}[TODO: #1]}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Thesis meta-information
%%

%% The title of the thesis:
\title{Data-efficient Neural\\
Appearance Manipulations}

%% The full name of the author (e.g.: James Smith):
\author{Fazilet Gokbudak}

%% College affiliation:
\college{Queens' College}

%% College shield [optional]:
% \collegeshield{CollegeShields/Christs}
% \collegeshield{CollegeShields/Churchill}
% \collegeshield{CollegeShields/Clare}
% \collegeshield{CollegeShields/ClareHall}
% \collegeshield{CollegeShields/CorpusChristi}
% \collegeshield{CollegeShields/Darwin}
% \collegeshield{CollegeShields/Downing}
% \collegeshield{CollegeShields/Emmanuel}
% \collegeshield{CollegeShields/Fitzwilliam}
% \collegeshield{CollegeShields/Girton}
% \collegeshield{CollegeShields/GonCaius}
% \collegeshield{CollegeShields/Homerton}
% \collegeshield{CollegeShields/HughesHall}
% \collegeshield{CollegeShields/Jesus}
% \collegeshield{CollegeShields/Kings}
% \collegeshield{CollegeShields/LucyCavendish}
% \collegeshield{CollegeShields/Magdalene}
% \collegeshield{CollegeShields/MurrayEdwards}
% \collegeshield{CollegeShields/Newnham}
% \collegeshield{CollegeShields/Pembroke}
% \collegeshield{CollegeShields/Peterhouse}
\collegeshield{CollegeShields/Queens}
% \collegeshield{CollegeShields/Robinson}
% \collegeshield{CollegeShields/Selwyn}
% \collegeshield{CollegeShields/SidneySussex}
% \collegeshield{CollegeShields/StCatharines}
% \collegeshield{CollegeShields/StEdmunds}
% \collegeshield{CollegeShields/StJohns}
% \collegeshield{CollegeShields/Trinity}
% \collegeshield{CollegeShields/TrinityHall}
% \collegeshield{CollegeShields/Wolfson}
% \collegeshield{CollegeShields/FitzwilliamRed}

%% Submission date [optional]:
% \submissiondate{November, 2042}

%% You can redefine the submission notice [optional]:
% \submissionnotice{A badass thesis submitted on time for the Degree of PhD}

%% Declaration date:
\date{My Month, My Year}

%% PDF meta-info:
\subjectline{Computer Science}
\keywords{one two three}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Abstract:
%%
\abstract{%
Recent works on appearance manipulation leverage deep learning techniques to achieve high-quality edits. However, such methods often rely on large amounts of data, making them undesirable in data-scarce regimes. In this thesis, I explore neural appearance manipulation techniques specifically designed for data-limited environments. More specifically, I aim to constrain the solution space of editing tasks with limited data through priors that are either imposed or learned through training.

I begin by exploring implicit appearance manipulations that work directly in the image domain without explicit information about the appearance, such as reflectance or illumination. Focusing on a low-level detail retouching task, I propose a machine learning framework that learns the style from only a single image pair. This is achieved by leveraging priors observed in how photographers manually edit photos. Continuing within the image domain, I later explore a more complex appearance manipulation task: transient attribute transfer, utilising a diffusion model in a zero-shot setting.

Despite their effectiveness and efficiency, working in the image domain presents challenges regarding controllability over appearance. For instance, editing each material individually within an image is complicated, requiring an additional segmentation algorithm to separate the materials \cite{}. Furthermore, patch-space information is intertwined with illumination, which is difficult to estimate \cite{}. Therefore, as an alternative to implicit editing, I explore explicit appearance modelling that aims for sparse reconstruction with real-world applications in physically-based rendering systems. I propose a neural material representation model that excels in generalising to new materials, editing existing appearances, and compressing them. This model can be integrated into either a rendering or capture system. While rendering offers finer control capabilities in scene editing, capture systems serve as the engine for such rendering processes.

In summary, my thesis focuses on neural network-based appearance editing suitable for data-scarce problem spaces. Here, I explore scene manipulation in two distinct aspects: 1) image-based implicit editing, encompassing both low-level and high-level editing, and 2) explicit appearance modelling with a neural Bidirectional Reflectance Distribution Function (BRDF) estimation. These approaches have the potential to significantly advance research in efficient and realistic appearance editing, offering valuable contributions to the research community working on similar subjects.

}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Acknowledgements:
%%
\acknowledgements{%

}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Glossary [optional]:
\makeglossaries

\newglossaryentry{AR/VR}{
    name=AR/VR,
    description={Augmented and Virtual Reality}
}

\newglossaryentry{BRDF}{
    name=BRDF,
    description={Bidirectional Reflectance Distribution Function}
}

\newglossaryentry{PBR}{
    name=PBR,
    description={Physics-based Rendering}
}


\newglossaryentry{sRGB}{
    name=sRGB,
    description={Standard Red Green Blue}
}

\newglossaryentry{RGBA}{
    name=RGBA,
    description={Red Green Blue Alpha}
}

\newglossaryentry{HDR}{
    name=HDR,
    description={High Dynamic Range}
}

\newglossaryentry{SVBRDF}{
    name=SVBRDF,
    description={Spatially Varying Bidirectional Reflectance Distribution Function}
}

\newglossaryentry{BSSRDF}{
    name=BSSRDF,
    description={Bidirectional Surface Scattering Reflectance Distribution Function}
}

\newglossaryentry{ML}{
    name=ML,
    description={Machine Learning}
}

\newglossaryentry{AI}{
    name=AI,
    description={Artificial Intelligence}
}

\newglossaryentry{SGD}{
    name=SGD,
    description={Stochastic Gradient Descent}
}

\newglossaryentry{Adam}{
    name=Adam,
    description={Adaptive Moment Estimation}
}

\newglossaryentry{MAE}{
    name=MAE,
    description={Mean Absolute Error}
}

\newglossaryentry{MSE}{
    name=MSE,
    description={Mean Squared Error}
}

\newglossaryentry{DNN}{
    name=DNN,
    description={Deep Neural Networks}
}

\newglossaryentry{MLP}{
    name=MLP,
    description={Multilayer Perceptron}
}


\newglossaryentry{ReLU}{
    name=ReLU,
    description={Rectified linear function}
}

\newglossaryentry{GAN}{
    name=GAN,
    description={Generative Adversarial Network}
}


\newglossaryentry{ELU}{
    name=ELU,
    description={Exponential linear function}
}


\newglossaryentry{ISP}{
    name=ISP,
    description={Image Signal Processor}
}

\newglossaryentry{CNN}{
    name=CNN,
    description={Convolutional Neural Network}
}

\newglossaryentry{PSNR}{
    name=PSNR,
    description={Peak Signal-to-Noise Ratio}
}

\newglossaryentry{SSIM}{
    name=SSIM,
    description={Structural Similarity Index Measure}
}


\newglossaryentry{LLF}{
    name=LLF,
    description={Local Laplacian Filter}
}


\newglossaryentry{GNF}{
    name=GNF,
    description={Generalisable Neural Field}
}


\newglossaryentry{HVS}{
    name=HVS,
    description={Human Visual System}
}

\newglossaryentry{DDIM}{
    name=DDIM,
    description={Denoising Diffusion Implicit Models}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Contents:
%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Title page, abstract, declaration etc.:
%% -    the title page (is automatically omitted in the technical report mode).
\frontmatter{}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Thesis body:
%%


\input{Chapters/Introduction}
\input{Chapters/Background}
\input{Chapters/DetailRetouching}
\input{Chapters/ZeroshotDiffusion}
\input{Chapters/HyperBRDF}


\chapter{Conclusion and Future Work}
This dissertation explores the appearance of a 3D scene from multiple perspectives, starting with low-level features in a rendered image and progressing towards the multifaceted nature of appearance that defines the light-material interactions. How our eyes perceive an object is shaped by the blending of 3D scene components, namely illumination, material properties, textures and the geometry. By investigating the effect of their roles in the formation of appearance, I developed learning-based techniques that allow efficient and accurate manipulation of appearance-related scene properties. Albeit their effectiveness to solve such ill-posed problems that we often come across in appearance editing, traditional methods still require significant time and skill. On the other hand, machine learning has revolutionised computer vision and graphics, enabling the capture of highly nonlinear and challenging edits, while maintaining the photorealism of the rendered scene. This thesis proposes learning-based techniques that aim for attaining high-quality manipulations with maximal data efficiency, introducing practical solutions for domains where appearance manipulations are essential, such as filmmaking, gaming, \gls{AR/VR} and product design.

The contributions of this thesis cover the core problems in appearance manipulation, particularly the scarcity of training data, the maintenance of photorealism throughout appearance transformations, and the challenges of control over scene features. Focusing on learning-based approaches that can generalise well to new data even when trained on a limited number of examples, I presented methods that leverage the priors that are either learned or imposed through architecture design to overcome the bottlenecks of traditional and machine learning-based techniques. For example, the \gls{ML} framework introduced in Chapter \ref{one-shot} is designed to mimic manual retouching techniques produced by artists/photographers, which, as a prior, helps constrain the solution space of the learning problem. Furthermore, HyperBRDF embodies the learned priors in a latent embedding that strengthens the sparse reconstruction ability of the model, while transient attribute transfer employs a pre-trained diffusion model as a learned prior to capture highly entangled appearance transitions. The proposed methods push the limits of what is possible with data-driven approaches by offering precise control over intricate details, transient attributes, and reflectance, opening the door to several new possibilities in the study of realistic and effective appearance manipulation.


Future work could expand upon the foundational methods introduced in this thesis by addressing certain limitations and exploring new areas for enhancement. One area for improvement is the handling of complex, non-repeating local effects such as tattoos or spatially varying lighting conditions, which currently pose a challenge for the proposed techniques. Developing more advanced spatial-awareness capabilities within the models could improve performance in these scenarios. Additionally, integrating multi-view or multi-modal data inputs would allow for even more accurate transformations, particularly in scenarios where aligning input and example images is difficult.

Another promising direction for future research is the extension of the proposed methods beyond single-pair example training to incorporate multiple examples for more robust and varied appearance manipulations. This could involve exploring new architectures or optimisation strategies that can efficiently handle multiple example pairs while maintaining consistent retouching results. Furthermore, incorporating new developments in real-time rendering and neural rendering could make the proposed techniques even more practical for interactive applications, such as video game development or virtual reality environments, where performance is critical.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Bibliography:
%%
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{plainnat}
\bibliography{thesis}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Appendix:
%%

\appendix

\input{Chapters/Appendix-One-Shot}

\input{Chapters/Appendix-TAT}

\input{Chapters/Appendix-HyperBRDF}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Index:
%%
\printthesisindex

\end{document}
