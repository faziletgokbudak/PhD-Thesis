%%%%%
%%
%% Sample document ``thesis.tex''
%%
%% Version: v0.2
%% Authors: Jean Martina, Rok Strnisa, Matej Urbas
%% Date: 30/07/2008
%%
%% Copyright (c) 2008-2011, Rok Strni≈°a, Jean Martina, Matej Urbas
%% License: Simplified BSD License
%% License file: ./License
%% Original License URL: http://www.freebsd.org/copyright/freebsd-license.html
%%%%%

% Available documentclass options:
%
%   <all `report` document class options, e.g.: `a5paper`>
%   withindex   - enables the index. New index entries can be added through `\index{my entry}`
%   glossary    - enables the glossary.
%   techreport  - typesets the thesis in the technical report format.
%   firstyr     - formats the document as a first-year report.
%   times       - uses the `Times` font.
%   backrefs    - add back references in the Bibliography section
%
% For more info see `README.md`

\documentclass[withindex, glossary]{cam-thesis}
\usepackage[table]{xcolor}% http://ctan.org/pkg/xcolor

% Citations using numbers
\usepackage[numbers]{natbib}
% \usepackage[dvipsnames]{xcolor}
\usepackage{graphicx,wrapfig,lipsum}

\usepackage[export]{adjustbox}
% \usepackage[square,sort,comma,numbers]{natbib}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{booktabs}
%\usepackage{algorithmic}
%\usepackage{algorithm}
\usepackage{textcomp}
\usepackage{mathtools,xparse}

\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{algpseudocode}

\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{pdfpages}
\usepackage{glossaries}

\usepackage{listings}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

\usepackage{subcaption}
\usepackage{float}
\usepackage{stfloats}

\usepackage{lscape}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

\renewcommand{\algorithmiccomment}[1]{$\triangleright$\textit{#1}}


\usepackage{lipsum} % Only used to generate random text.
% \usepackage{natbib}
% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

% The "axessiblity" package can be found at: https://ctan.org/pkg/axessibility?lang=en
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

\usepackage{bm}


\newcommand{\TODO}[1]{\textbf{\color{cyan}[TODO: #1]}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Thesis meta-information
%%

%% The title of the thesis:
\title{Data-efficient Neural\\
Appearance Manipulations}

%% The full name of the author (e.g.: James Smith):
\author{Fazilet Gokbudak}

%% College affiliation:
\college{Queens' College}

%% College shield [optional]:
% \collegeshield{CollegeShields/Christs}
% \collegeshield{CollegeShields/Churchill}
% \collegeshield{CollegeShields/Clare}
% \collegeshield{CollegeShields/ClareHall}
% \collegeshield{CollegeShields/CorpusChristi}
% \collegeshield{CollegeShields/Darwin}
% \collegeshield{CollegeShields/Downing}
% \collegeshield{CollegeShields/Emmanuel}
% \collegeshield{CollegeShields/Fitzwilliam}
% \collegeshield{CollegeShields/Girton}
% \collegeshield{CollegeShields/GonCaius}
% \collegeshield{CollegeShields/Homerton}
% \collegeshield{CollegeShields/HughesHall}
% \collegeshield{CollegeShields/Jesus}
% \collegeshield{CollegeShields/Kings}
% \collegeshield{CollegeShields/LucyCavendish}
% \collegeshield{CollegeShields/Magdalene}
% \collegeshield{CollegeShields/MurrayEdwards}
% \collegeshield{CollegeShields/Newnham}
% \collegeshield{CollegeShields/Pembroke}
% \collegeshield{CollegeShields/Peterhouse}
\collegeshield{CollegeShields/Queens}
% \collegeshield{CollegeShields/Robinson}
% \collegeshield{CollegeShields/Selwyn}
% \collegeshield{CollegeShields/SidneySussex}
% \collegeshield{CollegeShields/StCatharines}
% \collegeshield{CollegeShields/StEdmunds}
% \collegeshield{CollegeShields/StJohns}
% \collegeshield{CollegeShields/Trinity}
% \collegeshield{CollegeShields/TrinityHall}
% \collegeshield{CollegeShields/Wolfson}
% \collegeshield{CollegeShields/FitzwilliamRed}

%% Submission date [optional]:
% \submissiondate{November, 2042}

%% You can redefine the submission notice [optional]:
% \submissionnotice{A badass thesis submitted on time for the Degree of PhD}

%% Declaration date:
\date{09, 2024}

%% PDF meta-info:
\subjectline{Computer Science}
\keywords{one two three}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Abstract:
%%
\abstract{%
%Recent works on appearance manipulation leverage deep learning techniques to achieve high-quality edits. However, such methods often rely on large amounts of data, making them undesirable in data-scarce regimes. In this thesis, I explore neural appearance manipulation techniques specifically designed for data-limited environments. More specifically, I aim to constrain the solution space of editing tasks with limited data through priors that are either imposed or learned through training.

%I begin by exploring implicit appearance manipulations that work directly in the image domain without explicit information about the appearance, such as reflectance or illumination. Focusing on a low-level detail retouching task, I propose a machine learning framework that learns the style from only a single image pair. This is achieved by leveraging priors observed in how photographers manually edit photos. Continuing within the image domain, I later explore a more complex appearance manipulation task: transient attribute transfer, utilising a diffusion model in a zero-shot setting.

%Despite their effectiveness and efficiency, working in the image domain presents challenges regarding controllability over appearance. For instance, editing each material individually within an image is complicated, requiring an additional segmentation algorithm to separate the materials \cite{}. Furthermore, patch-space information is intertwined with illumination, which is difficult to estimate \cite{}. Therefore, as an alternative to implicit editing, I explore explicit appearance modelling that aims for sparse reconstruction with real-world applications in physically-based rendering systems. I propose a neural material representation model that excels in generalising to new materials, editing existing appearances, and compressing them. This model can be integrated into either a rendering or capture system. While rendering offers finer control capabilities in scene editing, capture systems serve as the engine for such rendering processes.

%In summary, my thesis focuses on neural network-based appearance editing suitable for data-scarce problem spaces. Here, I explore scene manipulation in two distinct aspects: 1) image-based implicit editing, encompassing both low-level and high-level editing, and 2) explicit appearance modelling with a neural Bidirectional Reflectance Distribution Function (BRDF) estimation. These approaches have the potential to significantly advance research in efficient and realistic appearance editing, offering valuable contributions to the research community working on similar subjects.


Recent works on appearance manipulation leverage deep learning techniques to achieve high-quality edits. However, such methods often rely on vast amounts of data, making them less effective in data-scarce scenarios. In this thesis, I explore neural appearance manipulation techniques specifically designed for data-limited environments. More precisely, I aim to constrain the solution space of editing tasks with limited data through priors, which are either imposed or learned during training.

I begin by investigating implicit appearance manipulations that operate directly within the image domain, without requiring explicit information about appearance attributes such as reflectance or illumination. Focusing on low-level detail retouching, I propose a machine learning framework that learns the editing style from only a single image pair. This is achieved by leveraging priors observed in how photographers manually edit photos. I then extend this approach to a more complex task: transient attribute transfer, employing a diffusion model in a zero-shot setting to explore higher-level appearance manipulations.

Despite their effectiveness, working in the image domain poses challenges in controlling appearance, such as the difficulty of editing individual materials, which often requires an additional segmentation algorithm to isolate different materials, or the entanglement with illumination, which is difficult to estimate due to the ill-posed nature of the problem. To address these limitations, I further explore explicit appearance modelling aimed at sparse reconstruction, with real-world applications in physically-based rendering systems. I propose a neural material representation model that excels at generalising to new materials, editing existing appearances, and compressing them. This model can be integrated into rendering or capture systems, where rendering offers more precise control over scene editing, and capture systems serve as the foundation for such rendering processes.

In summary, my thesis focuses on neural network-based appearance editing tailored for data-scarce problem spaces. I explore scene manipulation from two distinct perspectives: 1) image-based implicit editing, encompassing both low-level and high-level edits, and 2) explicit appearance modelling using neural Bidirectional Reflectance Distribution Function (BRDF) estimation. These approaches have the potential to advance research in efficient and realistic appearance editing, offering significant contributions to the research community engaged in similar work.

}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Acknowledgements:
%%
\acknowledgements{%

}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Glossary [optional]:
\makeglossaries

\newglossaryentry{AR/VR}{
    name=AR/VR,
    description={Augmented and Virtual Reality}
}

\newglossaryentry{BRDF}{
    name=BRDF,
    description={Bidirectional Reflectance Distribution Function}
}

\newglossaryentry{PBR}{
    name=PBR,
    description={Physics-based Rendering}
}


\newglossaryentry{sRGB}{
    name=sRGB,
    description={Standard Red Green Blue}
}

\newglossaryentry{RGBA}{
    name=RGBA,
    description={Red Green Blue Alpha}
}

\newglossaryentry{HDR}{
    name=HDR,
    description={High Dynamic Range}
}

\newglossaryentry{SVBRDF}{
    name=SVBRDF,
    description={Spatially Varying Bidirectional Reflectance Distribution Function}
}

\newglossaryentry{BSSRDF}{
    name=BSSRDF,
    description={Bidirectional Surface Scattering Reflectance Distribution Function}
}

\newglossaryentry{ML}{
    name=ML,
    description={Machine Learning}
}

\newglossaryentry{AI}{
    name=AI,
    description={Artificial Intelligence}
}

\newglossaryentry{SGD}{
    name=SGD,
    description={Stochastic Gradient Descent}
}

\newglossaryentry{Adam}{
    name=Adam,
    description={Adaptive Moment Estimation}
}

\newglossaryentry{MAE}{
    name=MAE,
    description={Mean Absolute Error}
}

\newglossaryentry{MSE}{
    name=MSE,
    description={Mean Squared Error}
}

\newglossaryentry{DNN}{
    name=DNN,
    description={Deep Neural Networks}
}

\newglossaryentry{MLP}{
    name=MLP,
    description={Multilayer Perceptron}
}


\newglossaryentry{ReLU}{
    name=ReLU,
    description={Rectified linear function}
}

\newglossaryentry{GAN}{
    name=GAN,
    description={Generative Adversarial Network}
}


\newglossaryentry{ELU}{
    name=ELU,
    description={Exponential linear function}
}


\newglossaryentry{ISP}{
    name=ISP,
    description={Image Signal Processor}
}

\newglossaryentry{CNN}{
    name=CNN,
    description={Convolutional Neural Network}
}

\newglossaryentry{PSNR}{
    name=PSNR,
    description={Peak Signal-to-Noise Ratio}
}

\newglossaryentry{SSIM}{
    name=SSIM,
    description={Structural Similarity Index Measure}
}


\newglossaryentry{LLF}{
    name=LLF,
    description={Local Laplacian Filter}
}


\newglossaryentry{GNF}{
    name=GNF,
    description={Generalisable Neural Field}
}


\newglossaryentry{HVS}{
    name=HVS,
    description={Human Visual System}
}

\newglossaryentry{DDIM}{
    name=DDIM,
    description={Denoising Diffusion Implicit Models}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Contents:
%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Title page, abstract, declaration etc.:
%% -    the title page (is automatically omitted in the technical report mode).
\frontmatter{}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Thesis body:
%%


\input{Chapters/Introduction}
\input{Chapters/Background}
\input{Chapters/DetailRetouching}
\input{Chapters/ZeroshotDiffusion}
\input{Chapters/HyperBRDF}


\chapter{Conclusion and Future Work}
This dissertation explores the appearance of a 3D scene from multiple perspectives, beginning with low-level features in a rendered image and progressing towards the multifaceted nature of appearance that defines light-material interactions. How our eyes perceive an object is shaped by the blending of 3D scene components, namely illumination, material properties, textures, and geometry. By investigating the role each of these elements plays in the formation of appearance, I developed learning-based techniques that allow for efficient and accurate manipulation of appearance-related scene properties. Although traditional methods are effective at solving the ill-posed problems often encountered in appearance editing, they still require significant time and skill. In contrast, machine learning has revolutionised computer vision and graphics, enabling the capture of highly non-linear and complex edits while maintaining the photorealism of the rendered scene. This thesis proposes learning-based techniques that aim to achieve high-quality manipulations with maximal data efficiency, introducing practical solutions for fields where appearance manipulations are essential, such as filmmaking, gaming, augmented/virtual reality (\gls{AR/VR}), and product design.

The contributions of this thesis address core problems in appearance manipulation, particularly the scarcity of training data, the preservation of photorealism throughout appearance transformations, and the challenges of controlling scene features. Focusing on learning-based approaches that can generalise well to new data, even when trained on a limited number of examples, I present methods that leverage priors that are either learned or imposed through architectural design to overcome the limitations of traditional and machine learning-based techniques. For instance, the \gls{ML} framework introduced in Chapter \ref{one-shot} is designed to mimic manual retouching techniques produced by artists and photographers, which, as a prior, helps to constrain the solution space of the learning problem. Additionally, HyperBRDF embodies learned priors in a latent embedding, enhancing the model's ability for sparse reconstruction, while transient attribute transfer utilises a pre-trained diffusion model as a learned prior to capture highly complex appearance transitions. The proposed methods push the boundaries of data-driven approaches by offering precise control over intricate details, transient attributes, and reflectance, thereby opening the door to several new possibilities in the study of realistic and effective appearance manipulation.

Considering the recent and rapid growth in the field of photorealistic 3D scene generation, future work could expand upon the fundamental approach introduced in this thesis for the study of appearance. This dissertation demonstrates that the physics-based nature of appearance is crucial to the success of photorealistic appearance manipulations, proposing HyperBRDF as an explicit approach to appearance representation and editing. A promising direction for future research is to extend the methods introduced for detail retouching and transient attribute transfer beyond image-based implicit editing, incorporating a scene-level understanding of texture and illumination for finer control over appearance. This could involve exploring new architectures or optimisation strategies that can extract intricate texture details or accurately estimate the intrinsic components of images, such as shading and reflectance, while maintaining data efficiency. Furthermore, integrating the proposed techniques into real-time and neural rendering pipelines could make them even more practical for interactive applications, where data is scarce and performance is critical.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Bibliography:
%%
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{plainnat}
\bibliography{thesis}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Appendix:
%%
\let\cleardoublepage\clearpage
\appendix

\input{Chapters/Appendix-One-Shot}

\input{Chapters/Appendix-TAT}

\input{Chapters/Appendix-HyperBRDF}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Index:
%%
\printthesisindex

\end{document}
