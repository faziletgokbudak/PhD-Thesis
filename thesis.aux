\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{thesis.ist}
\@glsorder{word}
\providecommand \oddpage@label [2]{}
\citation{}
\citation{}
\@writefile{toc}{\thispagestyle {empty}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{13}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Overview}{13}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}One-shot detail retouching}{15}{subsection.1.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces A close-up scene from the movie ‘The Good, the Bad and the Ugly’ (1966). Details can convey valuable information about a scene, such as wrinkles hinting at the character's age in this shot. Image courtesy of [Produzioni Europee Associati].}}{15}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:close-up1}{{1.1}{15}{A close-up scene from the movie ‘The Good, the Bad and the Ugly’ (1966). Details can convey valuable information about a scene, such as wrinkles hinting at the character's age in this shot. Image courtesy of [Produzioni Europee Associati]}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Another close-up shot example. In photography, captivating details contribute to aesthetics, highlighting the identity of the subject.}}{16}{figure.caption.6}\protected@file@percent }
\newlabel{fig:close-up2}{{1.2}{16}{Another close-up shot example. In photography, captivating details contribute to aesthetics, highlighting the identity of the subject}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Text-guided transient attribute transfer}{16}{subsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Generalisable Neural BRDF Representation}{16}{subsection.1.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Seasonal transitions of a tree throughout the year. Image from [Michael Melford/Getty Images]}}{17}{figure.caption.7}\protected@file@percent }
\newlabel{fig:colour-approximate}{{1.3}{17}{Seasonal transitions of a tree throughout the year. Image from [Michael Melford/Getty Images]}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Contributions and publications}{17}{section.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Photorealistic rendering can be achieved through extensive ray tracing for which reflectance should be accurately measured and represented. Image from [Star Wars Real-Time Ray Tracing Demo by Epic Games, NVIDIA, and ILMxLAB].}}{18}{figure.caption.8}\protected@file@percent }
\newlabel{fig:brdf-intro}{{1.4}{18}{Photorealistic rendering can be achieved through extensive ray tracing for which reflectance should be accurately measured and represented. Image from [Star Wars Real-Time Ray Tracing Demo by Epic Games, NVIDIA, and ILMxLAB]}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{21}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Rendering}{21}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Displaying an image}{21}{subsection.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A pixel, a picture element, is the smallest unit of a rendered image. Displaying an image with fewer pixels causes losses in details as one pixel starts covering a larger area in the 3D scene.}}{21}{figure.caption.9}\protected@file@percent }
\newlabel{fig:colour-approximate}{{2.1}{21}{A pixel, a picture element, is the smallest unit of a rendered image. Displaying an image with fewer pixels causes losses in details as one pixel starts covering a larger area in the 3D scene}{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces To compute the colour of a pixel with variations, it is common to divide the pixel into sub-pixels and take the weighted average of each sub-pixel.}}{22}{figure.caption.10}\protected@file@percent }
\newlabel{fig:display-grid}{{2.2}{22}{To compute the colour of a pixel with variations, it is common to divide the pixel into sub-pixels and take the weighted average of each sub-pixel}{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Comparison of different colour gamuts. Image from [Schewe and Fraser, 2007].}}{22}{figure.2.3}\protected@file@percent }
\newlabel{fig:colour-gamut}{{2.3}{22}{Comparison of different colour gamuts. Image from [Schewe and Fraser, 2007]}{figure.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces When a colour is represented with too few bits, the transition between colours appears as discrete steps, known as colour banding (right).}}{23}{figure.caption.11}\protected@file@percent }
\newlabel{fig:colour-band}{{2.4}{23}{When a colour is represented with too few bits, the transition between colours appears as discrete steps, known as colour banding (right)}{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Rendering an image of a 3D scene requires lighting, viewpoint (camera) and geometry. Image from [Boss et al., 2021].}}{24}{figure.caption.12}\protected@file@percent }
\newlabel{fig:teaser}{{2.5}{24}{Rendering an image of a 3D scene requires lighting, viewpoint (camera) and geometry. Image from [Boss et al., 2021]}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}3D scene components}{24}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Geometry.}{24}{section*.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Triangulation of surfaces is a commonly used method in computer graphics to define the shape of objects. It allows the creation of complex shapes. Increasing the number of triangles also increases the resolution of the shape that leads to smoother surface representations. Image taken from [Broeren et al., 2019].}}{25}{figure.caption.14}\protected@file@percent }
\newlabel{fig:triangulation}{{2.6}{25}{Triangulation of surfaces is a commonly used method in computer graphics to define the shape of objects. It allows the creation of complex shapes. Increasing the number of triangles also increases the resolution of the shape that leads to smoother surface representations. Image taken from [Broeren et al., 2019]}{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Photo-realistic rendering}{26}{subsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Perspective projection and visibility.}{26}{section*.15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Perspective projection mimics the foreshortening effect our visual system creates while observing a scene. The image plane/canvas includes the 2D projections of the visible objects within the view frustum.}}{27}{figure.caption.16}\protected@file@percent }
\newlabel{fig:perspective_projection}{{2.7}{27}{Perspective projection mimics the foreshortening effect our visual system creates while observing a scene. The image plane/canvas includes the 2D projections of the visible objects within the view frustum}{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Single-point vs two-point perspective. Computer graphics mimics our visual system, focusing on single-point projection. However, multi-point perspective is widely explored in artistic drawing.}}{27}{figure.caption.17}\protected@file@percent }
\newlabel{fig:artistic_drawing}{{2.8}{27}{Single-point vs two-point perspective. Computer graphics mimics our visual system, focusing on single-point projection. However, multi-point perspective is widely explored in artistic drawing}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {paragraph}{Appearance.}{28}{section*.18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces The reflected light from the surface of an object defines its colour. Image from \href  {http://www.mstworkbooks.co.za/natural-sciences/gr8/images/gr8ec04-gd-0027.png}{website link}.}}{28}{figure.2.9}\protected@file@percent }
\newlabel{fig:object-colour}{{2.9}{28}{The reflected light from the surface of an object defines its colour. Image from \href {http://www.mstworkbooks.co.za/natural-sciences/gr8/images/gr8ec04-gd-0027.png}{website link}}{figure.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces The ripples on the water's surface cause a blurred image of the bridge, acting as a rough surface.}}{28}{figure.2.10}\protected@file@percent }
\newlabel{fig:water_reflection}{{2.10}{28}{The ripples on the water's surface cause a blurred image of the bridge, acting as a rough surface}{figure.2.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Reflection on a surface with microfacets vs smooth surface. Image from [Guy and Agopian, 2024]}}{29}{figure.caption.19}\protected@file@percent }
\newlabel{fig:microfacet}{{2.11}{29}{Reflection on a surface with microfacets vs smooth surface. Image from [Guy and Agopian, 2024]}{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Diffuse surfaces often have complex internal structures that cause the light to be reflected multiple times underneath the surface.}}{29}{figure.2.12}\protected@file@percent }
\newlabel{fig:diffuse-scattering}{{2.12}{29}{Diffuse surfaces often have complex internal structures that cause the light to be reflected multiple times underneath the surface}{figure.2.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Specular vs diffuse reflection. Diffuse surfaces cause rays to scatter randomly, brightening the object equally in all viewing directions. Specular reflection is, on the other hand, view-dependent, reflected rays concentrated around the mirror reflection. Image from [Park and Baek, 2021].}}{30}{figure.caption.20}\protected@file@percent }
\newlabel{fig:specularvsdiffuse}{{2.13}{30}{Specular vs diffuse reflection. Diffuse surfaces cause rays to scatter randomly, brightening the object equally in all viewing directions. Specular reflection is, on the other hand, view-dependent, reflected rays concentrated around the mirror reflection. Image from [Park and Baek, 2021]}{figure.caption.20}{}}
\@writefile{toc}{\contentsline {paragraph}{Ray tracing - Light transport simulation.}{31}{section*.21}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Ray tracing simulate the light's journey to compute the colour of each pixel on the rendered image.Image from [Henrik, 2008].}}{32}{figure.caption.22}\protected@file@percent }
\newlabel{fig:raytracing}{{2.14}{32}{Ray tracing simulate the light's journey to compute the colour of each pixel on the rendered image.Image from [Henrik, 2008]}{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Shaders and BRDF}{32}{subsection.2.1.4}\protected@file@percent }
\citation{Matusik2003jul}
\citation{Matusik2003jul}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces A material composed of both specular and diffuse components, lit by a point light source. The specular highlight (white circle) is centered around the light source direction. The material is specular-maroon-phenolic from MERL dataset \cite  {Matusik2003jul}.}}{33}{figure.2.15}\protected@file@percent }
\newlabel{fig:diffuse+spec}{{2.15}{33}{A material composed of both specular and diffuse components, lit by a point light source. The specular highlight (white circle) is centered around the light source direction. The material is specular-maroon-phenolic from MERL dataset \cite {Matusik2003jul}}{figure.2.15}{}}
\citation{phong1998illumination}
\citation{cooktorrance1982}
\citation{ward1992}
\citation{walter2007microfacet}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Diffuse and specular reflections.. Figures from [Mantiuk, 2022]}}{34}{figure.caption.23}\protected@file@percent }
\newlabel{fig:diffuse-spec-angle}{{2.16}{34}{Diffuse and specular reflections.. Figures from [Mantiuk, 2022]}{figure.caption.23}{}}
\newlabel{eq:Phong-eq}{{2.4}{34}{Shaders and BRDF}{equation.2.1.4}{}}
\citation{Matusik2003jul}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces Phong shading model implementation with decreasing roughness/increasing glossiness from left to right.}}{35}{figure.caption.24}\protected@file@percent }
\newlabel{fig:phong-roughness}{{2.17}{35}{Phong shading model implementation with decreasing roughness/increasing glossiness from left to right}{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces BRDF depends on the incident light and viewing direction.}}{35}{figure.2.18}\protected@file@percent }
\newlabel{fig:brdf}{{2.18}{35}{BRDF depends on the incident light and viewing direction}{figure.2.18}{}}
\@writefile{toc}{\contentsline {paragraph}{BRDF.}{35}{section*.25}\protected@file@percent }
\citation{Matusik2003jul}
\citation{rusinkiewicz1998new}
\citation{rusinkiewicz1998new}
\citation{dupuy2018adaptive}
\citation{dupuy2018adaptive}
\citation{rusinkiewicz1998new}
\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces Standard (left) vs Rusinkiewicz parametrisation. Figure from [Rusinkiewicz, 1998]}}{36}{figure.caption.26}\protected@file@percent }
\newlabel{fig:RusinkiewiczvsStandard}{{2.19}{36}{Standard (left) vs Rusinkiewicz parametrisation. Figure from [Rusinkiewicz, 1998]}{figure.caption.26}{}}
\citation{Chenliang's paper}
\citation{Chenliang's paper}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Rendering equation}{37}{subsection.2.1.5}\protected@file@percent }
\newlabel{eqn:rendering-eqn}{{2.5}{37}{Rendering equation}{equation.2.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Machine learning}{38}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Foundations}{38}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Types of learning.}{38}{section*.27}\protected@file@percent }
\citation{kingma2014adam}
\@writefile{toc}{\contentsline {paragraph}{Training.}{39}{section*.28}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.20}{\ignorespaces Gradient descent steps.}}{39}{figure.2.20}\protected@file@percent }
\newlabel{fig:gradient-descent}{{2.20}{39}{Gradient descent steps}{figure.2.20}{}}
\citation{johnson2016perceptuallossesrealtimestyle}
\@writefile{toc}{\contentsline {paragraph}{Loss function.}{40}{section*.29}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.21}{\ignorespaces L1 vs L2 loss values for a single data point. Ground truth $y$ is set to 0.}}{41}{figure.caption.30}\protected@file@percent }
\newlabel{fig:l1vsl2loss}{{2.21}{41}{L1 vs L2 loss values for a single data point. Ground truth $y$ is set to 0}{figure.caption.30}{}}
\@writefile{toc}{\contentsline {paragraph}{Regularisation.}{41}{section*.31}\protected@file@percent }
\newlabel{MSE-with-L2reg}{{2.6}{41}{Regularisation}{equation.2.2.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Data augmentation}{41}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Multilayer perceptrons}{42}{subsection.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.22}{\ignorespaces Graph representation of a linear equation. Figure from [Issac, 2018].}}{42}{figure.2.22}\protected@file@percent }
\newlabel{fig:neuron}{{2.22}{42}{Graph representation of a linear equation. Figure from [Issac, 2018]}{figure.2.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.23}{\ignorespaces A basic multilayer perceptron model. Figure from [Kishore NG, 2023].}}{43}{figure.caption.33}\protected@file@percent }
\newlabel{fig:mlp}{{2.23}{43}{A basic multilayer perceptron model. Figure from [Kishore NG, 2023]}{figure.caption.33}{}}
\citation{cybenko1989approximation}
\@writefile{toc}{\contentsline {paragraph}{Activation functions.}{44}{section*.34}\protected@file@percent }
\newlabel{eq:sine-act}{{2.11}{44}{Activation functions}{equation.2.2.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Neural implicit representations}{44}{subsection.2.2.3}\protected@file@percent }
\citation{ffn}
\citation{nguyen2015deep}
\citation{mildenhall2021nerf}
\citation{park2019deepsdf}
\citation{sztrajman2021neural}
\citation{rahaman2019spectral}
\citation{ffn}
\citation{ffn}
\citation{rahimi2007random}
\citation{sitzmann2020siren}
\citation{sitzmann2020siren}
\citation{ffn}
\citation{ffn}
\citation{rebain2022attention}
\newlabel{eq:ffn}{{2.12}{45}{Neural implicit representations}{equation.2.2.12}{}}
\citation{rebain2022attention}
\citation{rebain2022attention}
\citation{rebain2022attention}
\citation{rebain2022attention}
\@writefile{lof}{\contentsline {figure}{\numberline {2.24}{\ignorespaces To overcome the spectral bias in coordinate-based MLPs, a Fourier feature mapping \cite  {ffn} can be applied before feeding the input to the model. Here, 'none' represents an MLP model without mapping. 'basic' and 'gauss' are different variants of Fourier feature maps, and 'GT' is the ground truth.}}{46}{figure.caption.35}\protected@file@percent }
\newlabel{fig:ffn}{{2.24}{46}{To overcome the spectral bias in coordinate-based MLPs, a Fourier feature mapping \cite {ffn} can be applied before feeding the input to the model. Here, 'none' represents an MLP model without mapping. 'basic' and 'gauss' are different variants of Fourier feature maps, and 'GT' is the ground truth}{figure.caption.35}{}}
\@writefile{toc}{\contentsline {paragraph}{Generalisable neural implicit representations.}{46}{section*.36}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.25}{\ignorespaces Three conditioning mechanisms for generalisable neural representations. Coordinate-based MLPs can be conditioned by 1) the concatenation of input coordinates \textbf  {x} and latent code \textbf  {z} (left), 2) another neural network predicting the weights of the primary MLP which takes the inpur coordinates as the input (middle), or 3) attention layers with a set of tokens (\textbf  {z}) and coordinate-related query . Here, $\gamma $ is the mapping function discussed earlier and an optional component. Figure from [\citeauthor  {rebain2022attention}, \citeyear  {rebain2022attention}]}}{47}{figure.caption.37}\protected@file@percent }
\newlabel{fig:ffn}{{2.25}{47}{Three conditioning mechanisms for generalisable neural representations. Coordinate-based MLPs can be conditioned by 1) the concatenation of input coordinates \textbf {x} and latent code \textbf {z} (left), 2) another neural network predicting the weights of the primary MLP which takes the inpur coordinates as the input (middle), or 3) attention layers with a set of tokens (\textbf {z}) and coordinate-related query . Here, $\gamma $ is the mapping function discussed earlier and an optional component. Figure from [\citeauthor {rebain2022attention}, \citeyear {rebain2022attention}]}{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Diffusion models}{47}{subsection.2.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}One-shot Detail Retouching}{49}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{one-shot}{{3}{49}{One-shot Detail Retouching}{chapter.3}{}}
\citation{shaham2021spatially,li2020lapar}
\citation{moran2020deeplpf}
\citation{Gharbi17Deep}
\citation{yan2016automatic}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The one-shot technique automatically transfers retouching edits to new images by learning the desired edits from one example \textit  {before-after} pair (insets). The transferred edits accurately capture intricate details such as wrinkles, dark spots, strands of hair, or eyelashes, as shown in the input (top) and retouched (bottom) pairs. Images from [Jenavieve (top-left), Logan ProPro (top-left, inset), Marissa Oosterlee (top-middle). (CC-BY)].}}{50}{figure.caption.38}\protected@file@percent }
\newlabel{fig:teaser}{{3.1}{50}{The one-shot technique automatically transfers retouching edits to new images by learning the desired edits from one example \textit {before-after} pair (insets). The transferred edits accurately capture intricate details such as wrinkles, dark spots, strands of hair, or eyelashes, as shown in the input (top) and retouched (bottom) pairs. Images from [Jenavieve (top-left), Logan ProPro (top-left, inset), Marissa Oosterlee (top-middle). (CC-BY)]}{figure.caption.38}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{50}{section.3.1}\protected@file@percent }
\newlabel{sec:introduction}{{3.1}{50}{Introduction}{section.3.1}{}}
\citation{Faridul14ASurvey,mustafa2022distilling}
\citation{Bychkovsky11Learning,Bae06Two,Pitie05NDimensional,Pitie07Automated,Reinhard01Color,Sunkavalli10Multi,he2020conditional,park2018distort}
\citation{CohenOr06Color}
\citation{Bychkovsky11Learning}
\citation{Bychkovsky11Learning}
\citation{chen2017fast}
\citation{chen2017fast}
\citation{Hu18Exposure}
\citation{Hu18Exposure}
\citation{CohenOr06Color}
\citation{kim2021representative}
\citation{wang2019underexposed}
\citation{Shapira13Image}
\citation{Laffont14Transient,Tai07Soft}
\citation{Berthouzoz11AFramework,Chen18Deep,Huang14Parametric,Omiya18Learning,Saeedi18Multimodal}
\citation{An10User,Pouli11Progressive,Tai05Local}
\citation{Gharbi17Deep,Hwang12Context,Kaufman12Content,Nam17Deep,Yan14Automatic,Zhu18Automatic}
\citation{HaCohen11Nonrigid}
\citation{Kagarlitsky09Piecewise,Shih13Data}
\citation{moran2020deeplpf,Gharbi17Deep,chen2018deep,shaham2021spatially,li2020lapar}
\citation{chen2018deep}
\citation{chen2018deep}
\citation{chen2016bilateral}
\citation{Gharbi17Deep}
\citation{moran2020deeplpf}
\citation{moran2020deeplpf}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Related work}{52}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Digital photo enhancement}{52}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Global image enhancement.}{52}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Local context-aware image enhancement.}{52}{section*.40}\protected@file@percent }
\citation{Bae06Two}
\citation{HaCohen11Nonrigid}
\citation{Shih14Style}
\citation{Shih14Style}
\citation{tseng2019hyperparameter,yu2021reconfigisp}
\citation{tseng2019hyperparameter}
\citation{yu2021reconfigisp}
\citation{yu2021reconfigisp}
\citation{tseng2022neural}
\citation{tseng2022neural}
\citation{ma2021retinexgan}
\citation{yang2020fidelity}
\citation{9334429}
\citation{Liu16Makeup}
\citation{Frigo16Split}
\citation{Chen18Deep}
\citation{lin2020tuigan}
\citation{Zhang13Style,Hu18Exposure}
\@writefile{toc}{\contentsline {paragraph}{Differentiable image processing pipelines.}{53}{section*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Defining maps between images}{53}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Unsupervised methods.}{53}{section*.42}\protected@file@percent }
\citation{visual_attribute}
\citation{visual_attribute}
\citation{Hertzmann01Image}
\citation{kim2021representat