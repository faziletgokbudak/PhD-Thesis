\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{thesis.ist}
\@glsorder{word}
\providecommand \oddpage@label [2]{}
\citation{}
\citation{}
\@writefile{toc}{\thispagestyle {empty}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{13}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Research Vision and Motivation}{13}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Contributions}{13}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Relationship to Published Work}{13}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{15}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Displaying an image}{15}{section.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Need a figure to represent pixel representation of continuous image}}{15}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:display-grid}{{2.1}{15}{Need a figure to represent pixel representation of continuous image}{figure.caption.5}{}}
\citation{schewe2007color}
\citation{schewe2007color}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces \relax }}{16}{figure.caption.6}\protected@file@percent }
\newlabel{fig:color-approximate}{{2.2}{16}{\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Comparison of different color gamuts \cite  {schewe2007color}}}{16}{figure.2.3}\protected@file@percent }
\newlabel{fig:color-gamut}{{2.3}{16}{Comparison of different color gamuts \cite {schewe2007color}}{figure.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces When a color is represented with too few bits, the transition between colors appears as discrete steps, known as color banding (right).}}{17}{figure.caption.7}\protected@file@percent }
\newlabel{fig:color-band}{{2.4}{17}{When a color is represented with too few bits, the transition between colors appears as discrete steps, known as color banding (right)}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}3D scene components}{17}{section.2.2}\protected@file@percent }
\citation{boss2021nerd}
\citation{boss2021nerd}
\citation{boss2021nerd}
\citation{boss2021nerd}
\citation{triangulation}
\citation{triangulation}
\citation{triangulation}
\citation{triangulation}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Rendering an image of a 3D scene requires lighting, viewpoint (camera) and geometry. Image courtesy of \citeauthor  {boss2021nerd} \cite  {boss2021nerd}.}}{18}{figure.caption.8}\protected@file@percent }
\newlabel{fig:teaser}{{2.5}{18}{Rendering an image of a 3D scene requires lighting, viewpoint (camera) and geometry. Image courtesy of \citeauthor {boss2021nerd} \cite {boss2021nerd}}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Geometry}{18}{section*.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Triangulation of surfaces is a commonly used method in computer graphics to define surfaces. It allows the creation of complex shapes. Increasing the number of triangles also increases the resolution of the shape that leads to smoother surface representations. Image courtesy of \citeauthor  {triangulation} \cite  {triangulation}.}}{19}{figure.caption.10}\protected@file@percent }
\newlabel{fig:triangulation}{{2.6}{19}{Triangulation of surfaces is a commonly used method in computer graphics to define surfaces. It allows the creation of complex shapes. Increasing the number of triangles also increases the resolution of the shape that leads to smoother surface representations. Image courtesy of \citeauthor {triangulation} \cite {triangulation}}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Photo-realistic rendering}{19}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Perspective projection and visibility.}{20}{section*.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces \relax }}{20}{figure.caption.12}\protected@file@percent }
\newlabel{fig:perspective_projection}{{2.7}{20}{\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Single-point vs two-point perspective. Computer graphics mimics our visual system, focusing on single-point projection. However, multi-point perspective is widely explored in artistic drawing.}}{21}{figure.caption.13}\protected@file@percent }
\newlabel{fig:artistic_drawing}{{2.8}{21}{Single-point vs two-point perspective. Computer graphics mimics our visual system, focusing on single-point projection. However, multi-point perspective is widely explored in artistic drawing}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {paragraph}{Appearance.}{21}{section*.14}\protected@file@percent }
\citation{googlePhysicallyBased}
\citation{googlePhysicallyBased}
\citation{googlePhysicallyBased}
\citation{googlePhysicallyBased}
\citation{specfig}
\citation{specfig}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces The ripples on the water's surface cause a blurred image of the bridge, acting as a rough surface.}}{22}{figure.2.9}\protected@file@percent }
\newlabel{fig:water_reflection}{{2.9}{22}{The ripples on the water's surface cause a blurred image of the bridge, acting as a rough surface}{figure.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Reflection on a surface with microfacets vs smooth surface. Image from \citeauthor  {googlePhysicallyBased} \cite  {googlePhysicallyBased}}}{23}{figure.caption.15}\protected@file@percent }
\newlabel{fig:microfacet}{{2.10}{23}{Reflection on a surface with microfacets vs smooth surface. Image from \citeauthor {googlePhysicallyBased} \cite {googlePhysicallyBased}}{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Specular vs diffuse reflection. Diffuse surfaces cause rays to scatter randomly, brightening the object equally in all viewing directions. Specular reflection is, on the other hand, view-dependent, reflected rays concentrated around the mirror reflection. Image courtesy of \citeauthor  {specfig}.}}{23}{figure.caption.16}\protected@file@percent }
\newlabel{fig:specularvsdiffuse}{{2.11}{23}{Specular vs diffuse reflection. Diffuse surfaces cause rays to scatter randomly, brightening the object equally in all viewing directions. Specular reflection is, on the other hand, view-dependent, reflected rays concentrated around the mirror reflection. Image courtesy of \citeauthor {specfig}}{figure.caption.16}{}}
\@writefile{toc}{\contentsline {paragraph}{Ray tracing - Light transport simulation.}{24}{section*.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Ray tracing -Image courtesy of \href  {https://commons.wikimedia.org/wiki/File:Ray_trace_diagram.svg}{Henrik, CC BY-SA 4.0}}}{24}{figure.caption.18}\protected@file@percent }
\newlabel{fig:raytracing}{{2.12}{24}{Ray tracing -Image courtesy of \href {https://commons.wikimedia.org/wiki/File:Ray_trace_diagram.svg}{Henrik, CC BY-SA 4.0}}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Rendering and Image Formation}{25}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Deep learning}{25}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Multilayer Perceptron}{25}{subsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Hypernetworks}{25}{subsection.2.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Diffusion models}{25}{subsection.2.5.3}\protected@file@percent }
\citation{shaham2021spatially,li2020lapar}
\citation{moran2020deeplpf}
\citation{Gharbi17Deep}
\citation{yan2016automatic}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}One-shot Detail Retouching}{27}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{27}{section.3.1}\protected@file@percent }
\newlabel{sec:introduction}{{3.1}{27}{Introduction}{section.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Our technique automatically transfers retouching edits to new images by learning the desired edits from one example \textit  {before-after} pair (insets). The transferred edits accurately capture intricate details such as wrinkles, dark spots, strands of hair, or eyelashes, as shown in the input (top) and retouched (bottom) pairs. Image courtesy of Jenavieve (top-left), Logan ProPro (top-left, inset), Marissa Oosterlee (top-middle). (CC-BY).}}{28}{figure.caption.19}\protected@file@percent }
\newlabel{fig:teaser}{{3.1}{28}{Our technique automatically transfers retouching edits to new images by learning the desired edits from one example \textit {before-after} pair (insets). The transferred edits accurately capture intricate details such as wrinkles, dark spots, strands of hair, or eyelashes, as shown in the input (top) and retouched (bottom) pairs. Image courtesy of Jenavieve (top-left), Logan ProPro (top-left, inset), Marissa Oosterlee (top-middle). (CC-BY)}{figure.caption.19}{}}
\citation{Faridul14ASurvey,mustafa2022distilling}
\citation{Bychkovsky11Learning,Bae06Two,Pitie05NDimensional,Pitie07Automated,Reinhard01Color,Sunkavalli10Multi,he2020conditional,park2018distort}
\citation{CohenOr06Color}
\citation{Bychkovsky11Learning}
\citation{Bychkovsky11Learning}
\citation{chen2017fast}
\citation{chen2017fast}
\citation{Hu18Exposure}
\citation{CohenOr06Color}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Related Work}{29}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Digital Photo Enhancement}{29}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Global image enhancement.}{29}{section*.20}\protected@file@percent }
\citation{kim2021representative}
\citation{wang2019underexposed}
\citation{Shapira13Image}
\citation{Laffont14Transient,Tai07Soft}
\citation{Berthouzoz11AFramework,Chen18Deep,Huang14Parametric,Omiya18Learning,Saeedi18Multimodal}
\citation{An10User,Pouli11Progressive,Tai05Local}
\citation{Gharbi17Deep,Hwang12Context,Kaufman12Content,Nam17Deep,Yan14Automatic,Zhu18Automatic}
\citation{HaCohen11Nonrigid}
\citation{Kagarlitsky09Piecewise,Shih13Data}
\citation{moran2020deeplpf,Gharbi17Deep,chen2018deep,shaham2021spatially,li2020lapar}
\citation{chen2018deep}
\citation{chen2018deep}
\citation{chen2016bilateral}
\citation{Gharbi17Deep}
\citation{moran2020deeplpf}
\citation{moran2020deeplpf}
\citation{Bae06Two}
\citation{HaCohen11Nonrigid}
\citation{Shih14Style}
\citation{Shih14Style}
\citation{tseng2019hyperparameter,yu2021reconfigisp}
\citation{tseng2019hyperparameter}
\citation{yu2021reconfigisp}
\citation{yu2021reconfigisp}
\citation{tseng2022neural}
\citation{tseng2022neural}
\citation{ma2021retinexgan}
\citation{yang2020fidelity}
\citation{9334429}
\citation{Liu16Makeup}
\citation{Frigo16Split}
\citation{Chen18Deep}
\citation{lin2020tuigan}
\citation{Zhang13Style,Hu18Exposure}
\@writefile{toc}{\contentsline {paragraph}{Local context-aware image enhancement.}{30}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Differentiable image processing pipelines.}{30}{section*.22}\protected@file@percent }
\citation{visual_attribute}
\citation{visual_attribute}
\citation{Hertzmann01Image}
\citation{kim2021representative,wang2019underexposed}
\citation{10.1145/2790296}
\citation{tolstikhin2021mlp}
\citation{cazenavette2021mixergan}
\citation{cazenavette2021mixergan}
\citation{cazenavette2021mixergan}
\citation{cazenavette2021mixergan}
\citation{tolstikhin2021mlp}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Defining Maps between Images}{31}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Unsupervised methods.}{31}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Supervised methods.}{31}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Overview and Motivations}{32}{section.3.3}\protected@file@percent }
\newlabel{chap:motivations}{{3.3}{32}{Overview and Motivations}{section.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}One-shot Retouching}{32}{section.3.4}\protected@file@percent }
\newlabel{sec:Methodology}{{3.4}{32}{One-shot Retouching}{section.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Our technique learns a separate mapping per frequency band by decomposing images into five different bands with a Laplacian pyramid. At each Laplacian band $l$, we define a mapping between flattened patches $\mathbf  {x}_i$, $\mathbf  {y}_i$ extracted from before-after bands $X_l$, $Y_l$. Our field based method (MLP block) adapts transformations to input patches, providing local context-aware adjustments. The transformation matrices and MLP parameters are learned jointly from scratch for each Laplacian band of the before-after pair.}}{33}{figure.caption.25}\protected@file@percent }
\newlabel{fig:modelT}{{3.2}{33}{Our technique learns a separate mapping per frequency band by decomposing images into five different bands with a Laplacian pyramid. At each Laplacian band $l$, we define a mapping between flattened patches $\mathbf {x}_i$, $\mathbf {y}_i$ extracted from before-after bands $X_l$, $Y_l$. Our field based method (MLP block) adapts transformations to input patches, providing local context-aware adjustments. The transformation matrices and MLP parameters are learned jointly from scratch for each Laplacian band of the before-after pair}{figure.caption.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Frequency Decomposition}{34}{subsection.3.4.1}\protected@file@percent }
\newlabel{sec:thePatchMap}{{3.4.1}{34}{Frequency Decomposition}{subsection.3.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Transformation Blending}{34}{subsection.3.4.2}\protected@file@percent }
\newlabel{sec:Blending}{{3.4.2}{34}{Transformation Blending}{subsection.3.4.2}{}}
\newlabel{eq:weightedSum}{{3.2}{34}{Transformation Blending}{equation.3.4.2}{}}
\citation{Boyadzhiev15Band}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Retouching an Input Image}{35}{subsection.3.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Implementation}{35}{subsection.3.4.4}\protected@file@percent }
\newlabel{sec:Implementation}{{3.4.4}{35}{Implementation}{subsection.3.4.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Patch size and stride.}{35}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Detail and color modifications.}{35}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Evaluation metrics.}{35}{section*.28}\protected@file@percent }
\newlabel{train_det}{{3.4.4}{35}{Training details}{section*.29}{}}
\@writefile{toc}{\contentsline {paragraph}{Training details.}{35}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Results}{36}{section.3.5}\protected@file@percent }
\newlabel{sec:results}{{3.5}{36}{Results}{section.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Ablation Study}{36}{subsection.3.5.1}\protected@file@percent }
\newlabel{ablation}{{3.5.1}{36}{Ablation Study}{subsection.3.5.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Transformation Matrices.}{36}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Patch-adaptive Transformation Blending.}{36}{section*.33}\protected@file@percent }
\citation{karras2019style}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The higher the complexity of the learned algorithm, the more transformation matrices our technique requires to capture the effects on local regions accurately. While $K=1$ can be sufficient for our model to capture unsharp masking, it requires more matrices to represent bilateral filtering precisely.}}{37}{figure.caption.31}\protected@file@percent }
\newlabel{fig:ablation_K}{{3.3}{37}{The higher the complexity of the learned algorithm, the more transformation matrices our technique requires to capture the effects on local regions accurately. While $K=1$ can be sufficient for our model to capture unsharp masking, it requires more matrices to represent bilateral filtering precisely}{figure.caption.31}{}}
\@writefile{toc}{\contentsline {paragraph}{Weights visulation.}{37}{section*.34}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces An MLP regressor cannot capture local edits, resulting in inaccurate retouching edits, such as blurring on the skin or around the eyes.}}{38}{figure.caption.32}\protected@file@percent }
\newlabel{fig:ablation_MLP}{{3.4}{38}{An MLP regressor cannot capture local edits, resulting in inaccurate retouching edits, such as blurring on the skin or around the eyes}{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Visualization of the reconstructed patch-adaptive weights of the model trained with example images in our teaser figure. Here, the input to the model is shown on top, and each row shows the weights in the corresponding Laplacian band, indicated on the left.}}{38}{figure.caption.35}\protected@file@percent }
\newlabel{fig:weight-vis}{{3.5}{38}{Visualization of the reconstructed patch-adaptive weights of the model trained with example images in our teaser figure. Here, the input to the model is shown on top, and each row shows the weights in the corresponding Laplacian band, indicated on the left}{figure.caption.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Qualitative Results}{39}{subsection.3.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The reproduced retouching style from the example pair (inset) improves skin texture without affecting fine details, such as eyes and hair, for a visually improved portrait. Moreover, our technique generalizes well to faces with different lighting conditions and accurately reproduces the example retouching style.}}{39}{figure.caption.36}\protected@file@percent }
\newlabel{fig:newdataset_ex}{{3.6}{39}{The reproduced retouching style from the example pair (inset) improves skin texture without affecting fine details, such as eyes and hair, for a visually improved portrait. Moreover, our technique generalizes well to faces with different lighting conditions and accurately reproduces the example retouching style}{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Material editing results on photos (left), and rendered images (right), based on the before-after pair (inset). The details, such as scratches or lines are emphasized, and materials became shinier. Image courtesy of royalmix (top and bottom-inset), tsmdunn (bottom). (PixelSquid).}}{40}{figure.caption.37}\protected@file@percent }
\newlabel{fig:material_res}{{3.7}{40}{Material editing results on photos (left), and rendered images (right), based on the before-after pair (inset). The details, such as scratches or lines are emphasized, and materials became shinier. Image courtesy of royalmix (top and bottom-inset), tsmdunn (bottom). (PixelSquid)}{figure.caption.37}{}}
\citation{ronneberger2015u}
\citation{shaham2021spatially}
\citation{xu2015deep}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Our patch-adaptive technique can accurately capture the nuances between different retouching styles as given by the examples (top row).}}{41}{figure.caption.38}\protected@file@percent }
\newlabel{fig:retouchingstyles}{{3.8}{41}{Our patch-adaptive technique can accurately capture the nuances between different retouching styles as given by the examples (top row)}{figure.caption.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Comparison with the state-of-the-art}{41}{subsection.3.5.3}\protected@file@percent }
\newlabel{sec:Comparisons}{{3.5.3}{41}{Comparison with the state-of-the-art}{subsection.3.5.3}{}}
\citation{Bychkovsky11Learning}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Quantitative performance comparison for the reproduction of various image processing filters. Average PSNR and SSIM values are computed over 182 images of different types of images including faces, landscapes, materials, and rooms. Qualitative results can be found in the supplementary material.We highlight \colorbox {blue!25}{best PSNR} and \colorbox {orange!25}{best SSIM} results.}}{42}{table.caption.39}\protected@file@percent }
\newlabel{tablecomparison}{{3.1}{42}{Quantitative performance comparison for the reproduction of various image processing filters. Average PSNR and SSIM values are computed over 182 images of different types of images including faces, landscapes, materials, and rooms. Qualitative results can be found in the supplementary material.We highlight \colorbox {blue!25}{best PSNR} and \colorbox {orange!25}{best SSIM} results}{table.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Qualitative comparisons with state-of-the-art methods on different types of images. These results are included in Table 1 for the filter types indicated in rows. The training strategy for the state-of-the-art methods is summarized in Section \ref {sec:Comparisons}. Before-after pairs along with additional results can be found in the supplementary material. Image courtesy of Arnaud Rougetet (landscape) and virtualhorizonstudio (alarm clock). (CC-BY and PixelSquid).}}{43}{figure.caption.40}\protected@file@percent }
\newlabel{fig:QualitativeComp}{{3.9}{43}{Qualitative comparisons with state-of-the-art methods on different types of images. These results are included in Table 1 for the filter types indicated in rows. The training strategy for the state-of-the-art methods is summarized in Section \ref {sec:Comparisons}. Before-after pairs along with additional results can be found in the supplementary material. Image courtesy of Arnaud Rougetet (landscape) and virtualhorizonstudio (alarm clock). (CC-BY and PixelSquid)}{figure.caption.40}{}}
\citation{Shih14Style}
\citation{Yan14Automatic}
\citation{Besl92AMethod}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Limitations and Future Work}{44}{subsection.3.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Our technique cannot accurately handle extreme non-repeating local effects such as tattoos (top), and when example and input images are of very different semantics (bottom). Image courtesy of bgaj23 (coin). (PixelSquid).}}{44}{figure.caption.41}\protected@file@percent }
\newlabel{fig:limitations}{{3.10}{44}{Our technique cannot accurately handle extreme non-repeating local effects such as tattoos (top), and when example and input images are of very different semantics (bottom). Image courtesy of bgaj23 (coin). (PixelSquid)}{figure.caption.41}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Conclusions}{45}{section.3.6}\protected@file@percent }
\citation{ngan2005}
\citation{dupuy2015,guarnera2016}
\citation{nielsen2015optimal}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Neural Generalizable Material Representation}{47}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{47}{section.4.1}\protected@file@percent }
\newlabel{sec:intro}{{4.1}{47}{Introduction}{section.4.1}{}}
\citation{sitzmann2020siren,ffn,cnf2023}
\citation{sztrajman2021neural,cnf2023}
\citation{rebain2022attention}
\citation{park2019deepsdf}
\citation{ha2017hypernetworks}
\citation{jiang2021cotr}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A room scene rendered with our reconstructed materials, including sparse reconstruction (table top and legs, door, door and picture frames, hinge), compression (two teapots on the left, door handle) and BRDF interpolation (right-most teapot). Scene courtesy of Benedikt Bitterli.}}{48}{figure.caption.42}\protected@file@percent }
\newlabel{fig:teaser}{{4.1}{48}{A room scene rendered with our reconstructed materials, including sparse reconstruction (table top and legs, door, door and picture frames, hinge), compression (two teapots on the left, door handle) and BRDF interpolation (right-most teapot). Scene courtesy of Benedikt Bitterli}{figure.caption.42}{}}
\citation{blinn77}
\citation{cooktorrance1982}
\citation{ward1992}
\citation{walter2007microfacet}
\citation{dupuy2015,ashikhmin2007,bagher2016}
\citation{low2012}
\citation{ngan2005,guarnera2016}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Related Work}{49}{section.4.2}\protected@file@percent }
\newlabel{sec:relatedwork}{{4.2}{49}{Related Work}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Analytic BRDF Models}{49}{subsection.4.2.1}\protected@file@percent }
\citation{matusik2003data,nielsen2015optimal,serrano2018intuitive}
\citation{lawrence2004efficient,lawrence2006inverse}
\citation{sun2007interactive}
\citation{bilgili2011general,tongbuasirilai2020compact}
\citation{bagher2016non}
\citation{rainer2019neural,hu2020deepbrdf,sztrajman2021neural,zheng2021compact,maximov2019deep,chen2021invertible,fan2021neural,cnf2023}
\citation{maximov2019deep}
\citation{sztrajman2021neural}
\citation{cnf2023}
\citation{hu2020deepbrdf}
\citation{zheng2021compact}
\citation{nielsen2015optimal}
\citation{liu2023learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Regression-based BRDF Estimation}{50}{subsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Deep learning for BRDF modeling.}{50}{section*.43}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Efficient BRDF Acquisition}{50}{subsection.4.2.3}\protected@file@percent }
\citation{kang2018efficient,kang2019learning,ma2021free,ma2023opensvbrdf,tunwattanapong2013acquiring}
\citation{guo2021highlight,hui2017reflectance,deschaintre2018single,deschaintre2019flexible,martin2022materia,zhou2021adversarial,gao2019deep}
\citation{ratzlaff2019hypergan}
\citation{erkocc2023hyperdiffusion}
\citation{wang2022attention,yang2023contranerf}
\citation{nielsen2015optimal}
\@writefile{toc}{\contentsline {paragraph}{Spatially-varying BRDFs (SVBRDF):}{51}{section*.44}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Hypernetworks and GNFs}{51}{subsection.4.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Methodology}{51}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Pre-processing}{51}{subsection.4.3.1}\protected@file@percent }
\newlabel{sec:pre-proc}{{4.3.1}{51}{Pre-processing}{subsection.4.3.1}{}}
\newlabel{eq:preprocess}{{4.1}{51}{Pre-processing}{equation.4.3.1}{}}
\citation{sztrajman2021neural}
\citation{rusinkiewicz1998new}
\citation{sitzmann2020siren}
\citation{sitzmann2020metasdf}
\citation{rusinkiewicz1998new}
\citation{zaheer2017deepsets}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces During training, the set encoder and hypernetwork decoder are trained on a set of materials to predict the weights of hyponet (MLP) so that it can reconstruct the training set. The BRDF data is provided as a set of BRDF coordinates, $H_n,D_n$, and the corresponding reflectance values $f_r(H_n,D_n)$. To reconstruct a new material from a small set of BRDF reflectance samples, the trained set encoder and hypernetwork decoder are used to predict the weights of hyponet for the unknown material. Once those weights are known, we can query BRDF at any coordinates and for any new materials, conditioned on the embedding of their sampled BRDF values.}}{52}{figure.caption.45}\protected@file@percent }
\newlabel{fig:mainfig}{{4.2}{52}{During training, the set encoder and hypernetwork decoder are trained on a set of materials to predict the weights of hyponet (MLP) so that it can reconstruct the training set. The BRDF data is provided as a set of BRDF coordinates, $H_n,D_n$, and the corresponding reflectance values $f_r(H_n,D_n)$. To reconstruct a new material from a small set of BRDF reflectance samples, the trained set encoder and hypernetwork decoder are used to predict the weights of hyponet for the unknown material. Once those weights are known, we can query BRDF at any coordinates and for any new materials, conditioned on the embedding of their sampled BRDF values}{figure.caption.45}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Hypernetwork}{52}{subsection.4.3.2}\protected@file@percent }
\newlabel{sec:hypernet}{{4.3.2}{52}{Hypernetwork}{subsection.4.3.2}{}}
\citation{sztrajman2021neural}
\citation{ha2017hypernetworks}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2.1}Set Encoder}{53}{subsubsection.4.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2.2}Hypernetwork Decoder and Hyponet}{53}{subsubsection.4.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Training}{53}{subsection.4.3.3}\protected@file@percent }
\newlabel{sec:traindet}{{4.3.3}{53}{Training}{subsection.4.3.3}{}}
\newlabel{eq:loss}{{4.2}{53}{Training}{equation.4.3.2}{}}
\citation{ngan2005experimental}
\citation{Matusik2003jul}
\citation{dupuy2018adaptive}
\citation{Matusik2003jul}
\citation{rusinkiewicz1998new}
\newlabel{eq:Lrec}{{4.3}{54}{Training}{equation.4.3.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Inference:}{54}{section*.46}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Experiments}{54}{section.4.4}\protected@file@percent }
\newlabel{sec:exp}{{4.4}{54}{Experiments}{section.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Datasets and Baselines}{54}{subsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Sparse BRDF Reconstruction}{54}{subsection.4.4.2}\protected@file@percent }
\newlabel{sec:brdf_rec}{{4.4.2}{54}{Sparse BRDF Reconstruction}{subsection.4.4.2}{}}
\citation{sztrajman2021neural}
\citation{nielsen2015optimal}
\citation{sztrajman2021neural}
\citation{matusik2003data,ngan2006image}
\citation{nielsen2015optimal}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2.1}Qualitative Comparison}{55}{subsubsection.4.4.2.1}\protected@file@percent }
\newlabel{sec:qual_comp}{{4.4.2.1}{55}{Qualitative Comparison}{subsubsection.4.4.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces t-SNE clustering of the test embeddings with different sample sizes, including $N=8, 40, 160, 4\,000, 40\,000, 640\,000$.}}{56}{figure.caption.47}\protected@file@percent }
\newlabel{fig:tsne-vis-imputation}{{4.3}{56}{t-SNE clustering of the test embeddings with different sample sizes, including $N=8, 40, 160, 4\,000, 40\,000, 640\,000$}{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Qualitative comparison results for reconstruction with small sample sizes. Thanks to the prior that our hypernetwork model learns for material appearance through training, it can accurately estimate the BRDFs of unseen materials and preserve the colors better than the baselines.}}{57}{figure.caption.48}\protected@file@percent }
\newlabel{fig:imp_comp_upt}{{4.4}{57}{Qualitative comparison results for reconstruction with small sample sizes. Thanks to the prior that our hypernetwork model learns for material appearance through training, it can accurately estimate the BRDFs of unseen materials and preserve the colors better than the baselines}{figure.caption.48}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2.2}Quantitative Evaluation}{57}{subsubsection.4.4.2.2}\protected@file@percent }
\citation{zheng2021compact}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Average PSNR, Delta E (CIE 2000), and SSIM results across different sample sizes. }}{58}{figure.caption.49}\protected@file@percent }
\newlabel{fig:imp_plots}{{4.5}{58}{Average PSNR, Delta E (CIE 2000), and SSIM results across different sample sizes}{figure.caption.49}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Hypernetwork sparse reconstruction - Average metric results across varying sample sizes ($N$) over the test set. We highlight \colorbox {blue!25}{best} and \colorbox {orange!25}{second best} results.}}{58}{table.caption.50}\protected@file@percent }
\newlabel{table: ours_diff_samples}{{4.1}{58}{Hypernetwork sparse reconstruction - Average metric results across varying sample sizes ($N$) over the test set. We highlight \colorbox {blue!25}{best} and \colorbox {orange!25}{second best} results}{table.caption.50}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Compression}{58}{subsection.4.4.3}\protected@file@percent }
\newlabel{sec:compression}{{4.4.3}{58}{Compression}{subsection.4.4.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Compression - Average metric results over the renderings of the entire MERL dataset. We highlight \colorbox {blue!25}{best} and \colorbox {orange!25}{second best} results.}}{59}{table.caption.51}\protected@file@percent }
\newlabel{table: oursvsnps}{{4.2}{59}{Compression - Average metric results over the renderings of the entire MERL dataset. We highlight \colorbox {blue!25}{best} and \colorbox {orange!25}{second best} results}{table.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Reconstruction results for BRDF compression (GT: ground truths).}}{59}{figure.caption.52}\protected@file@percent }
\newlabel{fig:comp-fig}{{4.6}{59}{Reconstruction results for BRDF compression (GT: ground truths)}{figure.caption.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}BRDF Editing}{59}{subsection.4.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces BRDF editing through linear interpolation between the embeddings of two materials.}}{60}{figure.caption.53}\protected@file@percent }
\newlabel{fig:interpolation}{{4.7}{60}{BRDF editing through linear interpolation between the embeddings of two materials}{figure.caption.53}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.5}Limitations and Future Work}{60}{subsection.4.4.5}\protected@file@percent }
\newlabel{sec:limits}{{4.4.5}{60}{Limitations and Future Work}{subsection.4.4.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Specular components:}{60}{section*.54}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{BRDF editing:}{60}{section*.55}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{SVBRDF representations:}{60}{section*.56}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Conclusion}{60}{section.4.5}\protected@file@percent }
\newlabel{sec:conc}{{4.5}{60}{Conclusion}{section.4.5}{}}
\bibstyle{plainnat}
\bibdata{thesis}
\bibcite{An10User}{{1}{2010}{{An and Pellacini}}{{}}}
\bibcite{ashikhmin2007}{{2}{2007}{{Ashikhmin and Premoze}}{{}}}
\bibcite{Bae06Two}{{3}{2006}{{Bae et~al.}}{{Bae, Paris, and Durand}}}
\bibcite{bagher2016}{{4}{2016{}}{{Bagher et~al.}}{{Bagher, Snyder, and Nowrouzezahrai}}}
\bibcite{bagher2016non}{{5}{2016{}}{{Bagher et~al.}}{{Bagher, Snyder, and Nowrouzezahrai}}}
\bibcite{Berthouzoz11AFramework}{{6}{2011}{{Berthouzoz et~al.}}{{Berthouzoz, Li, Dontcheva, and Agrawala}}}
\bibcite{Besl92AMethod}{{7}{1992}{{Besl and McKay}}{{}}}
\bibcite{bilgili2011general}{{8}{2011}{{Bilgili et~al.}}{{Bilgili, {\"O}zt{\"u}rk, and Kurt}}}
\bibcite{blinn77}{{9}{1977}{{Blinn}}{{}}}
\bibcite{boss2021nerd}{{10}{2021}{{Boss et~al.}}{{Boss, Braun, Jampani, Barron, Liu, and Lensch}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{63}{section*.57}\protected@file@percent }
\bibcite{Boyadzhiev15Band}{{11}{2015}{{Boyadzhiev et~al.}}{{Boyadzhiev, Bala, Paris, and Adelson}}}
\bibcite{triangulation}{{12}{2019}{{Broeren et~al.}}{{Broeren, van~de Sande, van~der Wijk, and Herder}}}
\bibcite{Bychkovsky11Learning}{{13}{2011}{{Bychkovsky et~al.}}{{Bychkovsky, Paris, Chan, and Durand}}}
\bibcite{cazenavette2021mixergan}{{14}{2021}{{Cazenavette and De~Guevara}}{{}}}
\bibcite{chen2016bilateral}{{15}{2016}{{Chen et~al.}}{{Chen, Adams, Wadhwa, and Hasinoff}}}
\bibcite{chen2017fast}{{16}{2017}{{Chen et~al.}}{{Chen, Xu, and Koltun}}}
\bibcite{Chen18Deep}{{17}{2018{}}{{Chen et~al.}}{{Chen, Wang, Kao, and Chuang}}}
\bibcite{chen2018deep}{{18}{2018{}}{{Chen et~al.}}{{Chen, Wang, Kao, and Chuang}}}
\bibcite{chen2021invertible}{{19}{2021}{{Chen et~al.}}{{Chen, Nobuhara, and Nishino}}}
\bibcite{CohenOr06Color}{{20}{2006}{{Cohen-Or et~al.}}{{Cohen-Or, Sorkine, Gal, Leyvand, and Xu}}}
\bibcite{cooktorrance1982}{{21}{1982}{{Cook and Torrance}}{{}}}
\bibcite{deschaintre2018single}{{22}{2018}{{Deschaintre et~al.}}{{Deschaintre, Aittala, Durand, Drettakis, and Bousseau}}}
\bibcite{deschaintre2019flexible}{{23}{2019}{{Deschaintre et~al.}}{{Deschaintre, Aittala, Durand, Drettakis, and Bousseau}}}
\bibcite{dupuy2018adaptive}{{24}{2018}{{Dupuy and Jakob}}{{}}}
\bibcite{dupuy2015}{{25}{2015}{{Dupuy et~al.}}{{Dupuy, Heitz, Iehl, Poulin, and Ostromoukhov}}}
\bibcite{erkocc2023hyperdiffusion}{{26}{2023}{{Erko{\c {c}} et~al.}}{{Erko{\c {c}}, Ma, Shan, Nie{\ss }ner, and Dai}}}
\bibcite{fan2021neural}{{27}{2021}{{Fan et~al.}}{{Fan, Wang, Ha{\v {s}}an, Yang, and Yan}}}
\bibcite{Faridul14ASurvey}{{28}{2014}{{Faridul et~al.}}{{Faridul, Pouli, Chamaret, Stauder, Tremeau, and Reinhard}}}
\bibcite{Frigo16Split}{{29}{2016}{{Frigo et~al.}}{{Frigo, Sabater, Delon, and Hellier}}}
\bibcite{gao2019deep}{{30}{2019}{{Gao et~al.}}{{Gao, Li, Dong, Peers, Xu, and Tong}}}
\bibcite{Gharbi17Deep}{{31}{2017}{{Gharbi et~al.}}{{Gharbi, Chen, Barron, Hasinoff, and Durand}}}
\bibcite{guarnera2016}{{32}{2016}{{Guarnera et~al.}}{{Guarnera, Guarnera, Ghosh, Denk, and Glencross}}}
\bibcite{guo2021highlight}{{33}{2021}{{Guo et~al.}}{{Guo, Lai, Tao, Cai, Wang, Guo, and Yan}}}
\bibcite{googlePhysicallyBased}{{34}{}{{Guy and Agopian}}{{}}}
\bibcite{ha2017hypernetworks}{{35}{2017}{{Ha et~al.}}{{Ha, Dai, and Le}}}
\bibcite{HaCohen11Nonrigid}{{36}{2011}{{HaCohen et~al.}}{{HaCohen, Shechtman, Goldman, and Lischinski}}}
\bibcite{he2020conditional}{{37}{2020}{{He et~al.}}{{He, Liu, Qiao, and Dong}}}
\bibcite{Hertzmann01Image}{{38}{2001}{{Hertzmann et~al.}}{{Hertzmann, Jacobs, Oliver, Curless, and Salesin}}}
\bibcite{hu2020deepbrdf}{{39}{2020}{{Hu et~al.}}{{Hu, Guo, Chen, Li, and Guo}}}
\bibcite{Hu18Exposure}{{40}{2018}{{Hu et~al.}}{{Hu, He, Xu, Wang, and Lin}}}
\bibcite{Huang14Parametric}{{41}{2014}{{Huang et~al.}}{{Huang, Zhang, Lai, Kopf, Cohen-Or, and Hu}}}
\bibcite{hui2017reflectance}{{42}{2017}{{Hui et~al.}}{{Hui, Sunkavalli, Lee, Hadap, Wang, and Sankaranarayanan}}}
\bibcite{Hwang12Context}{{43}{2012}{{Hwang et~al.}}{{Hwang, Kapoor, and Kang}}}
\bibcite{jiang2021cotr}{{44}{2021{}}{{Jiang et~al.}}{{Jiang, Trulls, Hosang, Tagliasacchi, and Yi}}}
\bibcite{9334429}{{45}{2021{}}{{Jiang et~al.}}{{Jiang, Gong, Liu, Cheng, Fang, Shen, Yang, Zhou, and Wang}}}
\bibcite{Kagarlitsky09Piecewise}{{46}{2009}{{Kagarlitsky et~al.}}{{Kagarlitsky, Moses, and Hel-Or}}}
\bibcite{kang2018efficient}{{47}{2018}{{Kang et~al.}}{{}}}
\bibcite{kang2019learning}{{48}{2019}{{Kang et~al.}}{{}}}
\bibcite{karras2019style}{{49}{2021}{{Karras et~al.}}{{Karras, Laine, and Aila}}}
\bibcite{Kaufman12Content}{{50}{2012}{{Kaufman et~al.}}{{Kaufman, Lischinski, and Werman}}}
\bibcite{kim2021representative}{{51}{2021}{{Kim et~al.}}{{Kim, Choi, Kim, and Koh}}}
\bibcite{Laffont14Transient}{{52}{2014}{{Laffont et~al.}}{{Laffont, Ren, Tao, Qian, and Hays}}}
\bibcite{lawrence2004efficient}{{53}{2004}{{Lawrence et~al.}}{{Lawrence, Rusinkiewicz, and Ramamoorthi}}}
\bibcite{lawrence2006inverse}{{54}{2006}{{Lawrence et~al.}}{{Lawrence, Ben-Artzi, DeCoro, Matusik, Pfister, Ramamoorthi, and Rusinkiewicz}}}
\bibcite{li2020lapar}{{55}{2020}{{Li et~al.}}{{Li, Zhou, Qi, Jiang, Lu, and Jia}}}
\bibcite{visual_attribute}{{56}{2017}{{Liao et~al.}}{{Liao, Yao, Yuan, Hua, and Kang}}}
\bibcite{lin2020tuigan}{{57}{2020}{{Lin et~al.}}{{Lin, Pang, Xia, Chen, and Luo}}}
\bibcite{liu2023learning}{{58}{2023}{{Liu et~al.}}{{Liu, Fischer, and Ritschel}}}
\bibcite{Liu16Makeup}{{59}{2016}{{Liu et~al.}}{{Liu, Ou, Qian, Wang, and Cao}}}
\bibcite{low2012}{{60}{2012}{{L{\"{o}}w et~al.}}{{L{\"{o}}w, Kronander, Ynnerman, and Unger}}}
\bibcite{ma2021retinexgan}{{61}{2021{}}{{Ma et~al.}}{{Ma, Guo, Yu, Chen, Ren, Xi, Li, and Zhou}}}
\bibcite{ma2021free}{{62}{2021{}}{{Ma et~al.}}{{}}}
\bibcite{ma2023opensvbrdf}{{63}{2023}{{Ma et~al.}}{{Ma, Xu, Zhang, Zhou, and Wu}}}
\bibcite{martin2022materia}{{64}{2022}{{Martin et~al.}}{{Martin, Roullier, Rouffet, Kaiser, and Boubekeur}}}
\bibcite{Matusik2003jul}{{65}{2003}{{Matusik et~al.}}{{Matusik, Pfister, Brand, and McMillan}}}
\bibcite{matusik2003data}{{66}{2003}{{Matusik}}{{}}}
\bibcite{maximov2019deep}{{67}{2019}{{Maximov et~al.}}{{Maximov, Leal-Taix{\'e}, Fritz, and Ritschel}}}
\bibcite{moran2020deeplpf}{{68}{2020}{{Moran et~al.}}{{Moran, Marza, McDonagh, Parisot, and Slabaugh}}}
\bibcite{mustafa2022distilling}{{69}{2022}{{Mustafa et~al.}}{{Mustafa, Hanji, and Mantiuk}}}
\bibcite{Nam17Deep}{{70}{2017}{{Nam and Kim}}{{}}}
\bibcite{ngan2005experimental}{{71}{2005{}}{{Ngan et~al.}}{{Ngan, Durand, and Matusik}}}
\bibcite{ngan2005}{{72}{2005{}}{{Ngan et~al.}}{{Ngan, Durand, and Matusik}}}
\bibcite{ngan2006image}{{73}{2006}{{Ngan et~al.}}{{Ngan, Durand, and Matusik}}}
\bibcite{nielsen2015optimal}{{74}{2015}{{Nielsen et~al.}}{{Nielsen, Jensen, and Ramamoorthi}}}
\bibcite{Omiya18Learning}{{75}{2018}{{Omiya et~al.}}{{Omiya, Simo-Serra, Iizuka, and Ishikawa}}}
\bibcite{park2019deepsdf}{{76}{2019}{{Park et~al.}}{{Park, Florence, Straub, Newcombe, and Lovegrove}}}
\bibcite{park2018distort}{{77}{2018}{{Park et~al.}}{{Park, Lee, Yoo, and Kweon}}}
\bibcite{specfig}{{78}{2021}{{Park and Baek}}{{}}}
\bibcite{Pitie05NDimensional}{{79}{2005}{{Pitie et~al.}}{{Pitie, Kokaram, and Dahyot}}}
\bibcite{Pitie07Automated}{{80}{2007}{{Piti{\'e} et~al.}}{{Piti{\'e}, Kokaram, and Dahyot}}}
\bibcite{Pouli11Progressive}{{81}{2011}{{Pouli and Reinhard}}{{}}}
\bibcite{rainer2019neural}{{82}{2019}{{Rainer et~al.}}{{Rainer, Jakob, Ghosh, and Weyrich}}}
\bibcite{ratzlaff2019hypergan}{{83}{2019}{{Ratzlaff and Fuxin}}{{}}}
\bibcite{rebain2022attention}{{84}{2022}{{Rebain et~al.}}{{Rebain, Matthews, Yi, Sharma, Lagun, and Tagliasacchi}}}
\bibcite{Reinhard01Color}{{85}{2001}{{Reinhard et~al.}}{{Reinhard, Ashikhmin, Gooch, and Shirley}}}
\bibcite{ronneberger2015u}{{86}{2015}{{Ronneberger et~al.}}{{Ronneberger, Fischer, and Brox}}}
\bibcite{rusinkiewicz1998new}{{87}{1998}{{Rusinkiewicz}}{{}}}
\bibcite{Saeedi18Multimodal}{{88}{2018}{{Saeedi et~al.}}{{Saeedi, Hoffman, DiVerdi, Ghandeharioun, Johnson, and Adams}}}
\bibcite{schewe2007color}{{89}{2007}{{Schewe and Fraser}}{{}}}
\bibcite{serrano2018intuitive}{{90}{2018}{{Serrano et~al.}}{{Serrano, Gutierrez, Myszkowski, Seidel, and Masia}}}
\bibcite{shaham2021spatially}{{91}{2021}{{Shaham et~al.}}{{Shaham, Gharbi, Zhang, Shechtman, and Michaeli}}}
\bibcite{Shapira13Image}{{92}{2013}{{Shapira et~al.}}{{Shapira, Avidan, and Hel-Or}}}
\bibcite{Shih13Data}{{93}{2013}{{Shih et~al.}}{{Shih, Paris, Durand, and Freeman}}}
\bibcite{Shih14Style}{{94}{2014}{{Shih et~al.}}{{Shih, Paris, Barnes, Freeman, and Durand}}}
\bibcite{sitzmann2020metasdf}{{95}{2020{}}{{Sitzmann et~al.}}{{Sitzmann, Chan, Tucker, Snavely, and Wetzstein}}}
\bibcite{sitzmann2020siren}{{96}{2020{}}{{Sitzmann et~al.}}{{Sitzmann, Martel, Bergman, Lindell, and Wetzstein}}}
\bibcite{sun2007interactive}{{97}{2007}{{Sun et~al.}}{{Sun, Zhou, Chen, Lin, Shi, and Guo}}}
\bibcite{Sunkavalli10Multi}{{98}{2010}{{Sunkavalli et~al.}}{{Sunkavalli, Johnson, Matusik, and Pfister}}}
\bibcite{sztrajman2021neural}{{99}{2021}{{Sztrajman et~al.}}{{Sztrajman, Rainer, Ritschel, and Weyrich}}}
\bibcite{Tai07Soft}{{100}{2007}{{Tai et~al.}}{{Tai, Jia, and Tang}}}
\bibcite{Tai05Local}{{101}{2005}{{Tai et~al.}}{{Tai, Jia, and Tang}}}
\bibcite{ffn}{{102}{2020}{{Tancik et~al.}}{{Tancik, Srinivasan, Mildenhall, Fridovich-Keil, Raghavan, Singhal, Ramamoorthi, Barron, and Ng}}}
\bibcite{tolstikhin2021mlp}{{103}{2021}{{Tolstikhin et~al.}}{{Tolstikhin, Houlsby, Kolesnikov, Beyer, Zhai, Unterthiner, Yung, Steiner, Keysers, Uszkoreit, et~al.}}}
\bibcite{tongbuasirilai2020compact}{{104}{2020}{{Tongbuasirilai et~al.}}{{Tongbuasirilai, Unger, Kronander, and Kurt}}}
\bibcite{tseng2019hyperparameter}{{105}{2019}{{Tseng et~al.}}{{Tseng, Yu, Yang, Mannan, Arnaud, Nowrouzezahrai, Lalonde, and Heide}}}
\bibcite{tseng2022neural}{{106}{2022}{{Tseng et~al.}}{{Tseng, Zhang, Jebe, Zhang, Xia, Fan, Heide, and Chen}}}
\bibcite{tunwattanapong2013acquiring}{{107}{2013}{{Tunwattanapong et~al.}}{{Tunwattanapong, Fyffe, Graham, Busch, Yu, Ghosh, and Debevec}}}
\bibcite{walter2007microfacet}{{108}{2007}{{Walter et~al.}}{{Walter, Marschner, Li, and Torrance}}}
\bibcite{wang2022attention}{{109}{2022}{{Wang et~al.}}{{Wang, Chen, Chen, Venugopalan, Wang, et~al.}}}
\bibcite{wang2019underexposed}{{110}{2019}{{Wang et~al.}}{{Wang, Zhang, Fu, Shen, Zheng, and Jia}}}
\bibcite{ward1992}{{111}{1992}{{Ward}}{{}}}
\bibcite{xu2015deep}{{112}{2015}{{Xu et~al.}}{{Xu, Ren, Yan, Liao, and Jia}}}
\bibcite{Yan14Automatic}{{113}{2014}{{Yan et~al.}}{{Yan, Zhang, Wang, Paris, and Yu}}}
\bibcite{10.1145/2790296}{{114}{2016{}}{{Yan et~al.}}{{Yan, Zhang, Wang, Paris, and Yu}}}
\bibcite{yan2016automatic}{{115}{2016{}}{{Yan et~al.}}{{Yan, Zhang, Wang, Paris, and Yu}}}
\bibcite{yang2023contranerf}{{116}{2023}{{Yang et~al.}}{{Yang, Hong, Li, Hu, Li, Lee, and Wang}}}
\bibcite{yang2020fidelity}{{117}{2020}{{Yang et~al.}}{{Yang, Wang, Fang, Wang, and Liu}}}
\bibcite{yu2021reconfigisp}{{118}{2021}{{Yu et~al.}}{{Yu, Li, Peng, Loy, and Gu}}}
\bibcite{zaheer2017deepsets}{{119}{2017}{{Zaheer et~al.}}{{Zaheer, Kottur, Ravanbakhsh, Poczos, Salakhutdinov, and Smola}}}
\bibcite{Zhang13Style}{{120}{2013}{{Zhang et~al.}}{{Zhang, Cao, Chen, Liu, and Tang}}}
\bibcite{zheng2021compact}{{121}{2021}{{Zheng et~al.}}{{Zheng, Zheng, Wang, Zhao, and Bao}}}
\bibcite{cnf2023}{{122}{2023}{{Zhong et~al.}}{{Zhong, Fogarty, Hanji, Wu, Sztrajman, Spielberg, Tagliasacchi, Bosilj, and Oztireli}}}
\bibcite{zhou2021adversarial}{{123}{2021}{{Zhou and Kalantari}}{{}}}
\bibcite{Zhu18Automatic}{{124}{2018}{{Zhu and Yu}}{{}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Extra Information}{77}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\gdef \@abspage@last{78}
