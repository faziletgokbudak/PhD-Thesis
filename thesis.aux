\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{thesis.ist}
\@glsorder{word}
\providecommand \oddpage@label [2]{}
\citation{}
\citation{}
\@writefile{toc}{\thispagestyle {empty}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{15}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Research Vision and Motivation}{15}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Contributions}{15}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Relationship to Published Work}{15}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{17}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Rendering}{17}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Displaying an image}{17}{subsection.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A pixel, a picture element, is the smallest unit of a rendered image. Displaying an image with fewer pixels causes losses in details as one pixel starts covering a larger area in the 3D scene.}}{17}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:colour-approximate}{{2.1}{17}{A pixel, a picture element, is the smallest unit of a rendered image. Displaying an image with fewer pixels causes losses in details as one pixel starts covering a larger area in the 3D scene}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces To compute the colour of a pixel with variations, it is common to divide the pixel into sub-pixels and take the weighted average of each sub-pixel.}}{18}{figure.caption.6}\protected@file@percent }
\newlabel{fig:display-grid}{{2.2}{18}{To compute the colour of a pixel with variations, it is common to divide the pixel into sub-pixels and take the weighted average of each sub-pixel}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Comparison of different colour gamuts. Image from [Schewe and Fraser, 2007].}}{18}{figure.2.3}\protected@file@percent }
\newlabel{fig:colour-gamut}{{2.3}{18}{Comparison of different colour gamuts. Image from [Schewe and Fraser, 2007]}{figure.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces When a colour is represented with too few bits, the transition between colours appears as discrete steps, known as colour banding (right).}}{19}{figure.caption.7}\protected@file@percent }
\newlabel{fig:colour-band}{{2.4}{19}{When a colour is represented with too few bits, the transition between colours appears as discrete steps, known as colour banding (right)}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Rendering an image of a 3D scene requires lighting, viewpoint (camera) and geometry. Image from [Boss et al., 2021].}}{20}{figure.caption.8}\protected@file@percent }
\newlabel{fig:teaser}{{2.5}{20}{Rendering an image of a 3D scene requires lighting, viewpoint (camera) and geometry. Image from [Boss et al., 2021]}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}3D scene components}{20}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Geometry.}{20}{section*.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Triangulation of surfaces is a commonly used method in computer graphics to define the shape of objects. It allows the creation of complex shapes. Increasing the number of triangles also increases the resolution of the shape that leads to smoother surface representations. Image taken from [Broeren et al., 2019].}}{21}{figure.caption.10}\protected@file@percent }
\newlabel{fig:triangulation}{{2.6}{21}{Triangulation of surfaces is a commonly used method in computer graphics to define the shape of objects. It allows the creation of complex shapes. Increasing the number of triangles also increases the resolution of the shape that leads to smoother surface representations. Image taken from [Broeren et al., 2019]}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Photo-realistic rendering}{22}{subsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Perspective projection and visibility.}{22}{section*.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Perspective projection mimics the foreshortening effect our visual system creates while observing a scene. The image plane/canvas includes the 2D projections of the visible objects within the view frustum.}}{23}{figure.caption.12}\protected@file@percent }
\newlabel{fig:perspective_projection}{{2.7}{23}{Perspective projection mimics the foreshortening effect our visual system creates while observing a scene. The image plane/canvas includes the 2D projections of the visible objects within the view frustum}{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Single-point vs two-point perspective. Computer graphics mimics our visual system, focusing on single-point projection. However, multi-point perspective is widely explored in artistic drawing.}}{23}{figure.caption.13}\protected@file@percent }
\newlabel{fig:artistic_drawing}{{2.8}{23}{Single-point vs two-point perspective. Computer graphics mimics our visual system, focusing on single-point projection. However, multi-point perspective is widely explored in artistic drawing}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {paragraph}{Appearance.}{23}{section*.14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces The reflected light from the surface of an object defines its colour. Image from \href  {http://www.mstworkbooks.co.za/natural-sciences/gr8/images/gr8ec04-gd-0027.png}{website link}.}}{24}{figure.2.9}\protected@file@percent }
\newlabel{fig:object-colour}{{2.9}{24}{The reflected light from the surface of an object defines its colour. Image from \href {http://www.mstworkbooks.co.za/natural-sciences/gr8/images/gr8ec04-gd-0027.png}{website link}}{figure.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces The ripples on the water's surface cause a blurred image of the bridge, acting as a rough surface.}}{24}{figure.2.10}\protected@file@percent }
\newlabel{fig:water_reflection}{{2.10}{24}{The ripples on the water's surface cause a blurred image of the bridge, acting as a rough surface}{figure.2.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Reflection on a surface with microfacets vs smooth surface. Image from [Guy and Agopian, 2024]}}{25}{figure.caption.15}\protected@file@percent }
\newlabel{fig:microfacet}{{2.11}{25}{Reflection on a surface with microfacets vs smooth surface. Image from [Guy and Agopian, 2024]}{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Specular vs diffuse reflection. Diffuse surfaces cause rays to scatter randomly, brightening the object equally in all viewing directions. Specular reflection is, on the other hand, view-dependent, reflected rays concentrated around the mirror reflection. Image from [Park and Baek, 2021].}}{25}{figure.caption.16}\protected@file@percent }
\newlabel{fig:specularvsdiffuse}{{2.13}{25}{Specular vs diffuse reflection. Diffuse surfaces cause rays to scatter randomly, brightening the object equally in all viewing directions. Specular reflection is, on the other hand, view-dependent, reflected rays concentrated around the mirror reflection. Image from [Park and Baek, 2021]}{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Diffuse surfaces often have complex internal structures that cause the light to be reflected multiple times underneath the surface.}}{25}{figure.2.12}\protected@file@percent }
\newlabel{fig:diffuse-scattering}{{2.12}{25}{Diffuse surfaces often have complex internal structures that cause the light to be reflected multiple times underneath the surface}{figure.2.12}{}}
\@writefile{toc}{\contentsline {paragraph}{Ray tracing - Light transport simulation.}{26}{section*.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Ray tracing simulate the light's journey to compute the colour of each pixel on the rendered image.Image from [Henrik, 2008].}}{27}{figure.caption.18}\protected@file@percent }
\newlabel{fig:raytracing}{{2.14}{27}{Ray tracing simulate the light's journey to compute the colour of each pixel on the rendered image.Image from [Henrik, 2008]}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Shaders and BRDF}{28}{subsection.2.1.4}\protected@file@percent }
\citation{Matusik2003jul}
\citation{Matusik2003jul}
\citation{phong1998illumination}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces A material composed of both specular and diffuse components, lit by a point light source. The specular highlight (white circle) is centered around the light source direction. The material is specular-maroon-phenolic from MERL dataset \cite  {Matusik2003jul}.}}{29}{figure.2.15}\protected@file@percent }
\newlabel{fig:diffuse+spec}{{2.15}{29}{A material composed of both specular and diffuse components, lit by a point light source. The specular highlight (white circle) is centered around the light source direction. The material is specular-maroon-phenolic from MERL dataset \cite {Matusik2003jul}}{figure.2.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Diffuse and specular reflections.. Figures from [Mantiuk, 2022]}}{29}{figure.caption.19}\protected@file@percent }
\newlabel{fig:diffuse-spec-angle}{{2.16}{29}{Diffuse and specular reflections.. Figures from [Mantiuk, 2022]}{figure.caption.19}{}}
\newlabel{eq:Phong-eq}{{2.4}{29}{Shaders and BRDF}{equation.2.1.4}{}}
\citation{cooktorrance1982}
\citation{ward1992}
\citation{walter2007microfacet}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces Phong shading model implementation with decreasing roughness/increasing glossiness from left to right.}}{30}{figure.caption.20}\protected@file@percent }
\newlabel{fig:phong-roughness}{{2.17}{30}{Phong shading model implementation with decreasing roughness/increasing glossiness from left to right}{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces BRDF depends on the incident light and viewing direction.}}{30}{figure.2.18}\protected@file@percent }
\newlabel{fig:brdf}{{2.18}{30}{BRDF depends on the incident light and viewing direction}{figure.2.18}{}}
\@writefile{toc}{\contentsline {paragraph}{BRDF.}{30}{section*.21}\protected@file@percent }
\citation{Matusik2003jul}
\citation{Chenliang's paper}
\citation{Chenliang's paper}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Rendering Equation}{32}{subsection.2.1.5}\protected@file@percent }
\newlabel{eqn:rendering-eqn}{{2.5}{32}{Rendering Equation}{equation.2.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Machine learning}{32}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Foundations}{33}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Types of learning.}{33}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training.}{33}{section*.23}\protected@file@percent }
\citation{kingma2014adam}
\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces Gradient descent steps.}}{34}{figure.2.19}\protected@file@percent }
\newlabel{fig:gradient-descent}{{2.19}{34}{Gradient descent steps}{figure.2.19}{}}
\citation{johnson2016perceptuallossesrealtimestyle}
\@writefile{toc}{\contentsline {paragraph}{Loss function.}{35}{section*.24}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.20}{\ignorespaces L1 vs L2 loss values for a single data point. Ground truth $y$ is set to 0.}}{35}{figure.caption.25}\protected@file@percent }
\newlabel{fig:l1vsl2loss}{{2.20}{35}{L1 vs L2 loss values for a single data point. Ground truth $y$ is set to 0}{figure.caption.25}{}}
\@writefile{toc}{\contentsline {paragraph}{Regularisation.}{35}{section*.26}\protected@file@percent }
\newlabel{MSE-with-L2reg}{{2.6}{36}{Regularisation}{equation.2.2.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Data augmentation}{36}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Multilayer Perceptrons}{36}{subsection.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.21}{\ignorespaces Graph representation of a linear equation. Figure from [Issac, 2018].}}{37}{figure.2.21}\protected@file@percent }
\newlabel{fig:neuron}{{2.21}{37}{Graph representation of a linear equation. Figure from [Issac, 2018]}{figure.2.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.22}{\ignorespaces A basic multilayer perceptron model. Figure from [Kishore NG, 2023].}}{38}{figure.caption.28}\protected@file@percent }
\newlabel{fig:mlp}{{2.22}{38}{A basic multilayer perceptron model. Figure from [Kishore NG, 2023]}{figure.caption.28}{}}
\@writefile{toc}{\contentsline {paragraph}{Activation functions.}{38}{section*.29}\protected@file@percent }
\citation{cybenko1989approximation}
\citation{ffn}
\citation{nguyen2015deep}
\citation{mildenhall2021nerf}
\citation{park2019deepsdf}
\citation{sztrajman2021neural}
\citation{rahaman2019spectral}
\citation{ffn}
\citation{ffn}
\citation{rahimi2007random}
\citation{sitzmann2020siren}
\citation{sitzmann2020siren}
\newlabel{eq:sine-act}{{2.11}{39}{Activation functions}{equation.2.2.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Neural implicit representations}{39}{subsection.2.2.3}\protected@file@percent }
\citation{ffn}
\citation{ffn}
\newlabel{eq:ffn}{{2.12}{40}{Neural implicit representations}{equation.2.2.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.23}{\ignorespaces To overcome the spectral bias in coordinate-based MLPs, a Fourier feature mapping \cite  {ffn} can be applied before feeding the input to the model. Here, 'none' represents an MLP model without mapping. 'basic' and 'gauss' are different variants of Fourier feature maps, and 'GT' is the ground truth.}}{40}{figure.caption.30}\protected@file@percent }
\newlabel{fig:ffn}{{2.23}{40}{To overcome the spectral bias in coordinate-based MLPs, a Fourier feature mapping \cite {ffn} can be applied before feeding the input to the model. Here, 'none' represents an MLP model without mapping. 'basic' and 'gauss' are different variants of Fourier feature maps, and 'GT' is the ground truth}{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Diffusion models}{40}{subsection.2.2.4}\protected@file@percent }
\citation{shaham2021spatially,li2020lapar}
\citation{moran2020deeplpf}
\citation{Gharbi17Deep}
\citation{yan2016automatic}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}One-shot Detail Retouching}{41}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{41}{section.3.1}\protected@file@percent }
\newlabel{sec:introduction}{{3.1}{41}{Introduction}{section.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Our technique automatically transfers retouching edits to new images by learning the desired edits from one example \textit  {before-after} pair (insets). The transferred edits accurately capture intricate details such as wrinkles, dark spots, strands of hair, or eyelashes, as shown in the input (top) and retouched (bottom) pairs. Image courtesy of Jenavieve (top-left), Logan ProPro (top-left, inset), Marissa Oosterlee (top-middle). (CC-BY).}}{42}{figure.caption.31}\protected@file@percent }
\newlabel{fig:teaser}{{3.1}{42}{Our technique automatically transfers retouching edits to new images by learning the desired edits from one example \textit {before-after} pair (insets). The transferred edits accurately capture intricate details such as wrinkles, dark spots, strands of hair, or eyelashes, as shown in the input (top) and retouched (bottom) pairs. Image courtesy of Jenavieve (top-left), Logan ProPro (top-left, inset), Marissa Oosterlee (top-middle). (CC-BY)}{figure.caption.31}{}}
\citation{Faridul14ASurvey,mustafa2022distilling}
\citation{Bychkovsky11Learning,Bae06Two,Pitie05NDimensional,Pitie07Automated,Reinhard01Color,Sunkavalli10Multi,he2020conditional,park2018distort}
\citation{CohenOr06Color}
\citation{Bychkovsky11Learning}
\citation{Bychkovsky11Learning}
\citation{chen2017fast}
\citation{chen2017fast}
\citation{Hu18Exposure}
\citation{Hu18Exposure}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Related Work}{43}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Digital Photo Enhancement}{43}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Global image enhancement.}{43}{section*.32}\protected@file@percent }
\citation{CohenOr06Color}
\citation{kim2021representative}
\citation{wang2019underexposed}
\citation{Shapira13Image}
\citation{Laffont14Transient,Tai07Soft}
\citation{Berthouzoz11AFramework,Chen18Deep,Huang14Parametric,Omiya18Learning,Saeedi18Multimodal}
\citation{An10User,Pouli11Progressive,Tai05Local}
\citation{Gharbi17Deep,Hwang12Context,Kaufman12Content,Nam17Deep,Yan14Automatic,Zhu18Automatic}
\citation{HaCohen11Nonrigid}
\citation{Kagarlitsky09Piecewise,Shih13Data}
\citation{moran2020deeplpf,Gharbi17Deep,chen2018deep,shaham2021spatially,li2020lapar}
\citation{chen2018deep}
\citation{chen2018deep}
\citation{chen2016bilateral}
\citation{Gharbi17Deep}
\citation{moran2020deeplpf}
\citation{moran2020deeplpf}
\citation{Bae06Two}
\citation{HaCohen11Nonrigid}
\citation{Shih14Style}
\citation{Shih14Style}
\citation{tseng2019hyperparameter,yu2021reconfigisp}
\citation{tseng2019hyperparameter}
\citation{yu2021reconfigisp}
\citation{yu2021reconfigisp}
\citation{tseng2022neural}
\citation{tseng2022neural}
\@writefile{toc}{\contentsline {paragraph}{Local context-aware image enhancement.}{44}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Differentiable image processing pipelines.}{44}{section*.34}\protected@file@percent }
\citation{ma2021retinexgan}
\citation{yang2020fidelity}
\citation{9334429}
\citation{Liu16Makeup}
\citation{Frigo16Split}
\citation{Chen18Deep}
\citation{lin2020tuigan}
\citation{Zhang13Style,Hu18Exposure}
\citation{visual_attribute}
\citation{visual_attribute}
\citation{Hertzmann01Image}
\citation{kim2021representative,wang2019underexposed}
\citation{10.1145/2790296}
\citation{tolstikhin2021mlp}
\citation{cazenavette2021mixergan}
\citation{cazenavette2021mixergan}
\citation{cazenavette2021mixergan}
\citation{cazenavette2021mixergan}
\citation{tolstikhin2021mlp}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Defining Maps between Images}{45}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Unsupervised methods.}{45}{section*.35}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Supervised methods.}{45}{section*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Overview and Motivations}{46}{section.3.3}\protected@file@percent }
\newlabel{chap:motivations}{{3.3}{46}{Overview and Motivations}{section.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Frequency decomposition allows artists to have a higher control over different frequency components of an image. Here, the layer shown in the top-left corner is obtained by high-pass filtering of the portrait in the background. Screenshots from [Eustace Kanyanda, 2022].}}{46}{figure.caption.37}\protected@file@percent }
\newlabel{fig:PS-high-pass}{{3.2}{46}{Frequency decomposition allows artists to have a higher control over different frequency components of an image. Here, the layer shown in the top-left corner is obtained by high-pass filtering of the portrait in the background. Screenshots from [Eustace Kanyanda, 2022]}{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Local filters, such as brushes, help smoothing the skin by removing the undesired pores on the face. However, it is a tedious task for artists, requiring them to go through each of the visible pores.}}{47}{figure.caption.38}\protected@file@percent }
\newlabel{fig:PS-brush}{{3.3}{47}{Local filters, such as brushes, help smoothing the skin by removing the undesired pores on the face. However, it is a tedious task for artists, requiring them to go through each of the visible pores}{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Artists usually work on separate layers, adjusting varying properties, such as texture, colour, etc. Later, these layers are blended together with their corresponding "opacity" values, shown in the box above the layers.}}{47}{figure.caption.39}\protected@file@percent }
\newlabel{fig:PS-all-together}{{3.4}{47}{Artists usually work on separate layers, adjusting varying properties, such as texture, colour, etc. Later, these layers are blended together with their corresponding "opacity" values, shown in the box above the layers}{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The proposed technique learns a separate mapping per frequency band by decomposing images into five different bands with a Laplacian pyramid. At each Laplacian band $l$, a separate mapping is defined between flattened patches $\mathbf  {x}_i$, $\mathbf  {y}_i$ extracted from before-after bands $X_l$, $Y_l$. The field based method (MLP block) adapts transformations to input patches, providing local context-aware adjustments. The transformation matrices and MLP parameters are learned jointly from scratch for each Laplacian band of the before-after pair.}}{48}{figure.caption.40}\protected@file@percent }
\newlabel{fig:modelT}{{3.5}{48}{The proposed technique learns a separate mapping per frequency band by decomposing images into five different bands with a Laplacian pyramid. At each Laplacian band $l$, a separate mapping is defined between flattened patches $\mathbf {x}_i$, $\mathbf {y}_i$ extracted from before-after bands $X_l$, $Y_l$. The field based method (MLP block) adapts transformations to input patches, providing local context-aware adjustments. The transformation matrices and MLP parameters are learned jointly from scratch for each Laplacian band of the before-after pair}{figure.caption.40}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}One-shot Retouching}{49}{section.3.4}\protected@file@percent }
\newlabel{sec:Methodology}{{3.4}{49}{One-shot Retouching}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Frequency Decomposition}{49}{subsection.3.4.1}\protected@file@percent }
\newlabel{sec:thePatchMap}{{3.4.1}{49}{Frequency Decomposition}{subsection.3.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Transformation Blending}{49}{subsection.3.4.2}\protected@file@percent }
\newlabel{sec:Blending}{{3.4.2}{49}{Transformation Blending}{subsection.3.4.2}{}}
\newlabel{eq:weightedSum}{{3.2}{49}{Transformation Blending}{equation.3.4.2}{}}
\citation{Boyadzhiev15Band}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Retouching an Input Image}{50}{subsection.3.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Implementation}{50}{subsection.3.4.4}\protected@file@percent }
\newlabel{sec:Implementation}{{3.4.4}{50}{Implementation}{subsection.3.4.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Patch size and stride.}{50}{section*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Detail and colour modifications.}{50}{section*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Evaluation metrics.}{50}{section*.43}\protected@file@percent }
\newlabel{train_det}{{3.4.4}{50}{Training details}{section*.44}{}}
\@writefile{toc}{\contentsline {paragraph}{Training details.}{50}{section*.44}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces One-shot Detail Retouching}}{51}{algocf.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Results}{52}{section.3.5}\protected@file@percent }
\newlabel{sec:results}{{3.5}{52}{Results}{section.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Ablation Studies}{52}{subsection.3.5.1}\protected@file@percent }
\newlabel{ablation}{{3.5.1}{52}{Ablation Studies}{subsection.3.5.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Transformation Matrices.}{52}{section*.45}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The higher the complexity of the learned algorithm, the more transformation matrices the technique requires to capture the effects on local regions accurately. While $K=1$ can be sufficient for the model to capture unsharp masking, it requires more matrices to represent bilateral filtering precisely.}}{52}{figure.caption.46}\protected@file@percent }
\newlabel{fig:ablation_K}{{3.6}{52}{The higher the complexity of the learned algorithm, the more transformation matrices the technique requires to capture the effects on local regions accurately. While $K=1$ can be sufficient for the model to capture unsharp masking, it requires more matrices to represent bilateral filtering precisely}{figure.caption.46}{}}
\@writefile{toc}{\contentsline {paragraph}{Patch-adaptive Transformation Blending.}{52}{section*.48}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces An MLP regressor cannot capture local edits, resulting in inaccurate retouching edits, such as blurring on the skin or around the eyes.}}{53}{figure.caption.47}\protected@file@percent }
\newlabel{fig:ablation_MLP}{{3.7}{53}{An MLP regressor cannot capture local edits, resulting in inaccurate retouching edits, such as blurring on the skin or around the eyes}{figure.caption.47}{}}
\@writefile{toc}{\contentsline {paragraph}{Weights visualisation.}{53}{section*.49}\protected@file@percent }
\citation{karras2019style}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Visualisation of the reconstructed patch-adaptive weights of the model trained with example images in the teaser. Here, the input to the model is shown on top, and each row shows the weights in the corresponding Laplacian band, indicated on the left.}}{54}{figure.caption.50}\protected@file@percent }
\newlabel{fig:weight-vis}{{3.8}{54}{Visualisation of the reconstructed patch-adaptive weights of the model trained with example images in the teaser. Here, the input to the model is shown on top, and each row shows the weights in the corresponding Laplacian band, indicated on the left}{figure.caption.50}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Qualitative Results}{54}{subsection.3.5.2}\protected@file@percent }
\citation{ronneberger2015u}
\citation{shaham2021spatially}
\citation{xu2015deep}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces The reproduced retouching style from the example pair (inset) improves skin texture without affecting fine details, such as eyes and hair, for a visually improved portrait. Moreover, the technique generalises well to faces with different lighting conditions and accurately reproduces the example retouching style.}}{55}{figure.caption.51}\protected@file@percent }
\newlabel{fig:newdataset_ex}{{3.9}{55}{The reproduced retouching style from the example pair (inset) improves skin texture without affecting fine details, such as eyes and hair, for a visually improved portrait. Moreover, the technique generalises well to faces with different lighting conditions and accurately reproduces the example retouching style}{figure.caption.51}{}}
\citation{Bychkovsky11Learning}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Material editing results on photos (left), and rendered images (right), based on the before-after pair (inset). The details, such as scratches or lines are emphasised, and materials became shinier. Image courtesy of royalmix (top and bottom-inset), tsmdunn (bottom). (PixelSquid).}}{56}{figure.caption.52}\protected@file@percent }
\newlabel{fig:material_res}{{3.10}{56}{Material editing results on photos (left), and rendered images (right), based on the before-after pair (inset). The details, such as scratches or lines are emphasised, and materials became shinier. Image courtesy of royalmix (top and bottom-inset), tsmdunn (bottom). (PixelSquid)}{figure.caption.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Comparison with the state-of-the-art}{56}{subsection.3.5.3}\protected@file@percent }
\newlabel{sec:Comparisons}{{3.5.3}{56}{Comparison with the state-of-the-art}{subsection.3.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Our patch-adaptive technique can accurately capture the nuances between different retouching styles as given by the examples (top row).}}{57}{figure.caption.53}\protected@file@percent }
\newlabel{fig:retouchingstyles}{{3.11}{57}{Our patch-adaptive technique can accurately capture the nuances between different retouching styles as given by the examples (top row)}{figure.caption.53}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Quantitative performance comparison for the reproduction of various image processing filters. Average PSNR and SSIM values are computed over 182 images of different types of images including faces, landscapes, materials, and rooms. Qualitative results can be found in the supplementary material.We highlight \colorbox {blue!25}{best PSNR} and \colorbox {orange!25}{best SSIM} results.}}{58}{table.caption.54}\protected@file@percent }
\newlabel{tablecomparison}{{3.1}{58}{Quantitative performance comparison for the reproduction of various image processing filters. Average PSNR and SSIM values are computed over 182 images of different types of images including faces, landscapes, materials, and rooms. Qualitative results can be found in the supplementary material.We highlight \colorbox {blue!25}{best PSNR} and \colorbox {orange!25}{best SSIM} results}{table.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Qualitative comparison with baseline approaches on bilateral filtering. Bilateral filter is a nonlinear filter designed for smoothing the surfaces while preserving the edges. Results show that our proposed technique captures the bilateral-filtering effect more effectively, reducing the roughness of the surfaces while maintaining important details.}}{59}{figure.caption.55}\protected@file@percent }
\newlabel{fig:QualitativeComp_BF}{{3.12}{59}{Qualitative comparison with baseline approaches on bilateral filtering. Bilateral filter is a nonlinear filter designed for smoothing the surfaces while preserving the edges. Results show that our proposed technique captures the bilateral-filtering effect more effectively, reducing the roughness of the surfaces while maintaining important details}{figure.caption.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Qualitative comparison with baseline approaches on Local Laplacian filter (LLF) ($\alpha =2, \sigma =0.2$). LLF is another edge-aware nonlinear filter designed for multiple tasks, namely edge-preserving smoothing, detail enhancement, tone mapping, and inverse tone mapping. In these examples, the parameters are chosen to have an edge-preserving smoothing effect, similar to bilateral filtering. Results show that ours offer smoother textures.}}{60}{figure.caption.56}\protected@file@percent }
\newlabel{fig:QualitativeComp_LLF_a2}{{3.13}{60}{Qualitative comparison with baseline approaches on Local Laplacian filter (LLF) ($\alpha =2, \sigma =0.2$). LLF is another edge-aware nonlinear filter designed for multiple tasks, namely edge-preserving smoothing, detail enhancement, tone mapping, and inverse tone mapping. In these examples, the parameters are chosen to have an edge-preserving smoothing effect, similar to bilateral filtering. Results show that ours offer smoother textures}{figure.caption.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Qualitative comparison with baseline approaches on Local Laplacian filter (LLF) ($\alpha =0.5, \sigma =0.1$). Here, the parameters are chosen accordingly to enhance the details. Our results highlight edges, such as the transition between the cloud and the sky, the numbers in the clock or the texture on the bed sheet.}}{61}{figure.caption.57}\protected@file@percent }
\newlabel{fig:QualitativeComp_LLF_a05}{{3.14}{61}{Qualitative comparison with baseline approaches on Local Laplacian filter (LLF) ($\alpha =0.5, \sigma =0.1$). Here, the parameters are chosen accordingly to enhance the details. Our results highlight edges, such as the transition between the cloud and the sky, the numbers in the clock or the texture on the bed sheet}{figure.caption.57}{}}
\citation{Shih14Style}
\citation{Yan14Automatic}
\citation{Besl92AMethod}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Limitations and Future Work}{62}{subsection.3.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces  The proposed technique cannot accurately handle extreme non-repeating local effects such as tattoos (top), and when example and input images are of very different semantics (bottom). Image courtesy of bgaj23 (coin). (PixelSquid).}}{62}{figure.caption.58}\protected@file@percent }
\newlabel{fig:limitations}{{3.15}{62}{The proposed technique cannot accurately handle extreme non-repeating local effects such as tattoos (top), and when example and input images are of very different semantics (bottom). Image courtesy of bgaj23 (coin). (PixelSquid)}{figure.caption.58}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Conclusions}{63}{section.3.6}\protected@file@percent }
\citation{rombach2022high}
\citation{zhang2023adding}
\citation{laffont2014transient}
\citation{laffont2014transient}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Text-guided Transient Attribute Transfer}{65}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{zero-shot}{{4}{65}{Text-guided Transient Attribute Transfer}{chapter.4}{}}
\citation{laffont2014transient}
\citation{laffont2014transient}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{66}{section.4.1}\protected@file@percent }
\newlabel{sec:zero-shot-intro}{{4.1}{66}{Introduction}{section.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Transition of the illumination throughout the day or the weather changes across seasons alter the scene appearance significantly in terms of colour, tone, texture, and style. However, main content, such as the structure, permanent objects, etc. remains unchanged. Transient attribute transfer aims to capture alterations related to such temporal effects. Images from the Transient Attribute Dataset \cite  {laffont2014transient}.}}{66}{figure.caption.59}\protected@file@percent }
\newlabel{fig:zero-shot-teaser}{{4.1}{66}{Transition of the illumination throughout the day or the weather changes across seasons alter the scene appearance significantly in terms of colour, tone, texture, and style. However, main content, such as the structure, permanent objects, etc. remains unchanged. Transient attribute transfer aims to capture alterations related to such temporal effects. Images from the Transient Attribute Dataset \cite {laffont2014transient}}{figure.caption.59}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Related Work}{67}{section.4.2}\protected@file@percent }
\newlabel{zero-shot-RW}{{4.2}{67}{Related Work}{section.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}High-level image editing}{67}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Diffusion models for high-level image editing}{67}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Conclusions}{67}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}HyperBRDF: Neural Generalizable Material Representation}{69}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:HyperBRDF}{{5}{69}{HyperBRDF: Neural Generalizable Material Representation}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{69}{section.5.1}\protected@file@percent }
\newlabel{sec:intro}{{5.1}{69}{Introduction}{section.5.1}{}}
\citation{ngan2005}
\citation{dupuy2015,guarnera2016}
\citation{nielsen2015optimal}
\citation{sitzmann2020siren,ffn,cnf2023}
\citation{sztrajman2021neural,cnf2023}
\citation{rebain2022attention}
\citation{park2019deepsdf}
\citation{ha2017hypernetworks}
\citation{jiang2021cotr}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces A room scene rendered with materials reconstructed by HyperBRDF, including sparse reconstruction (table top and legs, door, door and picture frames, hinge), compression (two teapots on the left, door handle) and BRDF interpolation (right-most teapot). Scene courtesy of Benedikt Bitterli.}}{70}{figure.caption.60}\protected@file@percent }
\newlabel{fig:teaser}{{5.1}{70}{A room scene rendered with materials reconstructed by HyperBRDF, including sparse reconstruction (table top and legs, door, door and picture frames, hinge), compression (two teapots on the left, door handle) and BRDF interpolation (right-most teapot). Scene courtesy of Benedikt Bitterli}{figure.caption.60}{}}
\citation{Matusik2003jul}
\citation{dupuy2018adaptive}
\citation{blinn77}
\citation{cooktorrance1982}
\citation{ward1992}
\citation{walter2007microfacet}
\citation{dupuy2015,ashikhmin2007,bagher2016}
\citation{low2012}
\citation{ngan2005,guarnera2016}
\citation{matusik2003data,nielsen2015optimal,serrano2018intuitive}
\citation{lawrence2004efficient,lawrence2006inverse}
\citation{sun2007interactive}
\citation{bilgili2011general,tongbuasirilai2020compact}
\citation{bagher2016non}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Motivation and Impact}{72}{section.5.2}\protected@file@percent }
\newlabel{sec:hyperbrdf-mot}{{5.2}{72}{Motivation and Impact}{section.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Related Work}{72}{section.5.3}\protected@file@percent }
\newlabel{sec:relatedwork}{{5.3}{72}{Related Work}{section.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Analytic BRDF Models}{72}{subsection.5.3.1}\protected@file@percent }
\newlabel{hyperbrdf-RW}{{5.3.1}{72}{Analytic BRDF Models}{subsection.5.3.1}{}}
\citation{rainer2019neural,hu2020deepbrdf,sztrajman2021neural,zheng2021compact,maximov2019deep,chen2021invertible,fan2021neural,cnf2023}
\citation{maximov2019deep}
\citation{sztrajman2021neural}
\citation{cnf2023}
\citation{hu2020deepbrdf}
\citation{zheng2021compact}
\citation{nielsen2015optimal}
\citation{liu2023learning}
\citation{kang2018efficient,kang2019learning,ma2021free,ma2023opensvbrdf,tunwattanapong2013acquiring}
\citation{guo2021highlight,hui2017reflectance,deschaintre2018single,deschaintre2019flexible,martin2022materia,zhou2021adversarial,gao2019deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Regression-based BRDF Estimation}{73}{subsection.5.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Deep learning for BRDF modeling.}{73}{section*.61}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Efficient BRDF Acquisition}{73}{subsection.5.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Spatially-varying BRDFs (SVBRDF):}{73}{section*.62}\protected@file@percent }
\citation{ratzlaff2019hypergan}
\citation{erkocc2023hyperdiffusion}
\citation{wang2022attention,yang2023contranerf}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces During training, the set encoder and hypernetwork decoder are trained on a set of materials to predict the weights of hyponet (MLP) so that it can reconstruct the training set. The BRDF data is provided as a set of BRDF coordinates, $H_n,D_n$, and the corresponding reflectance values $f_r(H_n,D_n)$. To reconstruct a new material from a small set of BRDF reflectance samples, the trained set encoder and hypernetwork decoder are used to predict the weights of hyponet for the unknown material. Once those weights are known, we can query BRDF at any coordinates and for any new materials, conditioned on the embedding of their sampled BRDF values.}}{74}{figure.caption.63}\protected@file@percent }
\newlabel{fig:mainfig}{{5.2}{74}{During training, the set encoder and hypernetwork decoder are trained on a set of materials to predict the weights of hyponet (MLP) so that it can reconstruct the training set. The BRDF data is provided as a set of BRDF coordinates, $H_n,D_n$, and the corresponding reflectance values $f_r(H_n,D_n)$. To reconstruct a new material from a small set of BRDF reflectance samples, the trained set encoder and hypernetwork decoder are used to predict the weights of hyponet for the unknown material. Once those weights are known, we can query BRDF at any coordinates and for any new materials, conditioned on the embedding of their sampled BRDF values}{figure.caption.63}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.4}Hypernetworks and GNFs}{74}{subsection.5.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Methodology}{74}{section.5.4}\protected@file@percent }
\citation{nielsen2015optimal}
\citation{sztrajman2021neural}
\citation{rusinkiewicz1998new}
\citation{sitzmann2020siren}
\citation{sitzmann2020metasdf}
\citation{rusinkiewicz1998new}
\citation{zaheer2017deepsets}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Pre-processing}{75}{subsection.5.4.1}\protected@file@percent }
\newlabel{sec:pre-proc}{{5.4.1}{75}{Pre-processing}{subsection.5.4.1}{}}
\newlabel{eq:preprocess}{{5.1}{75}{Pre-processing}{equation.5.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Hypernetwork}{75}{subsection.5.4.2}\protected@file@percent }
\newlabel{sec:hypernet}{{5.4.2}{75}{Hypernetwork}{subsection.5.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2.1}Set Encoder}{75}{subsubsection.5.4.2.1}\protected@file@percent }
\citation{sztrajman2021neural}
\citation{ha2017hypernetworks}
\citation{ngan2005experimental}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2.2}Hypernetwork Decoder and Hyponet}{76}{subsubsection.5.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}Training}{76}{subsection.5.4.3}\protected@file@percent }
\newlabel{sec:traindet}{{5.4.3}{76}{Training}{subsection.5.4.3}{}}
\newlabel{eq:loss}{{5.2}{76}{Training}{equation.5.4.2}{}}
\newlabel{eq:Lrec}{{5.3}{76}{Training}{equation.5.4.3}{}}
\citation{Matusik2003jul}
\citation{dupuy2018adaptive}
\citation{Matusik2003jul}
\citation{rusinkiewicz1998new}
\@writefile{toc}{\contentsline {paragraph}{Inference:}{77}{section*.64}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Experiments}{77}{section.5.5}\protected@file@percent }
\newlabel{sec:exp}{{5.5}{77}{Experiments}{section.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Datasets and Baselines}{77}{subsection.5.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Sparse BRDF Reconstruction}{77}{subsection.5.5.2}\protected@file@percent }
\newlabel{sec:brdf_rec}{{5.5.2}{77}{Sparse BRDF Reconstruction}{subsection.5.5.2}{}}
\citation{walter2007microfacet}
\citation{sztrajman2021neural}
\citation{nielsen2015optimal}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces t-SNE clustering of the test embeddings with different sample sizes, including $N=8, 40, 160, 4\,000, 40\,000, 640\,000$.}}{78}{figure.caption.65}\protected@file@percent }
\newlabel{fig:tsne-vis-imputation}{{5.3}{78}{t-SNE clustering of the test embeddings with different sample sizes, including $N=8, 40, 160, 4\,000, 40\,000, 640\,000$}{figure.caption.65}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.2.1}Qualitative Comparison}{78}{subsubsection.5.5.2.1}\protected@file@percent }
\newlabel{sec:qual_comp}{{5.5.2.1}{78}{Qualitative Comparison}{subsubsection.5.5.2.1}{}}
\citation{dupuy2015photorealistic}
\citation{sztrajman2021neural}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Qualitative comparison results for reconstruction with small sample sizes. Thanks to the prior that the hypernetwork model learns for material appearance through training, it can accurately estimate the BRDFs of unseen materials and preserve the colours better than the baselines.}}{79}{figure.caption.66}\protected@file@percent }
\newlabel{fig:imp_comp_upt}{{5.4}{79}{Qualitative comparison results for reconstruction with small sample sizes. Thanks to the prior that the hypernetwork model learns for material appearance through training, it can accurately estimate the BRDFs of unseen materials and preserve the colours better than the baselines}{figure.caption.66}{}}
\citation{matusik2003data,ngan2006image}
\citation{nielsen2015optimal}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Average PSNR, Delta E (CIE 2000), SSIM, MAE, RMSE, and RAE results across different sample sizes. }}{80}{figure.caption.67}\protected@file@percent }
\newlabel{fig:imp_plots}{{5.5}{80}{Average PSNR, Delta E (CIE 2000), SSIM, MAE, RMSE, and RAE results across different sample sizes}{figure.caption.67}{}}
\citation{chen2021invertible}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.2.2}Quantitative Evaluation}{81}{subsubsection.5.5.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Hypernetwork sparse reconstruction - Average metric results across varying sample sizes ($N$) over the test set. We highlight \colorbox {blue!25}{best} and \colorbox {orange!25}{second best} results.}}{81}{table.caption.68}\protected@file@percent }
\newlabel{table: ours_diff_samples}{{5.1}{81}{Hypernetwork sparse reconstruction - Average metric results across varying sample sizes ($N$) over the test set. We highlight \colorbox {blue!25}{best} and \colorbox {orange!25}{second best} results}{table.caption.68}{}}
\@writefile{toc}{\contentsline {paragraph}{Sparse and full reconstruction results:}{81}{table.caption.70}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.2.3}Comparison with Casual Capture Setups}{81}{subsubsection.5.5.2.3}\protected@file@percent }
\citation{chen2021invertible}
\citation{chen2021invertible}
\citation{}
\citation{zheng2021compact}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Hypernetwork - Average metric results across varying sample sizes ($N$) over the test set (Sparse and full reconstruction of unseen materials).}}{82}{table.caption.70}\protected@file@percent }
\newlabel{table: ours_large_samples}{{5.2}{82}{Hypernetwork - Average metric results across varying sample sizes ($N$) over the test set (Sparse and full reconstruction of unseen materials)}{table.caption.70}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Comparison with iBRDF \cite  {chen2021invertible} - Average metric results over the renderings of the entire MERL dataset. We highlight \colorbox {blue!25}{best} and \colorbox {orange!25}{second best} results.}}{82}{table.caption.71}\protected@file@percent }
\newlabel{table: oursvsibrdf}{{5.3}{82}{Comparison with iBRDF \cite {chen2021invertible} - Average metric results over the renderings of the entire MERL dataset. We highlight \colorbox {blue!25}{best} and \colorbox {orange!25}{second best} results}{table.caption.71}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.3}Compression}{82}{subsection.5.5.3}\protected@file@percent }
\newlabel{sec:compression}{{5.5.3}{82}{Compression}{subsection.5.5.3}{}}
\citation{nielsen2015optimal}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Compression - Average metric results over the renderings of the entire MERL dataset. We highlight \colorbox {blue!25}{best} and \colorbox {orange!25}{second best} results.}}{83}{table.caption.72}\protected@file@percent }
\newlabel{table: oursvsnps}{{5.4}{83}{Compression - Average metric results over the renderings of the entire MERL dataset. We highlight \colorbox {blue!25}{best} and \colorbox {orange!25}{second best} results}{table.caption.72}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Reconstruction results for BRDF compression (GT: ground truths).}}{83}{figure.caption.73}\protected@file@percent }
\newlabel{fig:comp-fig}{{5.6}{83}{Reconstruction results for BRDF compression (GT: ground truths)}{figure.caption.73}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.4}BRDF Editing}{83}{subsection.5.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.5}Ablation Studies}{83}{subsection.5.5.5}\protected@file@percent }
\newlabel{sec:abl}{{5.5.5}{83}{Ablation Studies}{subsection.5.5.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.5.1}Latent Dimension}{83}{subsubsection.5.5.5.1}\protected@file@percent }
\citation{ngan2005experimental}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces BRDF editing through linear interpolation between the embeddings of two materials.}}{84}{figure.caption.74}\protected@file@percent }
\newlabel{fig:interpolation}{{5.7}{84}{BRDF editing through linear interpolation between the embeddings of two materials}{figure.caption.74}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.5.2}Log Relative Mapping (LRM)}{84}{subsubsection.5.5.5.2}\protected@file@percent }
\newlabel{sec:lrm}{{5.5.5.2}{84}{Log Relative Mapping (LRM)}{subsubsection.5.5.5.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.5.3}Cosine Weighting}{84}{subsubsection.5.5.5.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces Average metric results over the renderings of the test set (20 MERL materials).}}{84}{table.caption.75}\protected@file@percent }
\newlabel{table: cos_abl}{{5.5}{84}{Average metric results over the renderings of the test set (20 MERL materials)}{table.caption.75}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.5.4}Principal Component Analysis (PCA)}{84}{subsubsection.5.5.5.4}\protected@file@percent }
\citation{resources16}
\@writefile{toc}{\contentsline {paragraph}{Number of Principal Components:}{85}{section*.76}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces IPCA optimization for the number of principal components.}}{85}{figure.caption.77}\protected@file@percent }
\newlabel{fig:ipca_opt}{{5.8}{85}{IPCA optimization for the number of principal components}{figure.caption.77}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces Average mean squared errors for varying latent space dimensions (first row) and number of principal components (second row)}}{85}{table.caption.78}\protected@file@percent }
\newlabel{table: z_abl}{{5.6}{85}{Average mean squared errors for varying latent space dimensions (first row) and number of principal components (second row)}{table.caption.78}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.6}Scene Renderings}{85}{subsection.5.5.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Scene renderings with materials reconstructed by HyperBRDF.}}{86}{figure.caption.79}\protected@file@percent }
\newlabel{fig:scene-render}{{5.9}{86}{Scene renderings with materials reconstructed by HyperBRDF}{figure.caption.79}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.7}Limitations and Future Work}{86}{subsection.5.5.7}\protected@file@percent }
\newlabel{sec:limits}{{5.5.7}{86}{Limitations and Future Work}{subsection.5.5.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Specular components:}{86}{section*.80}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{BRDF editing:}{87}{section*.81}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{SVBRDF representations:}{87}{section*.82}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Conclusion}{87}{section.5.6}\protected@file@percent }
\newlabel{sec:conc}{{5.6}{87}{Conclusion}{section.5.6}{}}
\bibstyle{plainnat}
\bibdata{thesis}
\bibcite{An10User}{{1}{2010}{{An and Pellacini}}{{}}}
\bibcite{ashikhmin2007}{{2}{2007}{{Ashikhmin and Premoze}}{{}}}
\bibcite{Bae06Two}{{3}{2006}{{Bae et~al.}}{{Bae, Paris, and Durand}}}
\bibcite{bagher2016}{{4}{2016{}}{{Bagher et~al.}}{{Bagher, Snyder, and Nowrouzezahrai}}}
\bibcite{bagher2016non}{{5}{2016{}}{{Bagher et~al.}}{{Bagher, Snyder, and Nowrouzezahrai}}}
\bibcite{Berthouzoz11AFramework}{{6}{2011}{{Berthouzoz et~al.}}{{Berthouzoz, Li, Dontcheva, and Agrawala}}}
\bibcite{Besl92AMethod}{{7}{1992}{{Besl and McKay}}{{}}}
\bibcite{bilgili2011general}{{8}{2011}{{Bilgili et~al.}}{{Bilgili, {\"O}zt{\"u}rk, and Kurt}}}
\bibcite{blinn77}{{9}{1977}{{Blinn}}{{}}}
\bibcite{boss2021nerd}{{10}{2021}{{Boss et~al.}}{{Boss, Braun, Jampani, Barron, Liu, and Lensch}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{89}{section*.83}\protected@file@percent }
\bibcite{Boyadzhiev15Band}{{11}{2015}{{Boyadzhiev et~al.}}{{Boyadzhiev, Bala, Paris, and Adelson}}}
\bibcite{triangulation}{{12}{2019}{{Broeren et~al.}}{{Broeren, van~de Sande, van~der Wijk, and Herder}}}
\bibcite{Bychkovsky11Learning}{{13}{2011}{{Bychkovsky et~al.}}{{Bychkovsky, Paris, Chan, and Durand}}}
\bibcite{cazenavette2021mixergan}{{14}{2021}{{Cazenavette and De~Guevara}}{{}}}
\bibcite{chen2016bilateral}{{15}{2016}{{Chen et~al.}}{{Chen, Adams, Wadhwa, and Hasinoff}}}
\bibcite{chen2017fast}{{16}{2017}{{Chen et~al.}}{{Chen, Xu, and Koltun}}}
\bibcite{Chen18Deep}{{17}{2018{}}{{Chen et~al.}}{{Chen, Wang, Kao, and Chuang}}}
\bibcite{chen2018deep}{{18}{2018{}}{{Chen et~al.}}{{Chen, Wang, Kao, and Chuang}}}
\bibcite{chen2021invertible}{{19}{2021}{{Chen et~al.}}{{Chen, Nobuhara, and Nishino}}}
\bibcite{CohenOr06Color}{{20}{2006}{{Cohen-Or et~al.}}{{Cohen-Or, Sorkine, Gal, Leyvand, and Xu}}}
\bibcite{cooktorrance1982}{{21}{1982}{{Cook and Torrance}}{{}}}
\bibcite{deschaintre2018single}{{22}{2018}{{Deschaintre et~al.}}{{Deschaintre, Aittala, Durand, Drettakis, and Bousseau}}}
\bibcite{deschaintre2019flexible}{{23}{2019}{{Deschaintre et~al.}}{{Deschaintre, Aittala, Durand, Drettakis, and Bousseau}}}
\bibcite{dupuy2018adaptive}{{24}{2018}{{Dupuy and Jakob}}{{}}}
\bibcite{dupuy2015}{{25}{2015}{{Dupuy et~al.}}{{Dupuy, Heitz, Iehl, Poulin, and Ostromoukhov}}}
\bibcite{erkocc2023hyperdiffusion}{{26}{2023}{{Erko{\c {c}} et~al.}}{{Erko{\c {c}}, Ma, Shan, Nie{\ss }ner, and Dai}}}
\bibcite{fan2021neural}{{27}{2021}{{Fan et~al.}}{{Fan, Wang, Ha{\v {s}}an, Yang, and Yan}}}
\bibcite{Faridul14ASurvey}{{28}{2014}{{Faridul et~al.}}{{Faridul, Pouli, Chamaret, Stauder, Tremeau, and Reinhard}}}
\bibcite{Frigo16Split}{{29}{2016}{{Frigo et~al.}}{{Frigo, Sabater, Delon, and Hellier}}}
\bibcite{gao2019deep}{{30}{2019}{{Gao et~al.}}{{Gao, Li, Dong, Peers, Xu, and Tong}}}
\bibcite{Gharbi17Deep}{{31}{2017}{{Gharbi et~al.}}{{Gharbi, Chen, Barron, Hasinoff, and Durand}}}
\bibcite{guarnera2016}{{32}{2016}{{Guarnera et~al.}}{{Guarnera, Guarnera, Ghosh, Denk, and Glencross}}}
\bibcite{guo2021highlight}{{33}{2021}{{Guo et~al.}}{{Guo, Lai, Tao, Cai, Wang, Guo, and Yan}}}
\bibcite{googlePhysicallyBased}{{34}{}{{Guy and Agopian}}{{}}}
\bibcite{ha2017hypernetworks}{{35}{2017}{{Ha et~al.}}{{Ha, Dai, and Le}}}
\bibcite{HaCohen11Nonrigid}{{36}{2011}{{HaCohen et~al.}}{{HaCohen, Shechtman, Goldman, and Lischinski}}}
\bibcite{he2020conditional}{{37}{2020}{{He et~al.}}{{He, Liu, Qiao, and Dong}}}
\bibcite{Hertzmann01Image}{{38}{2001}{{Hertzmann et~al.}}{{Hertzmann, Jacobs, Oliver, Curless, and Salesin}}}
\bibcite{hu2020deepbrdf}{{39}{2020}{{Hu et~al.}}{{Hu, Guo, Chen, Li, and Guo}}}
\bibcite{Hu18Exposure}{{40}{2018}{{Hu et~al.}}{{Hu, He, Xu, Wang, and Lin}}}
\bibcite{Huang14Parametric}{{41}{2014}{{Huang et~al.}}{{Huang, Zhang, Lai, Kopf, Cohen-Or, and Hu}}}
\bibcite{hui2017reflectance}{{42}{2017}{{Hui et~al.}}{{Hui, Sunkavalli, Lee, Hadap, Wang, and Sankaranarayanan}}}
\bibcite{Hwang12Context}{{43}{2012}{{Hwang et~al.}}{{Hwang, Kapoor, and Kang}}}
\bibcite{jiang2021cotr}{{44}{2021{}}{{Jiang et~al.}}{{Jiang, Trulls, Hosang, Tagliasacchi, and Yi}}}
\bibcite{9334429}{{45}{2021{}}{{Jiang et~al.}}{{Jiang, Gong, Liu, Cheng, Fang, Shen, Yang, Zhou, and Wang}}}
\bibcite{Kagarlitsky09Piecewise}{{46}{2009}{{Kagarlitsky et~al.}}{{Kagarlitsky, Moses, and Hel-Or}}}
\bibcite{kang2018efficient}{{47}{2018}{{Kang et~al.}}{{}}}
\bibcite{kang2019learning}{{48}{2019}{{Kang et~al.}}{{}}}
\bibcite{karras2019style}{{49}{2021}{{Karras et~al.}}{{Karras, Laine, and Aila}}}
\bibcite{Kaufman12Content}{{50}{2012}{{Kaufman et~al.}}{{Kaufman, Lischinski, and Werman}}}
\bibcite{kim2021representative}{{51}{2021}{{Kim et~al.}}{{Kim, Choi, Kim, and Koh}}}
\bibcite{Laffont14Transient}{{52}{2014}{{Laffont et~al.}}{{Laffont, Ren, Tao, Qian, and Hays}}}
\bibcite{lawrence2004efficient}{{53}{2004}{{Lawrence et~al.}}{{Lawrence, Rusinkiewicz, and Ramamoorthi}}}
\bibcite{lawrence2006inverse}{{54}{2006}{{Lawrence et~al.}}{{Lawrence, Ben-Artzi, DeCoro, Matusik, Pfister, Ramamoorthi, and Rusinkiewicz}}}
\bibcite{li2020lapar}{{55}{2020}{{Li et~al.}}{{Li, Zhou, Qi, Jiang, Lu, and Jia}}}
\bibcite{visual_attribute}{{56}{2017}{{Liao et~al.}}{{Liao, Yao, Yuan, Hua, and Kang}}}
\bibcite{lin2020tuigan}{{57}{2020}{{Lin et~al.}}{{Lin, Pang, Xia, Chen, and Luo}}}
\bibcite{liu2023learning}{{58}{2023}{{Liu et~al.}}{{Liu, Fischer, and Ritschel}}}
\bibcite{Liu16Makeup}{{59}{2016}{{Liu et~al.}}{{Liu, Ou, Qian, Wang, and Cao}}}
\bibcite{low2012}{{60}{2012}{{L{\"{o}}w et~al.}}{{L{\"{o}}w, Kronander, Ynnerman, and Unger}}}
\bibcite{ma2021retinexgan}{{61}{2021{}}{{Ma et~al.}}{{Ma, Guo, Yu, Chen, Ren, Xi, Li, and Zhou}}}
\bibcite{ma2021free}{{62}{2021{}}{{Ma et~al.}}{{}}}
\bibcite{ma2023opensvbrdf}{{63}{2023}{{Ma et~al.}}{{Ma, Xu, Zhang, Zhou, and Wu}}}
\bibcite{martin2022materia}{{64}{2022}{{Martin et~al.}}{{Martin, Roullier, Rouffet, Kaiser, and Boubekeur}}}
\bibcite{Matusik2003jul}{{65}{2003}{{Matusik et~al.}}{{Matusik, Pfister, Brand, and McMillan}}}
\bibcite{matusik2003data}{{66}{2003}{{Matusik}}{{}}}
\bibcite{maximov2019deep}{{67}{2019}{{Maximov et~al.}}{{Maximov, Leal-Taix{\'e}, Fritz, and Ritschel}}}
\bibcite{moran2020deeplpf}{{68}{2020}{{Moran et~al.}}{{Moran, Marza, McDonagh, Parisot, and Slabaugh}}}
\bibcite{mustafa2022distilling}{{69}{2022}{{Mustafa et~al.}}{{Mustafa, Hanji, and Mantiuk}}}
\bibcite{Nam17Deep}{{70}{2017}{{Nam and Kim}}{{}}}
\bibcite{ngan2005experimental}{{71}{2005{}}{{Ngan et~al.}}{{Ngan, Durand, and Matusik}}}
\bibcite{ngan2005}{{72}{2005{}}{{Ngan et~al.}}{{Ngan, Durand, and Matusik}}}
\bibcite{ngan2006image}{{73}{2006}{{Ngan et~al.}}{{Ngan, Durand, and Matusik}}}
\bibcite{nielsen2015optimal}{{74}{2015}{{Nielsen et~al.}}{{Nielsen, Jensen, and Ramamoorthi}}}
\bibcite{Omiya18Learning}{{75}{2018}{{Omiya et~al.}}{{Omiya, Simo-Serra, Iizuka, and Ishikawa}}}
\bibcite{park2019deepsdf}{{76}{2019}{{Park et~al.}}{{Park, Florence, Straub, Newcombe, and Lovegrove}}}
\bibcite{park2018distort}{{77}{2018}{{Park et~al.}}{{Park, Lee, Yoo, and Kweon}}}
\bibcite{specfig}{{78}{2021}{{Park and Baek}}{{}}}
\bibcite{Pitie05NDimensional}{{79}{2005}{{Pitie et~al.}}{{Pitie, Kokaram, and Dahyot}}}
\bibcite{Pitie07Automated}{{80}{2007}{{Piti{\'e} et~al.}}{{Piti{\'e}, Kokaram, and Dahyot}}}
\bibcite{Pouli11Progressive}{{81}{2011}{{Pouli and Reinhard}}{{}}}
\bibcite{rainer2019neural}{{82}{2019}{{Rainer et~al.}}{{Rainer, Jakob, Ghosh, and Weyrich}}}
\bibcite{ratzlaff2019hypergan}{{83}{2019}{{Ratzlaff and Fuxin}}{{}}}
\bibcite{rebain2022attention}{{84}{2022}{{Rebain et~al.}}{{Rebain, Matthews, Yi, Sharma, Lagun, and Tagliasacchi}}}
\bibcite{Reinhard01Color}{{85}{2001}{{Reinhard et~al.}}{{Reinhard, Ashikhmin, Gooch, and Shirley}}}
\bibcite{ronneberger2015u}{{86}{2015}{{Ronneberger et~al.}}{{Ronneberger, Fischer, and Brox}}}
\bibcite{rusinkiewicz1998new}{{87}{1998}{{Rusinkiewicz}}{{}}}
\bibcite{Saeedi18Multimodal}{{88}{2018}{{Saeedi et~al.}}{{Saeedi, Hoffman, DiVerdi, Ghandeharioun, Johnson, and Adams}}}
\bibcite{schewe2007color}{{89}{2007}{{Schewe and Fraser}}{{}}}
\bibcite{serrano2018intuitive}{{90}{2018}{{Serrano et~al.}}{{Serrano, Gutierrez, Myszkowski, Seidel, and Masia}}}
\bibcite{shaham2021spatially}{{91}{2021}{{Shaham et~al.}}{{Shaham, Gharbi, Zhang, Shechtman, and Michaeli}}}
\bibcite{Shapira13Image}{{92}{2013}{{Shapira et~al.}}{{Shapira, Avidan, and Hel-Or}}}
\bibcite{Shih13Data}{{93}{2013}{{Shih et~al.}}{{Shih, Paris, Durand, and Freeman}}}
\bibcite{Shih14Style}{{94}{2014}{{Shih et~al.}}{{Shih, Paris, Barnes, Freeman, and Durand}}}
\bibcite{sitzmann2020metasdf}{{95}{2020{}}{{Sitzmann et~al.}}{{Sitzmann, Chan, Tucker, Snavely, and Wetzstein}}}
\bibcite{sitzmann2020siren}{{96}{2020{}}{{Sitzmann et~al.}}{{Sitzmann, Martel, Bergman, Lindell, and Wetzstein}}}
\bibcite{sun2007interactive}{{97}{2007}{{Sun et~al.}}{{Sun, Zhou, Chen, Lin, Shi, and Guo}}}
\bibcite{Sunkavalli10Multi}{{98}{2010}{{Sunkavalli et~al.}}{{Sunkavalli, Johnson, Matusik, and Pfister}}}
\bibcite{sztrajman2021neural}{{99}{2021}{{Sztrajman et~al.}}{{Sztrajman, Rainer, Ritschel, and Weyrich}}}
\bibcite{Tai07Soft}{{100}{2007}{{Tai et~al.}}{{Tai, Jia, and Tang}}}
\bibcite{Tai05Local}{{101}{2005}{{Tai et~al.}}{{Tai, Jia, and Tang}}}
\bibcite{ffn}{{102}{2020}{{Tancik et~al.}}{{Tancik, Srinivasan, Mildenhall, Fridovich-Keil, Raghavan, Singhal, Ramamoorthi, Barron, and Ng}}}
\bibcite{tolstikhin2021mlp}{{103}{2021}{{Tolstikhin et~al.}}{{Tolstikhin, Houlsby, Kolesnikov, Beyer, Zhai, Unterthiner, Yung, Steiner, Keysers, Uszkoreit, et~al.}}}
\bibcite{tongbuasirilai2020compact}{{104}{2020}{{Tongbuasirilai et~al.}}{{Tongbuasirilai, Unger, Kronander, and Kurt}}}
\bibcite{tseng2019hyperparameter}{{105}{2019}{{Tseng et~al.}}{{Tseng, Yu, Yang, Mannan, Arnaud, Nowrouzezahrai, Lalonde, and Heide}}}
\bibcite{tseng2022neural}{{106}{2022}{{Tseng et~al.}}{{Tseng, Zhang, Jebe, Zhang, Xia, Fan, Heide, and Chen}}}
\bibcite{tunwattanapong2013acquiring}{{107}{2013}{{Tunwattanapong et~al.}}{{Tunwattanapong, Fyffe, Graham, Busch, Yu, Ghosh, and Debevec}}}
\bibcite{walter2007microfacet}{{108}{2007}{{Walter et~al.}}{{Walter, Marschner, Li, and Torrance}}}
\bibcite{wang2022attention}{{109}{2022}{{Wang et~al.}}{{Wang, Chen, Chen, Venugopalan, Wang, et~al.}}}
\bibcite{wang2019underexposed}{{110}{2019}{{Wang et~al.}}{{Wang, Zhang, Fu, Shen, Zheng, and Jia}}}
\bibcite{ward1992}{{111}{1992}{{Ward}}{{}}}
\bibcite{xu2015deep}{{112}{2015}{{Xu et~al.}}{{Xu, Ren, Yan, Liao, and Jia}}}
\bibcite{Yan14Automatic}{{113}{2014}{{Yan et~al.}}{{Yan, Zhang, Wang, Paris, and Yu}}}
\bibcite{10.1145/2790296}{{114}{2016{}}{{Yan et~al.}}{{Yan, Zhang, Wang, Paris, and Yu}}}
\bibcite{yan2016automatic}{{115}{2016{}}{{Yan et~al.}}{{Yan, Zhang, Wang, Paris, and Yu}}}
\bibcite{yang2023contranerf}{{116}{2023}{{Yang et~al.}}{{Yang, Hong, Li, Hu, Li, Lee, and Wang}}}
\bibcite{yang2020fidelity}{{117}{2020}{{Yang et~al.}}{{Yang, Wang, Fang, Wang, and Liu}}}
\bibcite{yu2021reconfigisp}{{118}{2021}{{Yu et~al.}}{{Yu, Li, Peng, Loy, and Gu}}}
\bibcite{zaheer2017deepsets}{{119}{2017}{{Zaheer et~al.}}{{Zaheer, Kottur, Ravanbakhsh, Poczos, Salakhutdinov, and Smola}}}
\bibcite{Zhang13Style}{{120}{2013}{{Zhang et~al.}}{{Zhang, Cao, Chen, Liu, and Tang}}}
\bibcite{zheng2021compact}{{121}{2021}{{Zheng et~al.}}{{Zheng, Zheng, Wang, Zhao, and Bao}}}
\bibcite{cnf2023}{{122}{2023}{{Zhong et~al.}}{{Zhong, Fogarty, Hanji, Wu, Sztrajman, Spielberg, Tagliasacchi, Bosilj, and Oztireli}}}
\bibcite{zhou2021adversarial}{{123}{2021}{{Zhou and Kalantari}}{{}}}
\bibcite{Zhu18Automatic}{{124}{2018}{{Zhu and Yu}}{{}}}
\citation{sztrajman2021neural}
\citation{nielsen2015optimal}
\citation{sztrajman2021neural}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Extra Information}{103}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}HyperBRDF - Additional Results}{103}{section.A.1}\protected@file@percent }
\newlabel{hyperbrdf:add_res}{{A.1}{103}{HyperBRDF - Additional Results}{section.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.1}Full Reconstruction}{103}{subsection.A.1.1}\protected@file@percent }
\newlabel{sec:full_rec}{{A.1.1}{103}{Full Reconstruction}{subsection.A.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.2}Compression}{103}{subsection.A.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Qualitative comparison for full reconstruction capacity on the test dataset.}}{104}{figure.caption.85}\protected@file@percent }
\newlabel{fig:qual_comp}{{A.1}{104}{Qualitative comparison for full reconstruction capacity on the test dataset}{figure.caption.85}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces Sparse reconstruction results, $N = 40$.}}{105}{figure.caption.87}\protected@file@percent }
\newlabel{fig:40}{{A.2}{105}{Sparse reconstruction results, $N = 40$}{figure.caption.87}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces Sparse reconstruction results, $N = 160$.}}{106}{figure.caption.88}\protected@file@percent }
\newlabel{fig:160}{{A.3}{106}{Sparse reconstruction results, $N = 160$}{figure.caption.88}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces Sparse reconstruction results, $N = 400$.}}{107}{figure.caption.89}\protected@file@percent }
\newlabel{fig:400}{{A.4}{107}{Sparse reconstruction results, $N = 400$}{figure.caption.89}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.5}{\ignorespaces Sparse reconstruction results, $N = 2000$.}}{108}{figure.caption.90}\protected@file@percent }
\newlabel{fig:2000}{{A.5}{108}{Sparse reconstruction results, $N = 2000$}{figure.caption.90}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.6}{\ignorespaces Sparse reconstruction results, $N = 4000$.}}{109}{figure.caption.91}\protected@file@percent }
\newlabel{fig:4000}{{A.6}{109}{Sparse reconstruction results, $N = 4000$}{figure.caption.91}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Quantitative comparison results for full reconstruction over the renderings of the test set.}}{110}{table.caption.86}\protected@file@percent }
\newlabel{table: comparison results}{{A.1}{110}{Quantitative comparison results for full reconstruction over the renderings of the test set}{table.caption.86}{}}
\gdef \@abspage@last{110}
