\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{thesis.ist}
\@glsorder{word}
\providecommand \oddpage@label [2]{}
\citation{}
\citation{}
\@writefile{toc}{\thispagestyle {empty}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{13}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Research Vision and Motivation}{13}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Contributions}{13}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Relationship to Published Work}{13}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{15}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Rendering}{15}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Displaying an image}{15}{subsection.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A pixel, a picture element, is the smallest unit of a rendered image. Displaying an image with fewer pixels causes losses in details as one pixel starts covering a larger area in the 3D scene.}}{15}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:colour-approximate}{{2.1}{15}{A pixel, a picture element, is the smallest unit of a rendered image. Displaying an image with fewer pixels causes losses in details as one pixel starts covering a larger area in the 3D scene}{figure.caption.5}{}}
\citation{schewe2007colour}
\citation{schewe2007colour}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Need a figure to represent pixel representation of continuous image}}{16}{figure.caption.6}\protected@file@percent }
\newlabel{fig:display-grid}{{2.2}{16}{Need a figure to represent pixel representation of continuous image}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Comparison of different colour gamuts \cite  {schewe2007colour}}}{16}{figure.2.3}\protected@file@percent }
\newlabel{fig:colour-gamut}{{2.3}{16}{Comparison of different colour gamuts \cite {schewe2007colour}}{figure.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces When a colour is represented with too few bits, the transition between colours appears as discrete steps, known as colour banding (right).}}{17}{figure.caption.7}\protected@file@percent }
\newlabel{fig:colour-band}{{2.4}{17}{When a colour is represented with too few bits, the transition between colours appears as discrete steps, known as colour banding (right)}{figure.caption.7}{}}
\citation{boss2021nerd}
\citation{boss2021nerd}
\citation{boss2021nerd}
\citation{boss2021nerd}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Rendering an image of a 3D scene requires lighting, viewpoint (camera) and geometry. Image courtesy of \citeauthor  {boss2021nerd} \cite  {boss2021nerd}.}}{18}{figure.caption.8}\protected@file@percent }
\newlabel{fig:teaser}{{2.5}{18}{Rendering an image of a 3D scene requires lighting, viewpoint (camera) and geometry. Image courtesy of \citeauthor {boss2021nerd} \cite {boss2021nerd}}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}3D scene components}{18}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Geometry.}{18}{section*.9}\protected@file@percent }
\citation{triangulation}
\citation{triangulation}
\citation{triangulation}
\citation{triangulation}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Triangulation of surfaces is a commonly used method in computer graphics to define surfaces. It allows the creation of complex shapes. Increasing the number of triangles also increases the resolution of the shape that leads to smoother surface representations. Image courtesy of \citeauthor  {triangulation} \cite  {triangulation}.}}{19}{figure.caption.10}\protected@file@percent }
\newlabel{fig:triangulation}{{2.6}{19}{Triangulation of surfaces is a commonly used method in computer graphics to define surfaces. It allows the creation of complex shapes. Increasing the number of triangles also increases the resolution of the shape that leads to smoother surface representations. Image courtesy of \citeauthor {triangulation} \cite {triangulation}}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Photo-realistic rendering}{20}{subsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Perspective projection and visibility.}{20}{section*.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Perspective projection mimics the foreshortening effect our visual system creates while observing a scene. The image plane/canvas includes the 2D projections of the visible objects within the view frustum.}}{21}{figure.caption.12}\protected@file@percent }
\newlabel{fig:perspective_projection}{{2.7}{21}{Perspective projection mimics the foreshortening effect our visual system creates while observing a scene. The image plane/canvas includes the 2D projections of the visible objects within the view frustum}{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Single-point vs two-point perspective. Computer graphics mimics our visual system, focusing on single-point projection. However, multi-point perspective is widely explored in artistic drawing.}}{21}{figure.caption.13}\protected@file@percent }
\newlabel{fig:artistic_drawing}{{2.8}{21}{Single-point vs two-point perspective. Computer graphics mimics our visual system, focusing on single-point projection. However, multi-point perspective is widely explored in artistic drawing}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {paragraph}{Appearance.}{21}{section*.14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces The reflected light from the surface of an object defines its colour. Image from \href  {http://www.mstworkbooks.co.za/natural-sciences/gr8/images/gr8ec04-gd-0027.png}{website link}.}}{22}{figure.2.9}\protected@file@percent }
\newlabel{fig:object-colour}{{2.9}{22}{The reflected light from the surface of an object defines its colour. Image from \href {http://www.mstworkbooks.co.za/natural-sciences/gr8/images/gr8ec04-gd-0027.png}{website link}}{figure.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces The ripples on the water's surface cause a blurred image of the bridge, acting as a rough surface.}}{22}{figure.2.10}\protected@file@percent }
\newlabel{fig:water_reflection}{{2.10}{22}{The ripples on the water's surface cause a blurred image of the bridge, acting as a rough surface}{figure.2.10}{}}
\citation{googlePhysicallyBased}
\citation{googlePhysicallyBased}
\citation{googlePhysicallyBased}
\citation{googlePhysicallyBased}
\citation{specfig}
\citation{specfig}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Reflection on a surface with microfacets vs smooth surface. Image from \citeauthor  {googlePhysicallyBased} \cite  {googlePhysicallyBased}}}{23}{figure.caption.15}\protected@file@percent }
\newlabel{fig:microfacet}{{2.11}{23}{Reflection on a surface with microfacets vs smooth surface. Image from \citeauthor {googlePhysicallyBased} \cite {googlePhysicallyBased}}{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Diffuse surfaces often have complex internal structures that cause the light to be reflected multiple times underneath the surface.}}{23}{figure.2.12}\protected@file@percent }
\newlabel{fig:diffuse-scattering}{{2.12}{23}{Diffuse surfaces often have complex internal structures that cause the light to be reflected multiple times underneath the surface}{figure.2.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Specular vs diffuse reflection. Diffuse surfaces cause rays to scatter randomly, brightening the object equally in all viewing directions. Specular reflection is, on the other hand, view-dependent, reflected rays concentrated around the mirror reflection. Image courtesy of \citeauthor  {specfig}.}}{24}{figure.caption.16}\protected@file@percent }
\newlabel{fig:specularvsdiffuse}{{2.13}{24}{Specular vs diffuse reflection. Diffuse surfaces cause rays to scatter randomly, brightening the object equally in all viewing directions. Specular reflection is, on the other hand, view-dependent, reflected rays concentrated around the mirror reflection. Image courtesy of \citeauthor {specfig}}{figure.caption.16}{}}
\@writefile{toc}{\contentsline {paragraph}{Ray tracing - Light transport simulation.}{24}{section*.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Ray tracing simulate the light's journey to compute the colour of each pixel on the rendered image. - Image courtesy of \href  {https://commons.wikimedia.org/wiki/File:Ray_trace_diagram.svg}{Henrik, CC BY-SA 4.0}}}{25}{figure.caption.18}\protected@file@percent }
\newlabel{fig:raytracing}{{2.14}{25}{Ray tracing simulate the light's journey to compute the colour of each pixel on the rendered image. - Image courtesy of \href {https://commons.wikimedia.org/wiki/File:Ray_trace_diagram.svg}{Henrik, CC BY-SA 4.0}}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Shaders and BRDF}{26}{subsection.2.1.4}\protected@file@percent }
\citation{Matusik2003jul}
\citation{Matusik2003jul}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces A material composed of both specular and diffuse components, lit by a point light source. The specular highlight (white circle) is centered around the light source direction. The material is specular-maroon-phenolic from MERL dataset \cite  {Matusik2003jul}.}}{27}{figure.2.15}\protected@file@percent }
\newlabel{fig:diffuse+spec}{{2.15}{27}{A material composed of both specular and diffuse components, lit by a point light source. The specular highlight (white circle) is centered around the light source direction. The material is specular-maroon-phenolic from MERL dataset \cite {Matusik2003jul}}{figure.2.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Diffuse and specular reflections.. Figures from \href  {https://www.cl.cam.ac.uk/teaching/2223/Graphics/Introduction_to_Graphics_1pp.pdf}{Slides by Rafal Mantiuk}}}{27}{figure.caption.19}\protected@file@percent }
\newlabel{fig:diffuse-spec-angle}{{2.16}{27}{Diffuse and specular reflections.. Figures from \href {https://www.cl.cam.ac.uk/teaching/2223/Graphics/Introduction_to_Graphics_1pp.pdf}{Slides by Rafal Mantiuk}}{figure.caption.19}{}}
\citation{phong1998illumination}
\citation{cooktorrance1982}
\citation{ward1992}
\citation{walter2007microfacet}
\newlabel{eq:Phong-eq}{{2.1}{28}{Shaders and BRDF}{equation.2.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces Phong shading model implementation with decreasing roughness/increasing glossiness from left to right.}}{28}{figure.caption.20}\protected@file@percent }
\newlabel{fig:phong-roughness}{{2.17}{28}{Phong shading model implementation with decreasing roughness/increasing glossiness from left to right}{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces BRDF depends on the incident light and viewing direction.}}{28}{figure.2.18}\protected@file@percent }
\newlabel{fig:brdf}{{2.18}{28}{BRDF depends on the incident light and viewing direction}{figure.2.18}{}}
\@writefile{toc}{\contentsline {paragraph}{BRDF.}{28}{section*.21}\protected@file@percent }
\citation{Matusik2003jul}
\citation{Chenliang's paper}
\citation{Chenliang's paper}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Rendering Equation}{30}{subsection.2.1.5}\protected@file@percent }
\newlabel{eqn:rendering-eqn}{{2.2}{30}{Rendering Equation}{equation.2.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Machine learning}{30}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Foundations}{31}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Types of learning.}{31}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training.}{31}{section*.23}\protected@file@percent }
\citation{kingma2017adammethodstochasticoptimization}
\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces Gradient descent steps.}}{32}{figure.2.19}\protected@file@percent }
\newlabel{fig:gradient-descent}{{2.19}{32}{Gradient descent steps}{figure.2.19}{}}
\citation{johnson2016perceptuallossesrealtimestyle}
\@writefile{toc}{\contentsline {paragraph}{Loss function.}{33}{section*.24}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.20}{\ignorespaces L1 vs L2 loss values for a single data point. Ground truth $y$ is set to 0.}}{33}{figure.caption.25}\protected@file@percent }
\newlabel{fig:l1vsl2loss}{{2.20}{33}{L1 vs L2 loss values for a single data point. Ground truth $y$ is set to 0}{figure.caption.25}{}}
\@writefile{toc}{\contentsline {paragraph}{Regularisation.}{33}{section*.26}\protected@file@percent }
\newlabel{MSE-with-L2reg}{{2.3}{34}{Regularisation}{equation.2.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Data augmentation}{34}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Multilayer Perceptrons}{34}{subsection.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.21}{\ignorespaces Graph representation of a linear equation.}}{35}{figure.2.21}\protected@file@percent }
\newlabel{fig:neuron}{{2.21}{35}{Graph representation of a linear equation}{figure.2.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.22}{\ignorespaces A basic multilayer perceptron model.}}{36}{figure.caption.28}\protected@file@percent }
\newlabel{fig:mlp}{{2.22}{36}{A basic multilayer perceptron model}{figure.caption.28}{}}
\@writefile{toc}{\contentsline {paragraph}{Activation functions.}{36}{section*.29}\protected@file@percent }
\citation{cybenko1989approximation}
\citation{ffn}
\citation{nguyen2015deep}
\citation{mildenhall2021nerf}
\citation{park2019deepsdf}
\citation{sztrajman2021neural}
\citation{rahaman2019spectral}
\citation{ffn}
\citation{ffn}
\citation{rahimi2007random}
\citation{sitzmann2020siren}
\citation{sitzmann2020siren}
\newlabel{eq:sine-act}{{2.8}{37}{Activation functions}{equation.2.2.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Neural implicit representations}{37}{subsection.2.2.3}\protected@file@percent }
\citation{ffn}
\citation{ffn}
\newlabel{eq:ffn}{{2.9}{38}{Neural implicit representations}{equation.2.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.23}{\ignorespaces To overcome the spectral bias in coordinate-based MLPs, a Fourier feature mapping \cite  {ffn} can be applied before feeding the input to the model. Here, 'none' represents an MLP model without mapping. 'basic' and 'gauss' are different variants of Fourier feature maps, and 'GT' is the ground truth.}}{38}{figure.caption.30}\protected@file@percent }
\newlabel{fig:ffn}{{2.23}{38}{To overcome the spectral bias in coordinate-based MLPs, a Fourier feature mapping \cite {ffn} can be applied before feeding the input to the model. Here, 'none' represents an MLP model without mapping. 'basic' and 'gauss' are different variants of Fourier feature maps, and 'GT' is the ground truth}{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Diffusion models}{38}{subsection.2.2.4}\protected@file@percent }
\citation{shaham2021spatially,li2020lapar}
\citation{moran2020deeplpf}
\citation{Gharbi17Deep}
\citation{yan2016automatic}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}One-shot Detail Retouching}{39}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{39}{section.3.1}\protected@file@percent }
\newlabel{sec:introduction}{{3.1}{39}{Introduction}{section.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Our technique automatically transfers retouching edits to new images by learning the desired edits from one example \textit  {before-after} pair (insets). The transferred edits accurately capture intricate details such as wrinkles, dark spots, strands of hair, or eyelashes, as shown in the input (top) and retouched (bottom) pairs. Image courtesy of Jenavieve (top-left), Logan ProPro (top-left, inset), Marissa Oosterlee (top-middle). (CC-BY).}}{40}{figure.caption.31}\protected@file@percent }
\newlabel{fig:teaser}{{3.1}{40}{Our technique automatically transfers retouching edits to new images by learning the desired edits from one example \textit {before-after} pair (insets). The transferred edits accurately capture intricate details such as wrinkles, dark spots, strands of hair, or eyelashes, as shown in the input (top) and retouched (bottom) pairs. Image courtesy of Jenavieve (top-left), Logan ProPro (top-left, inset), Marissa Oosterlee (top-middle). (CC-BY)}{figure.caption.31}{}}
\citation{Faridul14ASurvey,mustafa2022distilling}
\citation{Bychkovsky11Learning,Bae06Two,Pitie05NDimensional,Pitie07Automated,Reinhard01Color,Sunkavalli10Multi,he2020conditional,park2018distort}
\citation{CohenOr06Color}
\citation{Bychkovsky11Learning}
\citation{Bychkovsky11Learning}
\citation{chen2017fast}
\citation{chen2017fast}
\citation{Hu18Exposure}
\citation{CohenOr06Color}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Related Work}{41}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Digital Photo Enhancement}{41}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Global image enhancement.}{41}{section*.32}\protected@file@percent }
\citation{kim2021representative}
\citation{wang2019underexposed}
\citation{Shapira13Image}
\citation{Laffont14Transient,Tai07Soft}
\citation{Berthouzoz11AFramework,Chen18Deep,Huang14Parametric,Omiya18Learning,Saeedi18Multimodal}
\citation{An10User,Pouli11Progressive,Tai05Local}
\citation{Gharbi17Deep,Hwang12Context,Kaufman12Content,Nam17Deep,Yan14Automatic,Zhu18Automatic}
\citation{HaCohen11Nonrigid}
\citation{Kagarlitsky09Piecewise,Shih13Data}
\citation{moran2020deeplpf,Gharbi17Deep,chen2018deep,shaham2021spatially,li2020lapar}
\citation{chen2018deep}
\citation{chen2018deep}
\citation{chen2016bilateral}
\citation{Gharbi17Deep}
\citation{moran2020deeplpf}
\citation{moran2020deeplpf}
\citation{Bae06Two}
\citation{HaCohen11Nonrigid}
\citation{Shih14Style}
\citation{Shih14Style}
\citation{tseng2019hyperparameter,yu2021reconfigisp}
\citation{tseng2019hyperparameter}
\citation{yu2021reconfigisp}
\citation{yu2021reconfigisp}
\citation{tseng2022neural}
\citation{tseng2022neural}
\citation{ma2021retinexgan}
\citation{yang2020fidelity}
\citation{9334429}
\citation{Liu16Makeup}
\citation{Frigo16Split}
\citation{Chen18Deep}
\citation{lin2020tuigan}
\citation{Zhang13Style,Hu18Exposure}
\@writefile{toc}{\contentsline {paragraph}{Local context-aware image enhancement.}{42}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Differentiable image processing pipelines.}{42}{section*.34}\protected@file@percent }
\citation{visual_attribute}
\citation{visual_attribute}
\citation{Hertzmann01Image}
\citation{kim2021representative,wang2019underexposed}
\citation{10.1145/2790296}
\citation{tolstikhin2021mlp}
\citation{cazenavette2021mixergan}
\citation{cazenavette2021mixergan}
\citation{cazenavette2021mixergan}
\citation{cazenavette2021mixergan}
\citation{tolstikhin2021mlp}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Defining Maps between Images}{43}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Unsupervised methods.}{43}{section*.35}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Supervised methods.}{43}{section*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Overview and Motivations}{44}{section.3.3}\protected@file@percent }
\newlabel{chap:motivations}{{3.3}{44}{Overview and Motivations}{section.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}One-shot Retouching}{44}{section.3.4}\protected@file@percent }
\newlabel{sec:Methodology}{{3.4}{44}{One-shot Retouching}{section.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Our technique learns a separate mapping per frequency band by decomposing images into five different bands with a Laplacian pyramid. At each Laplacian band $l$, we define a mapping between flattened patches $\mathbf  {x}_i$, $\mathbf  {y}_i$ extracted from before-after bands $X_l$, $Y_l$. Our field based method (MLP block) adapts transformations to input patches, providing local context-aware adjustments. The transformation matrices and MLP parameters are learned jointly from scratch for each Laplacian band of the before-after pair.}}{45}{figure.caption.37}\protected@file@percent }
\newlabel{fig:modelT}{{3.2}{45}{Our technique learns a separate mapping per frequency band by decomposing images into five different bands with a Laplacian pyramid. At each Laplacian band $l$, we define a mapping between flattened patches $\mathbf {x}_i$, $\mathbf {y}_i$ extracted from before-after bands $X_l$, $Y_l$. Our field based method (MLP block) adapts transformations to input patches, providing local context-aware adjustments. The transformation matrices and MLP parameters are learned jointly from scratch for each Laplacian band of the before-after pair}{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Frequency Decomposition}{46}{subsection.3.4.1}\protected@file@percent }
\newlabel{sec:thePatchMap}{{3.4.1}{46}{Frequency Decomposition}{subsection.3.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Transformation Blending}{46}{subsection.3.4.2}\protected@file@percent }
\newlabel{sec:Blending}{{3.4.2}{46}{Transformation Blending}{subsection.3.4.2}{}}
\newlabel{eq:weightedSum}{{3.2}{46}{Transformation Blending}{equation.3.4.2}{}}
\citation{Boyadzhiev15Band}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Retouching an Input Image}{47}{subsection.3.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Implementation}{47}{subsection.3.4.4}\protected@file@percent }
\newlabel{sec:Implementation}{{3.4.4}{47}{Implementation}{subsection.3.4.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Patch size and stride.}{47}{section*.38}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Detail and color modifications.}{47}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Evaluation metrics.}{47}{section*.40}\protected@file@percent }
\newlabel{train_det}{{3.4.4}{47}{Training details}{section*.41}{}}
\@writefile{toc}{\contentsline {paragraph}{Training details.}{47}{section*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Results}{48}{section.3.5}\protected@file@percent }
\newlabel{sec:results}{{3.5}{48}{Results}{section.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Ablation Study}{48}{subsection.3.5.1}\protected@file@percent }
\newlabel{ablation}{{3.5.1}{48}{Ablation Study}{subsection.3.5.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Transformation Matrices.}{48}{section*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Patch-adaptive Transformation Blending.}{48}{section*.45}\protected@file@percent }
\citation{karras2019style}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The higher the complexity of the learned algorithm, the more transformation matrices our technique requires to capture the effects on local regions accurately. While $K=1$ can be sufficient for our model to capture unsharp masking, it requires more matrices to represent bilateral filtering precisely.}}{49}{figure.caption.43}\protected@file@percent }
\newlabel{fig:ablation_K}{{3.3}{49}{The higher the complexity of the learned algorithm, the more transformation matrices our technique requires to capture the effects on local regions accurately. While $K=1$ can be sufficient for our model to capture unsharp masking, it requires more matrices to represent bilateral filtering precisely}{figure.caption.43}{}}
\@writefile{toc}{\contentsline {paragraph}{Weights visulation.}{49}{section*.46}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces An MLP regressor cannot capture local edits, resulting in inaccurate retouching edits, such as blurring on the skin or around the eyes.}}{50}{figure.caption.44}\protected@file@percent }
\newlabel{fig:ablation_MLP}{{3.4}{50}{An MLP regressor cannot capture local edits, resulting in inaccurate retouching edits, such as blurring on the skin or around the eyes}{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Visualization of the reconstructed patch-adaptive weights of the model trained with example images in our teaser figure. Here, the input to the model is shown on top, and each row shows the weights in the corresponding Laplacian band, indicated on the left.}}{50}{figure.caption.47}\protected@file@percent }
\newlabel{fig:weight-vis}{{3.5}{50}{Visualization of the reconstructed patch-adaptive weights of the model trained with example images in our teaser figure. Here, the input to the model is shown on top, and each row shows the weights in the corresponding Laplacian band, indicated on the left}{figure.caption.47}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Qualitative Results}{51}{subsection.3.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The reproduced retouching style from the example pair (inset) improves skin texture without affecting fine details, such as eyes and hair, for a visually improved portrait. Moreover, our technique generalizes well to faces with different lighting conditions and accurately reproduces the example retouching style.}}{51}{figure.caption.48}\protected@file@percent }
\newlabel{fig:newdataset_ex}{{3.6}{51}{The reproduced retouching style from the example pair (inset) improves skin texture without affecting fine details, such as eyes and hair, for a visually improved portrait. Moreover, our technique generalizes well to faces with different lighting conditions and accurately reproduces the example retouching style}{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Material editing results on photos (left), and rendered images (right), based on the before-after pair (inset). The details, such as scratches or lines are emphasized, and materials became shinier. Image courtesy of royalmix (top and bottom-inset), tsmdunn (bottom). (PixelSquid).}}{52}{figure.caption.49}\protected@file@percent }
\newlabel{fig:material_res}{{3.7}{52}{Material editing results on photos (left), and rendered images (right), based on the before-after pair (inset). The details, such as scratches or lines are emphasized, and materials became shinier. Image courtesy of royalmix (top and bottom-inset), tsmdunn (bottom). (PixelSquid)}{figure.caption.49}{}}
\citation{ronneberger2015u}
\citation{shaham2021spatially}
\citation{xu2015deep}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Our patch-adaptive technique can accurately capture the nuances between different retouching styles as given by the examples (top row).}}{53}{figure.caption.50}\protected@file@percent }
\newlabel{fig:retouchingstyles}{{3.8}{53}{Our patch-adaptive technique can accurately capture the nuances between different retouching styles as given by the examples (top row)}{figure.caption.50}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Comparison with the state-of-the-art}{53}{subsection.3.5.3}\protected@file@percent }
\newlabel{sec:Comparisons}{{3.5.3}{53}{Comparison with the state-of-the-art}{subsection.3.5.3}{}}
\citation{Bychkovsky11Learning}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Quantitative performance comparison for the reproduction of various image processing filters. Average PSNR and SSIM values are computed over 182 images of different types of images including faces, landscapes, materials, and rooms. Qualitative results can be found in the supplementary material.We highlight \colorbox {blue!25}{best PSNR} and \colorbox {orange!25}{best SSIM} results.}}{54}{table.caption.51}\protected@file@percent }
\newlabel{tablecomparison}{{3.1}{54}{Quantitative performance comparison for the reproduction of various image processing filters. Average PSNR and SSIM values are computed over 182 images of different types of images including faces, landscapes, materials, and rooms. Qualitative results can be found in the supplementary material.We highlight \colorbox {blue!25}{best PSNR} and \colorbox {orange!25}{best SSIM} results}{table.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Qualitative comparisons with state-of-the-art methods on different types of images. These results are included in Table 1 for the filter types indicated in rows. The training strategy for the state-of-the-art methods is summarized in Section \ref {sec:Comparisons}. Before-after pairs along with additional results can be found in the supplementary material. Image courtesy of Arnaud Rougetet (landscape) and virtualhorizonstudio (alarm clock). (CC-BY and PixelSquid).}}{55}{figure.caption.52}\protected@file@percent }
\newlabel{fig:QualitativeComp}{{3.9}{55}{Qualitative comparisons with state-of-the-art methods on different types of images. These results are included in Table 1 for the filter types indicated in rows. The training strategy for the state-of-the-art methods is summarized in Section \ref {sec:Comparisons}. Before-after pairs along with additional results can be found in the supplementary material. Image courtesy of Arnaud Rougetet (landscape) and virtualhorizonstudio (alarm clock). (CC-BY and PixelSquid)}{figure.caption.52}{}}
\citation{Shih14Style}
\citation{Yan14Automatic}
\citation{Besl92AMethod}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Limitations and Future Work}{56}{subsection.3.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Our technique cannot accurately handle extreme non-repeating local effects such as tattoos (top), and when example and input images are of very different semantics (bottom). Image courtesy of bgaj23 (coin). (PixelSquid).}}{56}{figure.caption.53}\protected@file@percent }
\newlabel{fig:limitations}{{3.10}{56}{Our technique cannot accurately handle extreme non-repeating local effects such as tattoos (top), and when example and input images are of very different semantics (bottom). Image courtesy of bgaj23 (coin). (PixelSquid)}{figure.caption.53}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Conclusions}{57}{section.3.6}\protected@file@percent }
\citation{ngan2005}
\citation{dupuy2015,guarnera2016}
\citation{nielsen2015optimal}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Neural Generalizable Material Representation}{59}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:HyperBRDF}{{4}{59}{Neural Generalizable Material Representation}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{59}{section.4.1}\protected@file@percent }
\newlabel{sec:intro}{{4.1}{59}{Introduction}{section.4.1}{}}
\citation{sitzmann2020siren,ffn,cnf2023}
\citation{sztrajman2021neural,cnf2023}
\citation{rebain2022attention}
\citation{park2019deepsdf}
\citation{ha2017hypernetworks}
\citation{jiang2021cotr}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A room scene rendered with our reconstructed materials, including sparse reconstruction (table top and legs, door, door and picture frames, hinge), compression (two teapots on the left, door handle) and BRDF interpolation (right-most teapot). Scene courtesy of Benedikt Bitterli.}}{60}{figure.caption.54}\protected@file@percent }
\newlabel{fig:teaser}{{4.1}{60}{A room scene rendered with our reconstructed materials, including sparse reconstruction (table top and legs, door, door and picture frames, hinge), compression (two teapots on the left, door handle) and BRDF interpolation (right-most teapot). Scene courtesy of Benedikt Bitterli}{figure.caption.54}{}}
\citation{blinn77}
\citation{cooktorrance1982}
\citation{ward1992}
\citation{walter2007microfacet}
\citation{dupuy2015,ashikhmin2007,bagher2016}
\citation{low2012}
\citation{ngan2005,guarnera2016}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Acquisition Systems}{61}{section.4.2}\protected@file@percent }
\newlabel{sec:Acquisition-Systems}{{4.2}{61}{Acquisition Systems}{section.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Related Work}{61}{section.4.3}\protected@file@percent }
\newlabel{sec:relatedwork}{{4.3}{61}{Related Work}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Analytic BRDF Models}{61}{subsection.4.3.1}\protected@file@percent }
\newlabel{hyperbrdf-RW}{{4.3.1}{61}{Analytic BRDF Models}{subsection.4.3.1}{}}
\citation{matusik2003data,nielsen2015optimal,serrano2018intuitive}
\citation{lawrence2004efficient,lawrence2006inverse}
\citation{sun2007interactive}
\citation{bilgili2011general,tongbuasirilai2020compact}
\citation{bagher2016non}
\citation{rainer2019neural,hu2020deepbrdf,sztrajman2021neural,zheng2021compact,maximov2019deep,chen2021invertible,fan2021neural,cnf2023}
\citation{maximov2019deep}
\citation{sztrajman2021neural}
\citation{cnf2023}
\citation{hu2020deepbrdf}
\citation{zheng2021compact}
\citation{nielsen2015optimal}
\citation{liu2023learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Regression-based BRDF Estimation}{62}{subsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Deep learning for BRDF modeling.}{62}{section*.55}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Efficient BRDF Acquisition}{62}{subsection.4.3.3}\protected@file@percent }
\citation{kang2018efficient,kang2019learning,ma2021free,ma2023opensvbrdf,tunwattanapong2013acquiring}
\citation{guo2021highlight,hui2017reflectance,deschaintre2018single,deschaintre2019flexible,martin2022materia,zhou2021adversarial,gao2019deep}
\citation{ratzlaff2019hypergan}
\citation{erkocc2023hyperdiffusion}
\citation{wang2022attention,yang2023contranerf}
\citation{nielsen2015optimal}
\@writefile{toc}{\contentsline {paragraph}{Spatially-varying BRDFs (SVBRDF):}{63}{section*.56}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Hypernetworks and GNFs}{63}{subsection.4.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Methodology}{63}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Pre-processing}{63}{subsection.4.4.1}\protected@file@percent }
\newlabel{sec:pre-proc}{{4.4.1}{63}{Pre-processing}{subsection.4.4.1}{}}
\citation{sztrajman2021neural}
\citation{rusinkiewicz1998new}
\citation{sitzmann2020siren}
\citation{sitzmann2020metasdf}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces During training, the set encoder and hypernetwork decoder are trained on a set of materials to predict the weights of hyponet (MLP) so that it can reconstruct the training set. The BRDF data is provided as a set of BRDF coordinates, $H_n,D_n$, and the corresponding reflectance values $f_r(H_n,D_n)$. To reconstruct a new material from a small set of BRDF reflectance samples, the trained set encoder and hypernetwork decoder are used to predict the weights of hyponet for the unknown material. Once those weights are known, we can query BRDF at any coordinates and for any new materials, conditioned on the embedding of their sampled BRDF values.}}{64}{figure.caption.57}\protected@file@percent }
\newlabel{fig:mainfig}{{4.2}{64}{During training, the set encoder and hypernetwork decoder are trained on a set of materials to predict the weights of hyponet (MLP) so that it can reconstruct the training set. The BRDF data is provided as a set of BRDF coordinates, $H_n,D_n$, and the corresponding reflectance values $f_r(H_n,D_n)$. To reconstruct a new material from a small set of BRDF reflectance samples, the trained set encoder and hypernetwork decoder are used to predict the weights of hyponet for the unknown material. Once those weights are known, we can query BRDF at any coordinates and for any new materials, conditioned on the embedding of their sampled BRDF values}{figure.caption.57}{}}
\newlabel{eq:preprocess}{{4.1}{64}{Pre-processing}{equation.4.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Hypernetwork}{64}{subsection.4.4.2}\protected@file@percent }
\newlabel{sec:hypernet}{{4.4.2}{64}{Hypernetwork}{subsection.4.4.2}{}}
\citation{rusinkiewicz1998new}
\citation{zaheer2017deepsets}
\citation{sztrajman2021neural}
\citation{ha2017hypernetworks}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2.1}Set Encoder}{65}{subsubsection.4.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2.2}Hypernetwork Decoder and Hyponet}{65}{subsubsection.4.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Training}{65}{subsection.4.4.3}\protected@file@percent }
\newlabel{sec:traindet}{{4.4.3}{65}{Training}{subsection.4.4.3}{}}
\citation{ngan2005experimental}
\citation{Matusik2003jul}
\citation{dupuy2018adaptive}
\citation{Matusik2003jul}
\citation{rusinkiewicz1998new}
\newlabel{eq:loss}{{4.2}{66}{Training}{equation.4.4.2}{}}
\newlabel{eq:Lrec}{{4.3}{66}{Training}{equation.4.4.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Inference:}{66}{section*.58}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Experiments}{66}{section.4.5}\protected@file@percent }
\newlabel{sec:exp}{{4.5}{66}{Experiments}{section.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Datasets and Baselines}{66}{subsection.4.5.1}\protected@file@percent }
\citation{sztrajman2021neural}
\citation{nielsen2015optimal}
\citation{sztrajman2021neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Sparse BRDF Reconstruction}{67}{subsection.4.5.2}\protected@file@percent }
\newlabel{sec:brdf_rec}{{4.5.2}{67}{Sparse BRDF Reconstruction}{subsection.4.5.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2.1}Qualitative Comparison}{67}{subsubsection.4.5.2.1}\protected@file@percent }
\newlabel{sec:qual_comp}{{4.5.2.1}{67}{Qualitative Comparison}{subsubsection.4.5.2.1}{}}
\citation{matusik2003data,ngan2006image}
\citation{nielsen2015optimal}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces t-SNE clustering of the test embeddings with different sample sizes, including $N=8, 40, 160, 4\,000, 40\,000, 640\,000$.}}{68}{figure.caption.59}\protected@file@percent }
\newlabel{fig:tsne-vis-imputation}{{4.3}{68}{t-SNE clustering of the test embeddings with different sample sizes, including $N=8, 40, 160, 4\,000, 40\,000, 640\,000$}{figure.caption.59}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Qualitative comparison results for reconstruction with small sample sizes. Thanks to the prior that our hypernetwork model learns for material appearance through training, it can accurately estimate the BRDFs of unseen materials and preserve the colors better than the baselines.}}{69}{figure.caption.60}\protected@file@percent }
\newlabel{fig:imp_comp_upt}{{4.4}{69}{Qualitative comparison results for reconstruction with small sample sizes. Thanks to the prior that our hypernetwork model learns for material appearance through training, it can accurately estimate the BRDFs of unseen materials and preserve the colors better than the baselines}{figure.caption.60}{}}
\citation{zheng2021compact}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Average PSNR, Delta E (CIE 2000), and SSIM results across different sample sizes. }}{70}{figure.caption.61}\protected@file@percent }
\newlabel{fig:imp_plots}{{4.5}{70}{Average PSNR, Delta E (CIE 2000), and SSIM results across different sample sizes}{figure.caption.61}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Hypernetwork sparse reconstruction - Average metric results across varying sample sizes ($N$) over the test set. We highlight \colorbox {blue!25}{best} and \colorbox {orange!25}{second best} results.}}{70}{table.caption.62}\protected@file@percent }
\newlabel{table: ours_diff_samples}{{4.1}{70}{Hypernetwork sparse reconstruction - Average metric results across varying sample sizes ($N$) over the test set. We highlight \colorbox {blue!25}{best} and \colorbox {orange!25}{second best} results}{table.caption.62}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2.2}Quantitative Evaluation}{70}{subsubsection.4.5.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Compression - Average metric results over the renderings of the entire MERL dataset. We highlight \colorbox {blue!25}{best} and \colorbox {orange!25}{second best} results.}}{71}{table.caption.63}\protected@file@percent }
\newlabel{table: oursvsnps}{{4.2}{71}{Compression - Average metric results over the renderings of the entire MERL dataset. We highlight \colorbox {blue!25}{best} and \colorbox {orange!25}{second best} results}{table.caption.63}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Compression}{71}{subsection.4.5.3}\protected@file@percent }
\newlabel{sec:compression}{{4.5.3}{71}{Compression}{subsection.4.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Reconstruction results for BRDF compression (GT: ground truths).}}{71}{figure.caption.64}\protected@file@percent }
\newlabel{fig:comp-fig}{{4.6}{71}{Reconstruction results for BRDF compression (GT: ground truths)}{figure.caption.64}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}BRDF Editing}{72}{subsection.4.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces BRDF editing through linear interpolation between the embeddings of two materials.}}{72}{figure.caption.65}\protected@file@percent }
\newlabel{fig:interpolation}{{4.7}{72}{BRDF editing through linear interpolation between the embeddings of two materials}{figure.caption.65}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.5}Limitations and Future Work}{72}{subsection.4.5.5}\protected@file@percent }
\newlabel{sec:limits}{{4.5.5}{72}{Limitations and Future Work}{subsection.4.5.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Specular components:}{72}{section*.66}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{BRDF editing:}{72}{section*.67}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{SVBRDF representations:}{72}{section*.68}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Conclusion}{73}{section.4.6}\protected@file@percent }
\newlabel{sec:conc}{{4.6}{73}{Conclusion}{section.4.6}{}}
\bibstyle{plainnat}
\bibdata{thesis}
\bibcite{An10User}{{1}{2010}{{An and Pellacini}}{{}}}
\bibcite{ashikhmin2007}{{2}{2007}{{Ashikhmin and Premoze}}{{}}}
\bibcite{Bae06Two}{{3}{2006}{{Bae et~al.}}{{Bae, Paris, and Durand}}}
\bibcite{bagher2016}{{4}{2016{}}{{Bagher et~al.}}{{Bagher, Snyder, and Nowrouzezahrai}}}
\bibcite{bagher2016non}{{5}{2016{}}{{Bagher et~al.}}{{Bagher, Snyder, and Nowrouzezahrai}}}
\bibcite{Berthouzoz11AFramework}{{6}{2011}{{Berthouzoz et~al.}}{{Berthouzoz, Li, Dontcheva, and Agrawala}}}
\bibcite{Besl92AMethod}{{7}{1992}{{Besl and McKay}}{{}}}
\bibcite{bilgili2011general}{{8}{2011}{{Bilgili et~al.}}{{Bilgili, {\"O}zt{\"u}rk, and Kurt}}}
\bibcite{blinn77}{{9}{1977}{{Blinn}}{{}}}
\bibcite{boss2021nerd}{{10}{2021}{{Boss et~al.}}{{Boss, Braun, Jampani, Barron, Liu, and Lensch}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{75}{section*.69}\protected@file@percent }
\bibcite{Boyadzhiev15Band}{{11}{2015}{{Boyadzhiev et~al.}}{{Boyadzhiev, Bala, Paris, and Adelson}}}
\bibcite{triangulation}{{12}{2019}{{Broeren et~al.}}{{Broeren, van~de Sande, van~der Wijk, and Herder}}}
\bibcite{Bychkovsky11Learning}{{13}{2011}{{Bychkovsky et~al.}}{{Bychkovsky, Paris, Chan, and Durand}}}
\bibcite{cazenavette2021mixergan}{{14}{2021}{{Cazenavette and De~Guevara}}{{}}}
\bibcite{chen2016bilateral}{{15}{2016}{{Chen et~al.}}{{Chen, Adams, Wadhwa, and Hasinoff}}}
\bibcite{chen2017fast}{{16}{2017}{{Chen et~al.}}{{Chen, Xu, and Koltun}}}
\bibcite{Chen18Deep}{{17}{2018{}}{{Chen et~al.}}{{Chen, Wang, Kao, and Chuang}}}
\bibcite{chen2018deep}{{18}{2018{}}{{Chen et~al.}}{{Chen, Wang, Kao, and Chuang}}}
\bibcite{chen2021invertible}{{19}{2021}{{Chen et~al.}}{{Chen, Nobuhara, and Nishino}}}
\bibcite{CohenOr06Color}{{20}{2006}{{Cohen-Or et~al.}}{{Cohen-Or, Sorkine, Gal, Leyvand, and Xu}}}
\bibcite{cooktorrance1982}{{21}{1982}{{Cook and Torrance}}{{}}}
\bibcite{deschaintre2018single}{{22}{2018}{{Deschaintre et~al.}}{{Deschaintre, Aittala, Durand, Drettakis, and Bousseau}}}
\bibcite{deschaintre2019flexible}{{23}{2019}{{Deschaintre et~al.}}{{Deschaintre, Aittala, Durand, Drettakis, and Bousseau}}}
\bibcite{dupuy2018adaptive}{{24}{2018}{{Dupuy and Jakob}}{{}}}
\bibcite{dupuy2015}{{25}{2015}{{Dupuy et~al.}}{{Dupuy, Heitz, Iehl, Poulin, and Ostromoukhov}}}
\bibcite{erkocc2023hyperdiffusion}{{26}{2023}{{Erko{\c {c}} et~al.}}{{Erko{\c {c}}, Ma, Shan, Nie{\ss }ner, and Dai}}}
\bibcite{fan2021neural}{{27}{2021}{{Fan et~al.}}{{Fan, Wang, Ha{\v {s}}an, Yang, and Yan}}}
\bibcite{Faridul14ASurvey}{{28}{2014}{{Faridul et~al.}}{{Faridul, Pouli, Chamaret, Stauder, Tremeau, and Reinhard}}}
\bibcite{Frigo16Split}{{29}{2016}{{Frigo et~al.}}{{Frigo, Sabater, Delon, and Hellier}}}
\bibcite{gao2019deep}{{30}{2019}{{Gao et~al.}}{{Gao, Li, Dong, Peers, Xu, and Tong}}}
\bibcite{Gharbi17Deep}{{31}{2017}{{Gharbi et~al.}}{{Gharbi, Chen, Barron, Hasinoff, and Durand}}}
\bibcite{guarnera2016}{{32}{2016}{{Guarnera et~al.}}{{Guarnera, Guarnera, Ghosh, Denk, and Glencross}}}
\bibcite{guo2021highlight}{{33}{2021}{{Guo et~al.}}{{Guo, Lai, Tao, Cai, Wang, Guo, and Yan}}}
\bibcite{googlePhysicallyBased}{{34}{}{{Guy and Agopian}}{{}}}
\bibcite{ha2017hypernetworks}{{35}{2017}{{Ha et~al.}}{{Ha, Dai, and Le}}}
\bibcite{HaCohen11Nonrigid}{{36}{2011}{{HaCohen et~al.}}{{HaCohen, Shechtman, Goldman, and Lischinski}}}
\bibcite{he2020conditional}{{37}{2020}{{He et~al.}}{{He, Liu, Qiao, and Dong}}}
\bibcite{Hertzmann01Image}{{38}{2001}{{Hertzmann et~al.}}{{Hertzmann, Jacobs, Oliver, Curless, and Salesin}}}
\bibcite{hu2020deepbrdf}{{39}{2020}{{Hu et~al.}}{{Hu, Guo, Chen, Li, and Guo}}}
\bibcite{Hu18Exposure}{{40}{2018}{{Hu et~al.}}{{Hu, He, Xu, Wang, and Lin}}}
\bibcite{Huang14Parametric}{{41}{2014}{{Huang et~al.}}{{Huang, Zhang, Lai, Kopf, Cohen-Or, and Hu}}}
\bibcite{hui2017reflectance}{{42}{2017}{{Hui et~al.}}{{Hui, Sunkavalli, Lee, Hadap, Wang, and Sankaranarayanan}}}
\bibcite{Hwang12Context}{{43}{2012}{{Hwang et~al.}}{{Hwang, Kapoor, and Kang}}}
\bibcite{jiang2021cotr}{{44}{2021{}}{{Jiang et~al.}}{{Jiang, Trulls, Hosang, Tagliasacchi, and Yi}}}
\bibcite{9334429}{{45}{2021{}}{{Jiang et~al.}}{{Jiang, Gong, Liu, Cheng, Fang, Shen, Yang, Zhou, and Wang}}}
\bibcite{Kagarlitsky09Piecewise}{{46}{2009}{{Kagarlitsky et~al.}}{{Kagarlitsky, Moses, and Hel-Or}}}
\bibcite{kang2018efficient}{{47}{2018}{{Kang et~al.}}{{}}}
\bibcite{kang2019learning}{{48}{2019}{{Kang et~al.}}{{}}}
\bibcite{karras2019style}{{49}{2021}{{Karras et~al.}}{{Karras, Laine, and Aila}}}
\bibcite{Kaufman12Content}{{50}{2012}{{Kaufman et~al.}}{{Kaufman, Lischinski, and Werman}}}
\bibcite{kim2021representative}{{51}{2021}{{Kim et~al.}}{{Kim, Choi, Kim, and Koh}}}
\bibcite{Laffont14Transient}{{52}{2014}{{Laffont et~al.}}{{Laffont, Ren, Tao, Qian, and Hays}}}
\bibcite{lawrence2004efficient}{{53}{2004}{{Lawrence et~al.}}{{Lawrence, Rusinkiewicz, and Ramamoorthi}}}
\bibcite{lawrence2006inverse}{{54}{2006}{{Lawrence et~al.}}{{Lawrence, Ben-Artzi, DeCoro, Matusik, Pfister, Ramamoorthi, and Rusinkiewicz}}}
\bibcite{li2020lapar}{{55}{2020}{{Li et~al.}}{{Li, Zhou, Qi, Jiang, Lu, and Jia}}}
\bibcite{visual_attribute}{{56}{2017}{{Liao et~al.}}{{Liao, Yao, Yuan, Hua, and Kang}}}
\bibcite{lin2020tuigan}{{57}{2020}{{Lin et~al.}}{{Lin, Pang, Xia, Chen, and Luo}}}
\bibcite{liu2023learning}{{58}{2023}{{Liu et~al.}}{{Liu, Fischer, and Ritschel}}}
\bibcite{Liu16Makeup}{{59}{2016}{{Liu et~al.}}{{Liu, Ou, Qian, Wang, and Cao}}}
\bibcite{low2012}{{60}{2012}{{L{\"{o}}w et~al.}}{{L{\"{o}}w, Kronander, Ynnerman, and Unger}}}
\bibcite{ma2021retinexgan}{{61}{2021{}}{{Ma et~al.}}{{Ma, Guo, Yu, Chen, Ren, Xi, Li, and Zhou}}}
\bibcite{ma2021free}{{62}{2021{}}{{Ma et~al.}}{{}}}
\bibcite{ma2023opensvbrdf}{{63}{2023}{{Ma et~al.}}{{Ma, Xu, Zhang, Zhou, and Wu}}}
\bibcite{martin2022materia}{{64}{2022}{{Martin et~al.}}{{Martin, Roullier, Rouffet, Kaiser, and Boubekeur}}}
\bibcite{Matusik2003jul}{{65}{2003}{{Matusik et~al.}}{{Matusik, Pfister, Brand, and McMillan}}}
\bibcite{matusik2003data}{{66}{2003}{{Matusik}}{{}}}
\bibcite{maximov2019deep}{{67}{2019}{{Maximov et~al.}}{{Maximov, Leal-Taix{\'e}, Fritz, and Ritschel}}}
\bibcite{moran2020deeplpf}{{68}{2020}{{Moran et~al.}}{{Moran, Marza, McDonagh, Parisot, and Slabaugh}}}
\bibcite{mustafa2022distilling}{{69}{2022}{{Mustafa et~al.}}{{Mustafa, Hanji, and Mantiuk}}}
\bibcite{Nam17Deep}{{70}{2017}{{Nam and Kim}}{{}}}
\bibcite{ngan2005experimental}{{71}{2005{}}{{Ngan et~al.}}{{Ngan, Durand, and Matusik}}}
\bibcite{ngan2005}{{72}{2005{}}{{Ngan et~al.}}{{Ngan, Durand, and Matusik}}}
\bibcite{ngan2006image}{{73}{2006}{{Ngan et~al.}}{{Ngan, Durand, and Matusik}}}
\bibcite{nielsen2015optimal}{{74}{2015}{{Nielsen et~al.}}{{Nielsen, Jensen, and Ramamoorthi}}}
\bibcite{Omiya18Learning}{{75}{2018}{{Omiya et~al.}}{{Omiya, Simo-Serra, Iizuka, and Ishikawa}}}
\bibcite{park2019deepsdf}{{76}{2019}{{Park et~al.}}{{Park, Florence, Straub, Newcombe, and Lovegrove}}}
\bibcite{park2018distort}{{77}{2018}{{Park et~al.}}{{Park, Lee, Yoo, and Kweon}}}
\bibcite{specfig}{{78}{2021}{{Park and Baek}}{{}}}
\bibcite{Pitie05NDimensional}{{79}{2005}{{Pitie et~al.}}{{Pitie, Kokaram, and Dahyot}}}
\bibcite{Pitie07Automated}{{80}{2007}{{Piti{\'e} et~al.}}{{Piti{\'e}, Kokaram, and Dahyot}}}
\bibcite{Pouli11Progressive}{{81}{2011}{{Pouli and Reinhard}}{{}}}
\bibcite{rainer2019neural}{{82}{2019}{{Rainer et~al.}}{{Rainer, Jakob, Ghosh, and Weyrich}}}
\bibcite{ratzlaff2019hypergan}{{83}{2019}{{Ratzlaff and Fuxin}}{{}}}
\bibcite{rebain2022attention}{{84}{2022}{{Rebain et~al.}}{{Rebain, Matthews, Yi, Sharma, Lagun, and Tagliasacchi}}}
\bibcite{Reinhard01Color}{{85}{2001}{{Reinhard et~al.}}{{Reinhard, Ashikhmin, Gooch, and Shirley}}}
\bibcite{ronneberger2015u}{{86}{2015}{{Ronneberger et~al.}}{{Ronneberger, Fischer, and Brox}}}
\bibcite{rusinkiewicz1998new}{{87}{1998}{{Rusinkiewicz}}{{}}}
\bibcite{Saeedi18Multimodal}{{88}{2018}{{Saeedi et~al.}}{{Saeedi, Hoffman, DiVerdi, Ghandeharioun, Johnson, and Adams}}}
\bibcite{schewe2007color}{{89}{2007}{{Schewe and Fraser}}{{}}}
\bibcite{serrano2018intuitive}{{90}{2018}{{Serrano et~al.}}{{Serrano, Gutierrez, Myszkowski, Seidel, and Masia}}}
\bibcite{shaham2021spatially}{{91}{2021}{{Shaham et~al.}}{{Shaham, Gharbi, Zhang, Shechtman, and Michaeli}}}
\bibcite{Shapira13Image}{{92}{2013}{{Shapira et~al.}}{{Shapira, Avidan, and Hel-Or}}}
\bibcite{Shih13Data}{{93}{2013}{{Shih et~al.}}{{Shih, Paris, Durand, and Freeman}}}
\bibcite{Shih14Style}{{94}{2014}{{Shih et~al.}}{{Shih, Paris, Barnes, Freeman, and Durand}}}
\bibcite{sitzmann2020metasdf}{{95}{2020{}}{{Sitzmann et~al.}}{{Sitzmann, Chan, Tucker, Snavely, and Wetzstein}}}
\bibcite{sitzmann2020siren}{{96}{2020{}}{{Sitzmann et~al.}}{{Sitzmann, Martel, Bergman, Lindell, and Wetzstein}}}
\bibcite{sun2007interactive}{{97}{2007}{{Sun et~al.}}{{Sun, Zhou, Chen, Lin, Shi, and Guo}}}
\bibcite{Sunkavalli10Multi}{{98}{2010}{{Sunkavalli et~al.}}{{Sunkavalli, Johnson, Matusik, and Pfister}}}
\bibcite{sztrajman2021neural}{{99}{2021}{{Sztrajman et~al.}}{{Sztrajman, Rainer, Ritschel, and Weyrich}}}
\bibcite{Tai07Soft}{{100}{2007}{{Tai et~al.}}{{Tai, Jia, and Tang}}}
\bibcite{Tai05Local}{{101}{2005}{{Tai et~al.}}{{Tai, Jia, and Tang}}}
\bibcite{ffn}{{102}{2020}{{Tancik et~al.}}{{Tancik, Srinivasan, Mildenhall, Fridovich-Keil, Raghavan, Singhal, Ramamoorthi, Barron, and Ng}}}
\bibcite{tolstikhin2021mlp}{{103}{2021}{{Tolstikhin et~al.}}{{Tolstikhin, Houlsby, Kolesnikov, Beyer, Zhai, Unterthiner, Yung, Steiner, Keysers, Uszkoreit, et~al.}}}
\bibcite{tongbuasirilai2020compact}{{104}{2020}{{Tongbuasirilai et~al.}}{{Tongbuasirilai, Unger, Kronander, and Kurt}}}
\bibcite{tseng2019hyperparameter}{{105}{2019}{{Tseng et~al.}}{{Tseng, Yu, Yang, Mannan, Arnaud, Nowrouzezahrai, Lalonde, and Heide}}}
\bibcite{tseng2022neural}{{106}{2022}{{Tseng et~al.}}{{Tseng, Zhang, Jebe, Zhang, Xia, Fan, Heide, and Chen}}}
\bibcite{tunwattanapong2013acquiring}{{107}{2013}{{Tunwattanapong et~al.}}{{Tunwattanapong, Fyffe, Graham, Busch, Yu, Ghosh, and Debevec}}}
\bibcite{walter2007microfacet}{{108}{2007}{{Walter et~al.}}{{Walter, Marschner, Li, and Torrance}}}
\bibcite{wang2022attention}{{109}{2022}{{Wang et~al.}}{{Wang, Chen, Chen, Venugopalan, Wang, et~al.}}}
\bibcite{wang2019underexposed}{{110}{2019}{{Wang et~al.}}{{Wang, Zhang, Fu, Shen, Zheng, and Jia}}}
\bibcite{ward1992}{{111}{1992}{{Ward}}{{}}}
\bibcite{xu2015deep}{{112}{2015}{{Xu et~al.}}{{Xu, Ren, Yan, Liao, and Jia}}}
\bibcite{Yan14Automatic}{{113}{2014}{{Yan et~al.}}{{Yan, Zhang, Wang, Paris, and Yu}}}
\bibcite{10.1145/2790296}{{114}{2016{}}{{Yan et~al.}}{{Yan, Zhang, Wang, Paris, and Yu}}}
\bibcite{yan2016automatic}{{115}{2016{}}{{Yan et~al.}}{{Yan, Zhang, Wang, Paris, and Yu}}}
\bibcite{yang2023contranerf}{{116}{2023}{{Yang et~al.}}{{Yang, Hong, Li, Hu, Li, Lee, and Wang}}}
\bibcite{yang2020fidelity}{{117}{2020}{{Yang et~al.}}{{Yang, Wang, Fang, Wang, and Liu}}}
\bibcite{yu2021reconfigisp}{{118}{2021}{{Yu et~al.}}{{Yu, Li, Peng, Loy, and Gu}}}
\bibcite{zaheer2017deepsets}{{119}{2017}{{Zaheer et~al.}}{{Zaheer, Kottur, Ravanbakhsh, Poczos, Salakhutdinov, and Smola}}}
\bibcite{Zhang13Style}{{120}{2013}{{Zhang et~al.}}{{Zhang, Cao, Chen, Liu, and Tang}}}
\bibcite{zheng2021compact}{{121}{2021}{{Zheng et~al.}}{{Zheng, Zheng, Wang, Zhao, and Bao}}}
\bibcite{cnf2023}{{122}{2023}{{Zhong et~al.}}{{Zhong, Fogarty, Hanji, Wu, Sztrajman, Spielberg, Tagliasacchi, Bosilj, and Oztireli}}}
\bibcite{zhou2021adversarial}{{123}{2021}{{Zhou and Kalantari}}{{}}}
\bibcite{Zhu18Automatic}{{124}{2018}{{Zhu and Yu}}{{}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Extra Information}{89}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\gdef \@abspage@last{90}
